import os
import asyncio
import sys
import numpy as np
from loguru import logger
from dotenv import load_dotenv

# === Load Env ===
load_dotenv()

# LightRAG Imports
from lightrag import LightRAG
from lightrag.utils import EmbeddingFunc
from lightrag.llm.openai import azure_openai_complete, openai_embed
from lightrag.utils import DocStatus # å¼•å…¥ç‹€æ…‹æšèˆ‰

# === Path Setup ===
current_path = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(current_path))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# === Config ===
WORKING_DIR = "./data/rag_storage"
TARGET_DOC_ID = "doc-1a240209386d418c61ec3d1ae4a8a738" # ğŸ”¥ å¡«å…¥é‚£å€‹é ‘å›ºçš„ ID

# === Helper ===
def get_clean_env(key, default=None):
    val = os.getenv(key, default)
    return val.strip() if val else val

async def main():
    logger.info(f"ğŸš‘ å•Ÿå‹• LightRAG ä¿®å¾©ç¨‹åº (Fixer Mode)...")
    
    # 1. åˆå§‹åŒ– LightRAG (ç‚ºäº†é€£æ¥ Storage)
    embed_model_name = get_clean_env("EMBEDDING_MODEL")
    embed_api_key = get_clean_env("EMBEDDING_BINDING_API_KEY") 
    embed_base_url = get_clean_env("EMBEDDING_BINDING_HOST")
    embed_dim = int(get_clean_env("EMBEDDING_DIM", "1024"))

    async def embedding_func_wrapper(texts: list[str]) -> np.ndarray:
        return await openai_embed.func(
            texts=texts,
            model=embed_model_name,
            api_key=embed_api_key,     
            base_url=embed_base_url
        )

    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=azure_openai_complete,
        embedding_func=EmbeddingFunc(
            embedding_dim=embed_dim,
            max_token_size=8192,
            func=embedding_func_wrapper
        ),
    )
    await rag.initialize_storages()

    # 2. å˜—è©¦æƒææ‰€æœ‰ Chunksï¼Œæ‰¾å‡ºå±¬æ–¼ç›®æ¨™æ–‡æª”çš„å­¤å…’
    logger.info("ğŸ” æ­£åœ¨æƒæ text_chunks å°‹æ‰¾å­¤å…’ç¢ç‰‡...")
    
    # å­˜å–å…§éƒ¨æ•¸æ“š (Private Access)
    if not hasattr(rag.text_chunks, "_data"):
        logger.error("âŒ ç„¡æ³•å­˜å– text_chunksï¼Œæ“ä½œä¸­æ­¢ã€‚")
        return

    all_chunks = rag.text_chunks._data
    found_chunk_ids = []

    for chunk_id, chunk_data in all_chunks.items():
        # æª¢æŸ¥é€™å€‹ chunk æ˜¯å¦å±¬æ–¼æˆ‘å€‘çš„ç›®æ¨™æ–‡æª”
        # é€šå¸¸ chunk_data æœƒæœ‰ 'doc_id' æ¬„ä½
        if chunk_data.get("doc_id") == TARGET_DOC_ID:
            found_chunk_ids.append(chunk_id)

    logger.info(f"ğŸ“Š æ‰¾åˆ° {len(found_chunk_ids)} å€‹å±¬æ–¼ {TARGET_DOC_ID} çš„å­¤å…’ç¢ç‰‡ã€‚")

    # 3. å½é€  doc_status (Mocking the Status)
    logger.info("ğŸ› ï¸ æ­£åœ¨å½é€  doc_status è¨˜éŒ„...")
    
    fake_status = {
        "status": DocStatus.PROCESSED, # å‘Šè¨´ç³»çµ±é€™æ˜¯è™•ç†å¥½çš„
        "file_path": "force_restored_file", # éš¨ä¾¿å¡«ï¼Œå””é‡è¦
        "chunks_list": found_chunk_ids, # ğŸ”¥ é—œéµï¼šæŠŠæ‰¾åˆ°çš„ ID å¡é€²å»
        "chunks_count": len(found_chunk_ids),
        "create_time": 0,
        "update_time": 0
    }

    # å¼·åˆ¶å¯«å…¥ KV Store
    await rag.doc_status.upsert({TARGET_DOC_ID: fake_status})
    logger.success("âœ… doc_status å·²æˆåŠŸä¿®å¾©ï¼")

    # 4. ç¾åœ¨å¯ä»¥åŸ·è¡Œæ­£è¦åˆªé™¤äº†ï¼
    logger.info("ğŸ—‘ï¸ åŸ·è¡Œæ­£è¦åˆªé™¤ (adelete_by_doc_id)...")
    try:
        # å› ç‚º doc_status å­˜åœ¨äº†ï¼Œé€™æ¬¡å®ƒæœƒä¹–ä¹–åˆªé™¤ vector, graph å’Œ text chunks
        await rag.adelete_by_doc_id(TARGET_DOC_ID)
        logger.success("ğŸ‰ å®Œç¾ï¼æ–‡æª”åŠå…¶æ‰€æœ‰ç¢ç‰‡å·²è¢«å¾¹åº•æ¸…é™¤ã€‚")
    except Exception as e:
        logger.error(f"âŒ åˆªé™¤å¤±æ•—: {e}")

if __name__ == "__main__":
    asyncio.run(main())