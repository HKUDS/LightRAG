import sys
import os
import json
import glob
import time
import math
import gc
import io
import re
import numpy as np
import torch
import pymupdf  # ç„¡éœ€ Poppler
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from qwen_vl_utils import process_vision_info
from loguru import logger

# ==========================================
# ğŸ”¥ [ä½¿ç”¨è€…è¨­å®šå€]
# ==========================================

INPUT_DIR = "./data/input/__enqueued__"
OUTPUT_DIR_BASE = "./data/input/step1_output"
MODEL_ID = "ByteDance/Dolphin-v2"

# æ¸²æŸ“ PDF çš„ç•«è³ª (300 DPI ç‚ºä½³ï¼Œä¿è­‰å°å­—æ¸…æ™°)
RENDER_DPI = 300 

# å­˜æª”é »ç‡ (æ¯å¹¾é å­˜ä¸€æ¬¡)
SAVE_INTERVAL = 1

# ==========================================
# ğŸ› ï¸ æ ¸å¿ƒå·¥å…·å‡½å¼ (Dolphin å®˜æ–¹é‚è¼¯)
# ==========================================

def smart_resize(height, width, factor=28, min_pixels=784, max_pixels=2560000):
    """Dolphin å®˜æ–¹çš„åœ–ç‰‡ç¸®æ”¾é‚è¼¯ (ç”¨æ–¼åæ¨™æ›ç®—)"""
    if max(height, width) / min(height, width) > 200:
        resize_factor = max(height, width) // min_pixels
        if resize_factor > 1:
            height = height // resize_factor
            width = width // resize_factor
            return height, width
            
    h_bar = round(height / factor) * factor
    w_bar = round(width / factor) * factor
    
    if h_bar * w_bar > max_pixels:
        beta = math.sqrt((height * width) / max_pixels)
        h_bar = math.floor(height / beta / factor) * factor
        w_bar = math.floor(width / beta / factor) * factor
    elif h_bar * w_bar < min_pixels:
        beta = math.sqrt(min_pixels / (height * width))
        h_bar = math.ceil(height * beta / factor) * factor
        w_bar = math.ceil(width * beta / factor) * factor
        
    return h_bar, w_bar

def resize_img(image, max_size=1600, min_size=28):
    """Dolphin é è™•ç†ç¸®æ”¾ (ç”¨æ–¼æ¨è«–è¼¸å…¥)"""
    width, height = image.size
    if max(width, height) < max_size and min(width, height) >= 28:
        return image
    
    if max(width, height) > max_size:
        if width > height:
            new_width = max_size
            new_height = int(height * (max_size / width))
        else:
            new_height = max_size
            new_width = int(width * (max_size / height))
        image = image.resize((new_width, new_height))
        width, height = image.size
    
    if min(width, height) < 28:
        if width < height:
            new_width = min_size
            new_height = int(height * (min_size / width))
        else:
            new_height = min_size
            new_width = int(width * (min_size / height))
        image = image.resize((new_width, new_height))

    return image

def process_coordinates(coords, pil_image):
    """å°‡æ¨¡å‹è¼¸å‡ºçš„æ­¸ä¸€åŒ–åæ¨™è½‰å›åŸåœ–åæ¨™"""
    original_w, original_h = pil_image.size
    resized_pil = resize_img(pil_image)
    resized_image = np.array(resized_pil)
    resized_h, resized_w = resized_image.shape[:2]
    resized_h, resized_w = smart_resize(resized_h, resized_w, factor=28, min_pixels=784, max_pixels=2560000)

    w_ratio, h_ratio = original_w / resized_w, original_h / resized_h
    x1 = int(coords[0] * w_ratio)
    y1 = int(coords[1] * h_ratio)
    x2 = int(coords[2] * w_ratio)
    y2 = int(coords[3] * h_ratio)

    x1 = max(0, min(x1, original_w - 1))
    y1 = max(0, min(y1, original_h - 1))
    x2 = max(x1 + 1, min(x2, original_w))
    y2 = max(y1 + 1, min(y2, original_h))
    return x1, y1, x2, y2

def extract_labels_from_string(text):
    """å¾è¼¸å‡ºå­—ä¸²æå–æ¨™ç±¤"""
    all_matches = re.findall(r'\[([^\]]+)\]', text)
    labels = []
    for match in all_matches:
        if not re.match(r'^\d+,\d+,\d+,\d+$', match):
            labels.append(match)
    return labels

def parse_layout_string(bbox_str):
    """è§£ææ¨¡å‹è¼¸å‡ºçš„ Layout å­—ä¸²"""
    parsed_results = []
    if not bbox_str: return []
    
    segments = bbox_str.split('[PAIR_SEP]')
    new_segments = []
    for seg in segments:
        new_segments.extend(seg.split('[RELATION_SEP]'))
    segments = new_segments
    
    for segment in segments:
        segment = segment.strip()
        if not segment: continue
        
        coord_pattern = r'\[(\d*\.?\d+),(\d*\.?\d+),(\d*\.?\d+),(\d*\.?\d+)\]'
        coord_match = re.search(coord_pattern, segment)
        label_matches = extract_labels_from_string(segment)
        
        if coord_match and label_matches:
            coords = [float(coord_match.group(i)) for i in range(1, 5)]
            label = label_matches[0].strip()
            parsed_results.append((coords, label, label_matches[1:]))
            
    return parsed_results

# ==========================================
# âš™ï¸ æ ¸å¿ƒè™•ç†é‚è¼¯
# ==========================================

# Setup Logging
logger.remove()
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>", level="INFO")

def load_model():
    logger.info("="*60)
    
    # 1. è‡ªå‹•åµæ¸¬è£ç½®
    if torch.cuda.is_available():
        device = "cuda"
        logger.info(f"ğŸ“¥ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {MODEL_ID} (GPU 4-bit Mode)...")
    else:
        device = "cpu"
        logger.info(f"ğŸ“¥ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {MODEL_ID} (CPU High-Quality Mode)...")
        logger.warning("âš ï¸ æ³¨æ„ï¼šCPU æ¨¡å¼æ¨è«–é€Ÿåº¦è¼ƒæ…¢ï¼Œä¸”éœ€è¦ç´„ 12GB+ RAMã€‚")

    try:
        processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
        
        if device == "cuda":
            # === GPU æ¨¡å¼ï¼šä½¿ç”¨ 4-bit é‡åŒ–çœé¡¯å­˜ ===
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                MODEL_ID,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True
            )
        else:
            # === CPU æ¨¡å¼ï¼šä½¿ç”¨ Float32 ç¢ºä¿ç›¸å®¹æ€§ ===
            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float32, 
                device_map="cpu",
                trust_remote_code=True
            )

        logger.success(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼(Device: {model.device})")
        return model, processor

    except Exception as e:
        logger.critical(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        sys.exit(1)

def run_inference(model, processor, image, prompt):
    """é€šç”¨çš„æ¨è«–å‡½å¼ (è‡ªå‹•è™•ç† Device)"""
    # é è™•ç†åœ–ç‰‡
    image = resize_img(image)
    
    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": prompt}
        ]
    }]
    
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    
    inputs = processor(
        text=[text], 
        images=image_inputs, 
        videos=video_inputs, 
        padding=True, 
        return_tensors="pt"
    ).to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=2048,
            do_sample=False
        )
    
    gen_ids_trimmed = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]
    output_text = processor.batch_decode(gen_ids_trimmed, skip_special_tokens=True)[0]
    
    # Clean memory
    del inputs, generated_ids
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return output_text

def process_single_file(input_path, output_base, model, processor):
    filename = os.path.basename(input_path)
    file_stem = os.path.splitext(filename)[0]
    
    # å»ºç«‹ç›®éŒ„
    current_out_dir = os.path.join(output_base, file_stem)
    if not os.path.exists(current_out_dir): os.makedirs(current_out_dir)
    final_json = os.path.join(current_out_dir, "intermediate_result.json")
    
    # åˆ†åˆ¥å­˜æ”¾ crop å‡ºä¾†çš„åœ–ç‰‡
    images_dir = os.path.join(current_out_dir, "images")
    if not os.path.exists(images_dir): os.makedirs(images_dir)

    logger.info("-" * 40)
    logger.info(f"ğŸš€ è™•ç†æª”æ¡ˆ: {filename}")
    
    try:
        doc = pymupdf.open(input_path)
        total_pages = len(doc)
    except Exception as e:
        logger.error(f"âŒ ç„¡æ³•é–‹å•Ÿ PDF: {e}")
        return

    parsed_data = []

    for i in range(total_pages):
        logger.info(f"   ğŸ“„ Page {i+1}/{total_pages} åˆ†æ Layout ä¸­...")
        
        # 1. Render Page (High Res)
        try:
            page = doc[i]
            pix = page.get_pixmap(dpi=RENDER_DPI)
            img_data = pix.tobytes("png")
            pil_image = Image.open(io.BytesIO(img_data)).convert("RGB")
        except Exception as e:
            logger.error(f"âŒ Page {i+1} æ¸²æŸ“å¤±æ•—: {e}")
            continue

        # 2. Stage 1: Layout Parsing
        try:
            layout_text = run_inference(model, processor, pil_image, "Parse the reading order of this document.")
            layout_items = parse_layout_string(layout_text)
            
            # å¦‚æœ Layout è§£æå¤±æ•—æˆ–å›å‚³ç©ºï¼Œé€€å›åˆ° "distorted_page" (æ•´é ç•¶ä¸€å€‹ Text è™•ç†)
            if not layout_items:
                logger.warning(f"      âš ï¸ ç„¡æ³•è§£æ Layoutï¼Œé€€å›æ•´é  OCR æ¨¡å¼")
                layout_items = [([0,0,0,0], 'distorted_page', [])] # å‡ Layout
        except Exception as e:
            logger.error(f"âŒ Layout æ¨ç†éŒ¯èª¤: {e}")
            continue

        logger.info(f"      ğŸ” åµæ¸¬åˆ° {len(layout_items)} å€‹å…ƒç´ ï¼Œé–‹å§‹æå–...")
        
        page_reading_order = 0
        
        # 3. Stage 2: Element Extraction & Routing
        for bbox, label, tags in layout_items:
            # è™•ç†åæ¨™
            if label == 'distorted_page':
                x1, y1, x2, y2 = 0, 0, pil_image.width, pil_image.height
                pil_crop = pil_image
            else:
                x1, y1, x2, y2 = process_coordinates(bbox, pil_image)
                # å®‰å…¨é‚Šç•Œæª¢æŸ¥
                if x2 <= x1 or y2 <= y1: continue
                pil_crop = pil_image.crop((x1, y1, x2, y2))
            
            # å¿½ç•¥éå°çš„ç¢ç‰‡
            if pil_crop.width < 10 or pil_crop.height < 10: continue
            
            element_data = {
                "page_idx": i,
                "bbox": [x1, y1, x2, y2],
                "reading_order": page_reading_order,
                "label": label
            }
            
            # === ğŸ”¥ é—œéµè·¯ç”± (Routing Logic) ===
            
            # Case A: åœ–ç‰‡ (Image/Figure) -> åªå­˜åœ–ï¼Œä¸ OCR
            if label == "fig":
                img_filename = f"p{i+1}_{page_reading_order:03d}_fig.jpg"
                save_path = os.path.join(images_dir, img_filename)
                pil_crop.save(save_path)
                
                element_data["type"] = "image"
                element_data["content"] = f"![Figure]({img_filename})" # Markdown æ ¼å¼
                element_data["img_path"] = f"images/{img_filename}"
                logger.debug(f"         ğŸ–¼ï¸ åœ–ç‰‡ (Saved): {img_filename}")

            # Case B: è¡¨æ ¼ (Table) -> ä½¿ç”¨è¡¨æ ¼å°ˆç”¨ Prompt
            elif label == "tab":
                element_data["type"] = "table"
                md_table = run_inference(model, processor, pil_crop, "Parse the table in the image.")
                element_data["content"] = md_table
                logger.debug(f"         ğŸ“Š è¡¨æ ¼ (Parsed)")
                
                # é †ä¾¿å­˜å€‹è¡¨æ ¼æˆªåœ–å‚™ä»½
                tab_filename = f"p{i+1}_{page_reading_order:03d}_tab.jpg"
                pil_crop.save(os.path.join(images_dir, tab_filename))
                element_data["img_path"] = f"images/{tab_filename}"

            # Case C: æ–‡å­—/æ¨™é¡Œ (Text/Title) -> ä½¿ç”¨æ–‡å­— Prompt
            else:
                # åŒ…å«: text, section_header, title, list, code ç­‰
                element_data["type"] = "text"
                ocr_text = run_inference(model, processor, pil_crop, "Read text in the image.")
                element_data["content"] = ocr_text
                # æ–‡å­—é€šå¸¸ä¸å­˜åœ–ï¼Œçœç©ºé–“
                element_data["img_path"] = ""

            parsed_data.append(element_data)
            page_reading_order += 1
            
        # é‡‹æ”¾è¨˜æ†¶é«”
        del pil_image
        gc.collect()

        # Incremental Save
        if (i + 1) % SAVE_INTERVAL == 0:
            with open(final_json, "w", encoding="utf-8") as f:
                json.dump(parsed_data, f, ensure_ascii=False, indent=2)

    # Final Save
    with open(final_json, "w", encoding="utf-8") as f:
        json.dump(parsed_data, f, ensure_ascii=False, indent=2)
    
    logger.success(f"ğŸ‰ æª”æ¡ˆ {filename} å®Œæˆï¼")

def main():
    if not os.path.exists(INPUT_DIR):
        logger.error(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥ç›®éŒ„: {INPUT_DIR}")
        return
    if not os.path.exists(OUTPUT_DIR_BASE):
        os.makedirs(OUTPUT_DIR_BASE)

    # æª¢æŸ¥ PyMuPDF
    try:
        import pymupdf
    except ImportError:
        logger.error("âŒ ç¼ºå°‘ PyMuPDFï¼è«‹åŸ·è¡Œ: `uv pip install pymupdf`")
        return

    model, processor = load_model()

    all_files = glob.glob(os.path.join(INPUT_DIR, "*"))
    files = [f for f in all_files if os.path.isfile(f) and not os.path.basename(f).startswith(".")]
    
    logger.info(f"ğŸ“¦ ç™¼ç¾ {len(files)} å€‹æª”æ¡ˆ...")

    for file_path in files:
        if not file_path.lower().endswith(".pdf"):
            continue
        process_single_file(file_path, OUTPUT_DIR_BASE, model, processor)

if __name__ == "__main__":
    main()