# @title 3. åŸ·è¡Œ AI è§£æ (Dolphin-v2 | é™åˆ¶ 16k Tokens)
import sys
import os
import json
import glob
import time
import math
import torch
from pdf2image import convert_from_path
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from qwen_vl_utils import process_vision_info
from loguru import logger

# === è¨­å®š ===
INPUT_DIR = "./data/input/__enqueued__"
OUTPUT_DIR_BASE = "./data/input/step1_output"
MODEL_ID = "ByteDance/Dolphin-v2"

# === ğŸ”¥ é—œéµè¨­å®šï¼šToken é™åˆ¶ ===
# 16000 Tokens ç´„ç­‰æ–¼ 313è¬åƒç´  (ä¾‹å¦‚ 1500x2000)
# é€™èƒ½æœ‰æ•ˆé˜²æ­¢ OOMï¼ŒåŒæ™‚ä¿æŒæ¯” 150 DPI æ›´å¥½çš„ç•«è³ª
MAX_VISUAL_TOKENS = 16000 

# === Logger ===
logger.remove()
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>", level="INFO")

def resize_to_token_limit(image, max_tokens=16000):
    """
    æ ¹æ“šç›®æ¨™ Token æ•¸é‡è‡ªå‹•ç¸®æ”¾åœ–ç‰‡
    åŸç†: Qwen2.5-VL ä½¿ç”¨ 14x14 patchï¼Œ1 token â‰ˆ 196 pixels
    """
    w, h = image.size
    total_pixels = w * h
    
    # è¨ˆç®—ç›®å‰çš„è¦–è¦º Token ä¼°ç®—å€¼
    current_tokens = total_pixels / (14 * 14)
    
    # å¦‚æœè¶…éé™åˆ¶ï¼Œé€²è¡Œç¸®æ”¾
    if current_tokens > max_tokens:
        scale = math.sqrt(max_tokens / current_tokens)
        new_w = int(w * scale)
        new_h = int(h * scale)
        logger.info(f"ğŸ“‰ å£“ç¸®åœ–ç‰‡: {w}x{h} ({int(current_tokens)} tokens) -> {new_w}x{new_h} (~{max_tokens} tokens)")
        return image.resize((new_w, new_h), Image.Resampling.LANCZOS)
    
    return image

def load_model():
    logger.info(f"ğŸ“¥ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {MODEL_ID} (4-bit Mode)...")
    try:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            MODEL_ID,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
        logger.success("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
        return model, processor
    except Exception as e:
        logger.critical(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        sys.exit(1)

def process_single_file(input_path, output_base, model, processor):
    filename = os.path.basename(input_path)
    file_stem = os.path.splitext(filename)[0]
    
    current_out_dir = os.path.join(output_base, file_stem)
    if not os.path.exists(current_out_dir): os.makedirs(current_out_dir)
    final_json = os.path.join(current_out_dir, "intermediate_result.json")

    logger.info(f"ğŸš€ è™•ç†: {filename}")
    
    try:
        # å…ˆç”¨è¼ƒé«˜ DPI (200) è®€å–ï¼Œç„¶å¾Œç”¨ç¨‹å¼ç¢¼ç²¾æº–å£“ç¸®åˆ° 16k tokens
        # é€™æ¨£æ¯”ç›´æ¥è¨­ä½ DPI (å¦‚ 150) ç•«è³ªæ›´å¥½ï¼Œå› ç‚ºæ˜¯ Downsampling
        images = convert_from_path(input_path, dpi=200)
    except Exception as e:
        logger.error(f"âŒ è½‰åœ–å¤±æ•—: {e}")
        return

    parsed_results = []
    
    for i, image in enumerate(images):
        # ğŸ”¥ å¥—ç”¨ Token é™åˆ¶
        image = resize_to_token_limit(image, max_tokens=MAX_VISUAL_TOKENS)
        
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "Extract all text and layout from this document into Markdown format."}
            ]
        }]
        
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        
        inputs = processor(
            text=[text], images=image_inputs, videos=video_inputs,
            padding=True, return_tensors="pt"
        ).to(model.device)

        try:
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=2048,
                    do_sample=False
                )
            
            gen_ids_trimmed = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]
            md_text = processor.batch_decode(gen_ids_trimmed, skip_special_tokens=True)[0]
            
            parsed_results.append({
                "type": "text", "text": md_text, "page_idx": i,
                "img_path": "", "bbox": [0,0,image.width, image.height]
            })
            
            logger.info(f"   â†³ Page {i+1} å®Œæˆ")
            
        except torch.cuda.OutOfMemoryError:
            logger.error(f"âŒ Page {i+1} OOM (çˆ†é¡¯å­˜)ï¼å˜—è©¦æ¸…ç†å¿«å–...")
            torch.cuda.empty_cache()
            continue
            
        del inputs, generated_ids, image
        torch.cuda.empty_cache()

    with open(final_json, "w", encoding="utf-8") as f:
        json.dump(parsed_results, f, ensure_ascii=False, indent=2)
    logger.success(f"ğŸ’¾ å„²å­˜æˆåŠŸ: {final_json}")

# === ä¸»æµç¨‹ ===
files_list = glob.glob(os.path.join(INPUT_DIR, "*"))
if not files_list:
    logger.warning("ğŸ“‚ æ²’æœ‰æª”æ¡ˆï¼è«‹å…ˆåŸ·è¡Œ Step 2 ä¸Šå‚³æª”æ¡ˆã€‚")
else:
    model, processor = load_model()
    for f in files_list:
        process_single_file(f, OUTPUT_DIR_BASE, model, processor)
    logger.info("ğŸ‰ å…¨éƒ¨å®Œæˆï¼è«‹åŸ·è¡Œ Step 4 ä¸‹è¼‰çµæœã€‚")