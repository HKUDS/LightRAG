import sys
import os
import json
import time
import glob
import asyncio 
from loguru import logger
from collections import defaultdict
from dotenv import load_dotenv
from functools import partial

# === å¼·åˆ¶åŠ å…¥æœ¬åœ°è·¯å¾‘ ===
current_path = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_path)))

if project_root not in sys.path:
    sys.path.insert(0, project_root)
# ========================

# è®€å– .env
load_dotenv()

# === ğŸ”¥ é»‘åå–®è¨­å®š ===
SKIP_FILES = [
    "SFC",
    "Example_Doc_To_Skip"
]
# ====================

# å¼•å…¥ LightRAG
try:
    import lightrag 
    from lightrag import LightRAG
    from lightrag.utils import EmbeddingFunc
    from lightrag.llm.openai import azure_openai_complete, openai_embed
    
    logger.info("âœ… æˆåŠŸè¼‰å…¥ LightRAG")
    
except ImportError as e:
    logger.error(f"âŒ æ‰¾ä¸åˆ° LightRAG æˆ–ç›¸é—œæ¨¡çµ„: {e}")
    sys.exit(1)

# è¨­å®š Log
LOG_DIR = "./logs"
if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)
logger.remove()
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>", level="INFO")
logger.add(os.path.join(LOG_DIR, f"step3_multi_run_{time.strftime('%Y%m%d_%H%M%S')}.log"), rotation="10 MB", encoding="utf-8")

# è¨­å®šè·¯å¾‘
WORKING_DIR = "./data/rag_storage"
STEP2_BASE_DIR = "./data/output/step2_output_granular"

# è‡ªå‹•é…ç½®è®€å–
ENV_LLM_MODEL = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o-mini") 
ENV_EMBED_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")

SF_API_KEY = os.getenv("OPENAI_API_KEY") or os.getenv("EMBEDDING_BINDING_API_KEY") or os.getenv("SILICONFLOW_API_KEY")
SF_BASE_URL = os.getenv("OPENAI_BASE_URL") or os.getenv("EMBEDDING_BINDING_HOST") or "https://api.siliconflow.cn/v1"

if not SF_API_KEY:
    logger.error("âŒ æ‰¾ä¸åˆ° API Keyï¼è«‹æª¢æŸ¥ .env")
    sys.exit(1)

async def main():
    if not os.path.exists(STEP2_BASE_DIR):
        logger.error(f"âŒ æ‰¾ä¸åˆ° Step 2 è¼¸å‡ºç›®éŒ„: {STEP2_BASE_DIR}")
        return

    all_json_files = glob.glob(os.path.join(STEP2_BASE_DIR, "*", "granular_content.json"))
    
    if not all_json_files:
        logger.error("âŒ æ‰¾ä¸åˆ°ä»»ä½• granular_content.json")
        return

    if not os.path.exists(WORKING_DIR): os.makedirs(WORKING_DIR)
    
    logger.info("ğŸš€ åˆå§‹åŒ– LightRAG (Azure + SiliconFlow)...")
    
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=azure_openai_complete,
        embedding_func=EmbeddingFunc(
            embedding_dim=1024, 
            max_token_size=512,
            func=partial(
                openai_embed.func, 
                model=ENV_EMBED_MODEL,
                api_key=SF_API_KEY,      
                base_url=SF_BASE_URL     
            )
        ),
        chunk_token_size=512, 
        chunk_overlap_token_size=50,
        embedding_func_max_async=1, # é™é€Ÿé¿å… 403
        max_parallel_insert=1       # é™é€Ÿé¿å… 403
    )

    logger.info("âš™ï¸ æ­£åœ¨åˆå§‹åŒ– Storage...")
    await rag.initialize_storages()

    logger.info(f"ğŸ“¦ ç™¼ç¾ {len(all_json_files)} ä»½æ–‡ä»¶ï¼Œé–‹å§‹æ‰¹é‡æ³¨å…¥...")

    total_files = len(all_json_files)
    
    for i, json_file_path in enumerate(all_json_files):
        doc_label = os.path.basename(os.path.dirname(json_file_path))
        
        if doc_label in SKIP_FILES:
            logger.warning(f"ğŸš« [{i+1}/{total_files}] è·³éé»‘åå–®æ–‡ä»¶: {doc_label}")
            continue

        logger.info(f"\nğŸ“„ [File {i+1}/{total_files}] è™•ç†ä¸­: {doc_label}")
        
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                blocks = json.load(f)
        except Exception as e:
            logger.error(f"âŒ è®€å– JSON å¤±æ•— ({doc_label}): {e}")
            continue

        # æ•´ç†é é¢å…§å®¹
        pages_map = defaultdict(str)
        for block in blocks:
            page_num = block.get('page', 'Unknown')
            content = block.get('content', '').strip()
            if not content: continue
            
            sep = "\n\n" if block.get('type') in ['table', 'image'] else "\n"
            pages_map[page_num] += f"{sep}{content}{sep}"

        sorted_pages = sorted(pages_map.items(), key=lambda x: int(x[0]) if isinstance(x[0], int) or str(x[0]).isdigit() else 9999)
        
        # === ğŸ”¥ [æ–°é‚è¼¯] å‹•æ…‹åˆä½µ Buffer ===
        TARGET_CHUNK_SIZE = 3000  # ç›®æ¨™å­—å…ƒæ•¸ï¼Œç´„ 1000-1500 tokens
        
        buffer_content = ""
        start_page = -1
        end_page = -1
        chunk_count = 0
        total_pages = len(pages_map)

        logger.info(f"   â†³ å…±æœ‰ {total_pages} é ï¼Œæ­£åœ¨é€²è¡Œèªç¾©åˆä½µå¯«å…¥ (Target Size: {TARGET_CHUNK_SIZE})...")

        for j, (page_num, page_content) in enumerate(sorted_pages):
            # ç•¥éå¤ªçŸ­çš„é é¢ï¼ˆå¯èƒ½æ˜¯é›œè¨Šï¼‰
            if len(page_content) < 10: continue
            
            # åˆå§‹åŒ–ç•¶å‰ Buffer çš„èµ·å§‹é ç¢¼
            if start_page == -1:
                start_page = page_num
            
            # åŠ å…¥é é¢åˆ†éš”ç¬¦è™Ÿï¼Œå¹«åŠ© LLM è­˜åˆ¥
            sep = f"\n\n--- Page {page_num} ---\n\n" if buffer_content else f"--- Page {page_num} ---\n"
            buffer_content += sep + page_content
            end_page = page_num

            # åˆ¤æ–·æ˜¯å¦å¯«å…¥ (Buffer å¤ å¤§ æˆ– å·²ç¶“æ˜¯æœ€å¾Œä¸€é )
            is_last_page = (j == len(sorted_pages) - 1)
            
            if len(buffer_content) >= TARGET_CHUNK_SIZE or is_last_page:
                
                # ç”¢ç”Ÿ Source ID (e.g., "HSBC <Page 1-3>")
                if start_page == end_page:
                    page_range_str = f"<Page {start_page}>"
                else:
                    page_range_str = f"<Page {start_page}-{end_page}>"
                
                source_id = f"{doc_label} {page_range_str}"
                final_text = f"Source: {source_id}\n\n{buffer_content}"

                try:
                    await rag.ainsert(final_text, file_paths=source_id)
                    chunk_count += 1
                    logger.info(f"     âœ… æ³¨å…¥å€å¡Š: {source_id} (Size: {len(buffer_content)})")
                except Exception as e:
                    logger.error(f"     âŒ æ³¨å…¥å¤±æ•— ({source_id}): {e}")

                # === é‡ç½® Buffer (ä¿ç•™ Overlap) ===
                # ç°¡å–®çš„ Sliding Windowï¼šä¿ç•™æœ€å¾Œä¸€é ä½œç‚ºä¸‹ä¸€å€‹å€å¡Šçš„ä¸Šä¸‹æ–‡
                OVERLAP_SIZE = 500
                if len(page_content) > OVERLAP_SIZE and not is_last_page:
                    buffer_content = f"...(Context from Page {end_page})\n{page_content[-OVERLAP_SIZE:]}"
                else:
                    buffer_content = ""
                
                # é‡ç½® start_pageï¼Œè®“ä¸‹ä¸€åœˆè¿´åœˆè¨­å®šæ–°çš„èµ·å§‹é 
                start_page = -1

        logger.success(f"âœ… æ–‡ä»¶ {doc_label} å®Œæˆï¼å…±ç”¢ç”Ÿ {chunk_count} å€‹åˆä½µå€å¡Š")

    logger.info("\n" + "="*40)
    logger.success(f"ğŸ‰ æ‰€æœ‰æ–‡ä»¶è™•ç†å®Œç•¢ï¼")
    logger.info(f"ğŸ’¾ çŸ¥è­˜åº«ä½ç½®: {WORKING_DIR}")
    logger.info("="*40)

if __name__ == "__main__":
    asyncio.run(main())