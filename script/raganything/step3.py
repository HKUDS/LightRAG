import sys
import os
import json
import time
from loguru import logger
from collections import defaultdict
from dotenv import load_dotenv

# 1. è®€å– .env
load_dotenv()

# å¼•å…¥ LightRAG
try:
    sys.path.insert(0, os.path.abspath("..")) 
    from lightrag import LightRAG
    from lightrag.utils import EmbeddingFunc
    # å¼•å…¥å®˜æ–¹å‡½æ•¸
    from lightrag.llm import azure_openai_complete, openai_embedding
    logger.info("âœ… LightRAG Library è¼‰å…¥æˆåŠŸ")
except ImportError:
    logger.error("âŒ æ‰¾ä¸åˆ° LightRAG")
    sys.exit(1)

# è¨­å®šè·¯å¾‘
WORKING_DIR = "./rag_storage"
INPUT_JSON = "./data/output/step2_output_granular/granular_content.json"

# === è‡ªå‹•é…ç½®è®€å– (Auto-Configuration) ===
# é€™è£¡æˆ‘å€‘æ¨¡ä»¿ config.py çš„è¡Œç‚ºï¼Œè‡ªå‹•å¾ env è®€å–æ¨¡å‹åç¨±
# å¦‚æœ .env ç„¡å¯«ï¼Œå°±ç”¨ Default å€¼
ENV_LLM_MODEL = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o-mini") 
ENV_EMBED_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")

def main():
    if not os.path.exists(INPUT_JSON): return
    if not os.path.exists(WORKING_DIR): os.makedirs(WORKING_DIR)
    
    logger.info("ğŸš€ åˆå§‹åŒ– LightRAG (Env-Driven Mode)...")
    logger.info(f"ğŸ“‹ ä½¿ç”¨ LLM: {ENV_LLM_MODEL}")
    logger.info(f"ğŸ“‹ ä½¿ç”¨ Embedding: {ENV_EMBED_MODEL}")

    rag = LightRAG(
        working_dir=WORKING_DIR,
        
        # LLM å‡½æ•¸ (Azure): å®ƒæœƒè‡ªå‹•è®€ AZURE_OPENAI_API_KEY ç­‰ç’°å¢ƒè®Šæ•¸
        llm_model_func=azure_openai_complete,  
        
        # Embedding å‡½æ•¸ (SiliconFlow/OpenAI): å®ƒæœƒè‡ªå‹•è®€ OPENAI_API_KEY ç­‰ç’°å¢ƒè®Šæ•¸
        embedding_func=EmbeddingFunc(
            embedding_dim=1024, 
            max_token_size=512,  
            func=openai_embedding, 
            
            # ğŸŒŸ é€™è£¡ç›´æ¥è®€å– .env çš„ EMBEDDING_MODELï¼Œä¸ç”¨å¯«æ­»ï¼
            model=ENV_EMBED_MODEL   
        ),
        
        chunk_token_size=512, 
        chunk_overlap_token_size=50
    )

    # === ä»¥ä¸‹é‚è¼¯ä¸è®Š (è®€å– JSON -> åˆä½µ -> æ³¨å…¥) ===
    logger.info(f"ğŸ“‚ è®€å– JSON: {INPUT_JSON}")
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        blocks = json.load(f)

    # ç°¡å–®ä¼°å
    doc_label = "Document"
    try:
        if blocks and blocks[0].get("original_img"):
            p = blocks[0]["original_img"].split(os.sep)
            if len(p) > 1: doc_label = p[-3] if "images" in p else p[0]
    except: pass
    
    logger.info(f"ğŸ·ï¸ æ–‡ä»¶æ¨™ç±¤: {doc_label}")

    pages_map = defaultdict(str)
    for block in blocks:
        page_num = block.get('page', 'Unknown')
        content = block.get('content', '').strip()
        if not content: continue
        sep = "\n\n" if block.get('type') in ['table', 'image'] else "\n"
        pages_map[page_num] += f"{sep}{content}{sep}"

    sorted_pages = sorted(pages_map.items(), key=lambda x: int(x[0]) if isinstance(x[0], int) or str(x[0]).isdigit() else 9999)
    success_count = 0

    for page_num, full_content in sorted_pages:
        if len(full_content) < 10: continue
        source_id = f"{doc_label} <Page {page_num}>"
        final_text = f"Source: {source_id}\n\n{full_content}"
        try:
            rag.insert(final_text, custom_file_path=source_id)
            success_count += 1
            if success_count % 5 == 0: logger.info(f"â³ å·²æ³¨å…¥ {success_count} é ...")
        except Exception as e:
            logger.error(f"âŒ Error: {e}")

    logger.success(f"ğŸ‰ å®Œæˆï¼å…±æ³¨å…¥ {success_count} é ")

if __name__ == "__main__":
    main()