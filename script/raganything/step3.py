import sys
import os
import json
import time
import glob
import asyncio 
from loguru import logger
from collections import defaultdict
from dotenv import load_dotenv
from functools import partial

# === å¼·åˆ¶åŠ å…¥æœ¬åœ°è·¯å¾‘ ===
current_path = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_path)))

if project_root not in sys.path:
    sys.path.insert(0, project_root)
# ========================

# è®€å– .env
load_dotenv()

# === ğŸ”¥ [æ–°å¢] é»‘åå–®è¨­å®š ===
# å¡«å…¥ä¸æƒ³æ³¨å…¥çš„æ–‡ä»¶è³‡æ–™å¤¾åç¨± (å³ doc_label)
# ä¾‹å¦‚: ["SFC", "Old_Report_2023"]
SKIP_FILES = [
    "SFC",
    "Example_Doc_To_Skip"
]
# ============================

# å¼•å…¥ LightRAG
try:
    import lightrag 
    from lightrag import LightRAG
    from lightrag.utils import EmbeddingFunc
    # å¼•å…¥å®˜æ–¹å‡½æ•¸
    from lightrag.llm.openai import azure_openai_complete, openai_embed
    
    logger.info("âœ… æˆåŠŸè¼‰å…¥ LightRAG")
    logger.info(f"ğŸ“ LightRAG ä¾†æº: {os.path.dirname(lightrag.__file__)}")
    
except ImportError as e:
    logger.error(f"âŒ æ‰¾ä¸åˆ° LightRAG æˆ–ç›¸é—œæ¨¡çµ„: {e}")
    sys.exit(1)

# è¨­å®š Log
LOG_DIR = "./logs"
if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)
logger.remove()
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>", level="INFO")
logger.add(os.path.join(LOG_DIR, f"step3_multi_run_{time.strftime('%Y%m%d_%H%M%S')}.log"), rotation="10 MB", encoding="utf-8")

# è¨­å®šè·¯å¾‘
WORKING_DIR = "./data/rag_storage"
STEP2_BASE_DIR = "./data/output/step2_output_granular"

# è‡ªå‹•é…ç½®è®€å–
ENV_LLM_MODEL = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o-mini") 
ENV_EMBED_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")

# æ‰‹å‹•ç²å– SiliconFlow çš„ Key å’Œ URL
SF_API_KEY = os.getenv("OPENAI_API_KEY") or os.getenv("EMBEDDING_BINDING_API_KEY") or os.getenv("SILICONFLOW_API_KEY")
SF_BASE_URL = os.getenv("OPENAI_BASE_URL") or os.getenv("EMBEDDING_BINDING_HOST") or "https://api.siliconflow.cn/v1"

if not SF_API_KEY:
    logger.error("âŒ æ‰¾ä¸åˆ° API Keyï¼è«‹æª¢æŸ¥ .env æ˜¯å¦åŒ…å« OPENAI_API_KEY æˆ– SILICONFLOW_API_KEY")
    sys.exit(1)

async def main():
    if not os.path.exists(STEP2_BASE_DIR):
        logger.error(f"âŒ æ‰¾ä¸åˆ° Step 2 è¼¸å‡ºç›®éŒ„: {STEP2_BASE_DIR}")
        return

    all_json_files = glob.glob(os.path.join(STEP2_BASE_DIR, "*", "granular_content.json"))
    
    if not all_json_files:
        logger.error("âŒ æ‰¾ä¸åˆ°ä»»ä½• granular_content.json")
        return

    if not os.path.exists(WORKING_DIR): os.makedirs(WORKING_DIR)
    
    logger.info("ğŸš€ åˆå§‹åŒ– LightRAG (Azure + SiliconFlow)...")
    logger.info(f"ğŸ“‹ LLM: {ENV_LLM_MODEL} | Embedding: {ENV_EMBED_MODEL}")
    logger.info(f"ğŸ”Œ Embedding Endpoint: {SF_BASE_URL}")

    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=azure_openai_complete,
        
        # æ˜ç¢ºå‚³å…¥ api_key å’Œ base_url
        embedding_func=EmbeddingFunc(
            embedding_dim=1024, 
            max_token_size=512,
            func=partial(
                openai_embed.func, 
                model=ENV_EMBED_MODEL,
                api_key=SF_API_KEY,      
                base_url=SF_BASE_URL     
            )
        ),
        chunk_token_size=512, 
        chunk_overlap_token_size=50
    )

    logger.info("âš™ï¸ æ­£åœ¨åˆå§‹åŒ– Storage...")
    await rag.initialize_storages()

    logger.info(f"ğŸ“¦ ç™¼ç¾ {len(all_json_files)} ä»½æ–‡ä»¶ï¼Œé–‹å§‹æ‰¹é‡æ³¨å…¥...")

    total_files = len(all_json_files)
    
    for i, json_file_path in enumerate(all_json_files):
        # å–å¾—è³‡æ–™å¤¾åç¨±ä½œç‚º doc_label (ä¾‹å¦‚ "SFC")
        doc_label = os.path.basename(os.path.dirname(json_file_path))
        
        # === ğŸš« Blacklist Check (æ–°å¢æª¢æŸ¥é‚è¼¯) ===
        if doc_label in SKIP_FILES:
            logger.warning(f"ğŸš« [{i+1}/{total_files}] è·³éé»‘åå–®æ–‡ä»¶: {doc_label}")
            continue
        # ========================================

        logger.info(f"\nğŸ“„ [File {i+1}/{total_files}] è™•ç†ä¸­: {doc_label}")
        
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                blocks = json.load(f)
        except Exception as e:
            logger.error(f"âŒ è®€å– JSON å¤±æ•— ({doc_label}): {e}")
            continue

        pages_map = defaultdict(str)
        for block in blocks:
            page_num = block.get('page', 'Unknown')
            content = block.get('content', '').strip()
            if not content: continue
            
            sep = "\n\n" if block.get('type') in ['table', 'image'] else "\n"
            pages_map[page_num] += f"{sep}{content}{sep}"

        sorted_pages = sorted(pages_map.items(), key=lambda x: int(x[0]) if isinstance(x[0], int) or str(x[0]).isdigit() else 9999)
        file_success_count = 0
        total_pages = len(pages_map)

        logger.info(f"   â†³ å…±æœ‰ {total_pages} é ï¼Œæ­£åœ¨å¯«å…¥ Graph...")

        for page_num, full_content in sorted_pages:
            if len(full_content) < 10: continue
            
            source_id = f"{doc_label} <Page {page_num}>"
            final_text = f"Source: {source_id}\n\n{full_content}"

            try:
                await rag.ainsert(final_text, file_paths=source_id)
                
                file_success_count += 1
                if file_success_count % 10 == 0: 
                    logger.info(f"     â³ å·²æ³¨å…¥ {file_success_count}/{total_pages} é ...")
            except Exception as e:
                logger.error(f"     âŒ æ³¨å…¥å¤±æ•— (Page {page_num}): {e}")

        logger.success(f"âœ… æ–‡ä»¶ {doc_label} å®Œæˆï¼å…±æ³¨å…¥ {file_success_count} é ")

    logger.info("\n" + "="*40)
    logger.success(f"ğŸ‰ æ‰€æœ‰æ–‡ä»¶è™•ç†å®Œç•¢ï¼")
    logger.info(f"ğŸ’¾ çŸ¥è­˜åº«ä½ç½®: {WORKING_DIR}")
    logger.info("="*40)

if __name__ == "__main__":
    asyncio.run(main())