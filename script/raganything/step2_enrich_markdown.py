import os
import re
import time
import base64
import sys
import glob
from loguru import logger
from dotenv import load_dotenv
from openai import AzureOpenAI

# å¼·åˆ¶è¼‰å…¥ .env
load_dotenv()

# === 1. è¨­å®šå€ (Configuration) ===
# Azure OpenAI è¨­å®š
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
AZURE_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o") 

# è¼¸å…¥èˆ‡è¼¸å‡ºè·¯å¾‘
INPUT_BASE_DIR = "./data/output/step1_vlm_output"
OUTPUT_SUFFIX = "_enriched" # è™•ç†å¾Œçš„æª”æ¡ˆæœƒåŠ ä¸Šé€™å€‹å¾Œç¶´ï¼Œä¾‹å¦‚ doc_enriched.md

# Log è¨­å®š
LOG_DIR = "./logs"
if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)
logger.remove()
logger.add(sys.stderr, level="INFO")
logger.add(os.path.join(LOG_DIR, f"markdown_enrich_{time.strftime('%Y%m%d_%H%M%S')}.log"), rotation="10 MB", encoding="utf-8")

# ============

HAS_AI = False
ai_client = None

try:
    if AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT:
        ai_client = AzureOpenAI(
            api_key=AZURE_OPENAI_API_KEY,
            api_version=AZURE_OPENAI_API_VERSION,
            azure_endpoint=AZURE_OPENAI_ENDPOINT
        )
        HAS_AI = True
        logger.info(f"âœ… Azure OpenAI å·²å•Ÿç”¨ (Deployment: {AZURE_DEPLOYMENT_NAME})")
    else:
        logger.warning("âš ï¸ æœªè¨­å®š Azure API Key æˆ– Endpointï¼ŒAI åŠŸèƒ½å°‡è·³é")
except ImportError:
    logger.error("âš ï¸ ç¼ºå°‘ openai å¥—ä»¶ï¼Œè«‹åŸ·è¡Œ: pip install openai")

def encode_image(image_path):
    if not os.path.exists(image_path):
        return None
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def generate_image_description(img_path, context_text=""):
    """å‘¼å« Azure OpenAI ç”Ÿæˆåœ–ç‰‡æè¿°"""
    if not HAS_AI: return None
    
    base64_image = encode_image(img_path)
    if not base64_image: 
        logger.warning(f"   âš ï¸ æ‰¾ä¸åˆ°åœ–ç‰‡æˆ–ç„¡æ³•è®€å–: {img_path}")
        return None

    try:
        system_prompt = "You are a helpful assistant assisting in document digitization. Your task is to provide a concise but descriptive summary of the image provided."
        user_msg = "Describe this image in detail. If it is a chart, summarize the key trends. If it is a diagram, explain its components. Output plain text only."
        
        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥åŠ å¼· Prompt
        if context_text:
            user_msg += f"\n\nContext surrounding this image:\n{context_text[:500]}"

        response = ai_client.chat.completions.create(
            model=AZURE_DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": [
                    {"type": "text", "text": user_msg},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]}
            ],
            temperature=0.3, 
            max_tokens=1024
        )
        content = response.choices[0].message.content.strip()
        
        # æ¸…ç†å¯èƒ½å‡ºç¾çš„ markdown code block
        if content.startswith("```"):
            content = content.replace("```markdown", "").replace("```", "").strip()
            
        return content

    except Exception as e:
        logger.error(f"âŒ Azure API Error on {os.path.basename(img_path)}: {e}")
        return None

def process_single_markdown(md_file_path):
    """è™•ç†å–®å€‹ Markdown æª”æ¡ˆ"""
    file_dir = os.path.dirname(md_file_path)
    file_name = os.path.basename(md_file_path)
    file_stem = os.path.splitext(file_name)[0]
    
    # å®šç¾©è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
    output_path = os.path.join(file_dir, f"{file_stem}{OUTPUT_SUFFIX}.md")
    
    # å¦‚æœå·²ç¶“è™•ç†éï¼Œå¯ä»¥é¸æ“‡è·³é (é€™è£¡è¨­ç‚º True å‰‡è·³é)
    if os.path.exists(output_path):
        logger.info(f"â­ï¸ æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³é: {output_path}")
        return

    logger.info(f"ğŸ“– è®€å–æª”æ¡ˆ: {md_file_path}")
    
    with open(md_file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # Regex å°‹æ‰¾ Markdown åœ–ç‰‡èªæ³•: ![alt text](image_path)
    # capture group 1: alt text
    # capture group 2: image path
    img_pattern = re.compile(r'!\[(.*?)\]\((.*?)\)')
    
    # æ‰¾å‡ºæ‰€æœ‰åœ–ç‰‡é€£çµ
    matches = list(img_pattern.finditer(content))
    
    if not matches:
        logger.info("   â„¹ï¸ æ­¤æ–‡ä»¶æ²’æœ‰ç™¼ç¾åœ–ç‰‡")
        return

    logger.info(f"   ğŸ” ç™¼ç¾ {len(matches)} å¼µåœ–ç‰‡ï¼Œé–‹å§‹ç”Ÿæˆæè¿°...")
    
    # æˆ‘å€‘ä½¿ç”¨ã€Œæ›¿æ›ã€çš„æ–¹å¼ï¼Œç‚ºäº†é¿å…æ›¿æ›å¾Œ offset è·‘æ‰ï¼Œæˆ‘å€‘å¾å¾Œé¢å¾€å‰è™•ç†ï¼Œæˆ–è€…é‡å»ºå­—ä¸²
    # é€™è£¡ä½¿ç”¨é‡å»ºå­—ä¸²çš„æ–¹å¼æ¯”è¼ƒå®‰å…¨
    
    new_content = content
    # çµ±è¨ˆç”¨
    processed_count = 0
    
    # ç‚ºäº†é¿å…å¤šæ¬¡æ›¿æ›å°è‡´æ··äº‚ï¼Œæˆ‘å€‘å»ºç«‹ä¸€å€‹æ›¿æ›æ¸…å–®
    replacements = {}

    for match in matches:
        alt_text = match.group(1)
        rel_img_path = match.group(2)
        full_match_str = match.group(0)
        
        # çµ„åˆåœ–ç‰‡çš„çµ•å°è·¯å¾‘ (Mineru çš„åœ–ç‰‡è·¯å¾‘é€šå¸¸æ˜¯ç›¸å°çš„)
        abs_img_path = os.path.join(file_dir, rel_img_path)
        
        # å–å¾—åœ–ç‰‡å‘¨åœçš„ä¸Šä¸‹æ–‡ (å‰å¾Œ 200 å­—)
        start_idx = match.start()
        context_text = content[max(0, start_idx-200) : min(len(content), start_idx+200)]

        # å‘¼å« AI ç”Ÿæˆæè¿°
        logger.info(f"   ğŸ–¼ï¸ æ­£åœ¨åˆ†æåœ–ç‰‡: {rel_img_path}")
        description = generate_image_description(abs_img_path, context_text)
        
        if description:
            # æ§‹å»ºæ–°çš„ Markdown å€å¡Š
            # æ ¼å¼:
            # ![alt](path)
            # > **AI Description:** ...
            new_block = f"{full_match_str}\n\n> **AI Image Description:** {description}\n"
            replacements[full_match_str] = new_block
            processed_count += 1
            
            # ä¼‘æ¯ä¸€ä¸‹é¿å… Rate Limit
            time.sleep(1) 
        else:
            replacements[full_match_str] = full_match_str # æ²’æè¿°å°±ä¿æŒåŸæ¨£

    # åŸ·è¡Œæ›¿æ› (ä¸€æ¬¡æ€§æ›¿æ›æ‰€æœ‰åœ–ç‰‡)
    # æ³¨æ„ï¼šå¦‚æœæœ‰å¤šå€‹ç›¸åŒçš„åœ–ç‰‡æ¨™ç±¤ï¼Œé€™è£¡æœƒå…¨éƒ¨æ›¿æ›
    for old_str, new_str in replacements.items():
        new_content = new_content.replace(old_str, new_str)

    # å¯«å…¥æ–°æª”æ¡ˆ
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(new_content)

    logger.success(f"âœ… å®Œæˆï¼å·²è™•ç† {processed_count}/{len(matches)} å¼µåœ–ç‰‡")
    logger.info(f"   ğŸ’¾ å„²å­˜è‡³: {output_path}")

def main():
    # 1. æœå°‹æ‰€æœ‰ Markdown æª”æ¡ˆ
    # è·¯å¾‘æ¨¡å¼: data/output/step1_vlm_output/<folder>/vlm/<file>.md
    # ä½¿ç”¨éè¿´æœå°‹
    search_pattern = os.path.join(INPUT_BASE_DIR, "**", "*.md")
    all_md_files = glob.glob(search_pattern, recursive=True)
    
    # éæ¿¾æ‰å·²ç¶“æ˜¯ _enriched çš„æª”æ¡ˆï¼Œé¿å…é‡è¤‡è™•ç†
    target_files = [f for f in all_md_files if OUTPUT_SUFFIX not in f and "README" not in f]

    if not target_files:
        logger.error(f"âŒ åœ¨ {INPUT_BASE_DIR} æ‰¾ä¸åˆ°ä»»ä½• Markdown (.md) æª”æ¡ˆ")
        return

    logger.info(f"ğŸ“¦ ç¸½å…±ç™¼ç¾ {len(target_files)} å€‹ Markdown æª”æ¡ˆå¾…è™•ç†")

    for i, md_path in enumerate(target_files):
        logger.info(f"\nğŸš€ [{i+1}/{len(target_files)}] è™•ç†æ–‡ä»¶...")
        process_single_markdown(md_path)

    logger.success("\nğŸ‰ æ‰€æœ‰ Markdown æª”æ¡ˆè™•ç†å®Œç•¢ï¼")

if __name__ == "__main__":
    main()