import sys
import os
import json
import glob
import time
import requests
from loguru import logger
from dotenv import load_dotenv

# === 1. è¨­å®š Logging ===
LOG_DIR = "./logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

log_file = os.path.join(LOG_DIR, f"step1_api_run_{time.strftime('%Y%m%d_%H%M%S')}.log")
logger.remove() 
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>", level="INFO")
logger.add(log_file, rotation="10 MB", level="DEBUG", encoding="utf-8")
logger.info(f"ğŸ“ Log æª”æ¡ˆå·²å»ºç«‹: {log_file}")

# è®€å– .env
load_dotenv()

# === API è¨­å®š ===
API_TOKEN = os.getenv("MINERU_API_TOKEN")
UPLOAD_APPLY_URL = "https://mineru.net/api/v4/file-urls/batch"
QUERY_BASE_URL = "https://mineru.net/api/v4/extract/task"
# =================

def process_single_file_via_api(input_path, step1_std_base_dir):
    """
    è™•ç†å–®å€‹æª”æ¡ˆï¼š
    1. å¦‚æœæ˜¯ TXT/MD -> æœ¬åœ°è™•ç† (Bypass API)
    2. å¦‚æœæ˜¯ PDF -> å‘¼å« Mineru API (å«æ–·ç·šé‡é€£æ©Ÿåˆ¶)
    """
    filename = os.path.basename(input_path)
    file_stem = os.path.splitext(filename)[0]
    ext = os.path.splitext(filename)[1].lower()
    
    # å»ºç«‹å°ˆå±¬è³‡æ–™å¤¾
    current_file_std_dir = os.path.join(step1_std_base_dir, file_stem)
    if not os.path.exists(current_file_std_dir):
        os.makedirs(current_file_std_dir)
    
    final_json_path = os.path.join(current_file_std_dir, "intermediate_result.json")

    logger.info("-" * 40)
    logger.info(f"ğŸš€ [Start] è™•ç†: {filename}")

    # === [ç­–ç•¥ A] ç´”æ–‡å­—æª” Bypass (çœéŒ¢/çœæ™‚é–“) ===
    if ext in ['.txt', '.md']:
        logger.info(f"ğŸ“„ ç´”æ–‡å­—æª” ({ext}) -> è·³é APIï¼Œç›´æ¥è½‰æ›...")
        try:
            with open(input_path, "r", encoding="utf-8") as f:
                content = f.read()
            
            mock_data = [{
                "type": "text",
                "text": content,
                "page_idx": 0,
                "bbox": [0, 0, 0, 0], 
                "img_path": ""
            }]
            
            with open(final_json_path, "w", encoding="utf-8") as f:
                json.dump(mock_data, f, ensure_ascii=False, indent=2)
                
            logger.success(f"âœ… [Text Mode] è½‰æ›æˆåŠŸï¼")
            return True
        except Exception as e:
            logger.error(f"âŒ è®€å–æ–‡å­—æª”å¤±æ•—: {e}")
            return False

    # === [ç­–ç•¥ B] PDF/å…¶ä»– -> ä½¿ç”¨ Mineru API ===
    if not API_TOKEN:
        logger.error("âŒ æ‰¾ä¸åˆ° MINERU_API_TOKENï¼Œè«‹æª¢æŸ¥ .env")
        return False

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_TOKEN}"
    }

    try:
        # 1. ç”³è«‹ä¸Šå‚³é€£çµ
        logger.info("ğŸ“¡ 1. ç”³è«‹ API ä¸Šå‚³é€£çµ...")
        apply_data = {
            "files": [{"name": filename, "data_id": f"local_{int(time.time())}"}],
            "model_version": "vlm"
        }
        
        # åŠ å…¥ timeout é˜²æ­¢å¡æ­»
        res = requests.post(UPLOAD_APPLY_URL, headers=headers, json=apply_data, timeout=30)
        res.raise_for_status()
        res_json = res.json()
        
        if res_json.get("code") != 0:
            logger.error(f"âŒ API ç”³è«‹å¤±æ•—: {res_json.get('msg')}")
            return False

        batch_id = res_json["data"]["batch_id"]
        upload_url = res_json["data"]["file_urls"][0]
        logger.info(f"   Batch ID: {batch_id}")

        # 2. ä¸Šå‚³æª”æ¡ˆ
        logger.info("ğŸ“¤ 2. ä¸Šå‚³æª”æ¡ˆè‡³ Mineru...")
        with open(input_path, 'rb') as f:
            # ä¸Šå‚³é€šå¸¸æ¯”è¼ƒä¹…ï¼Œtimeout è¨­é•·ä¸€é» (ä¾‹å¦‚ 300ç§’)
            upload_res = requests.put(upload_url, data=f, timeout=300)
            if upload_res.status_code != 200:
                logger.error(f"âŒ ä¸Šå‚³å¤±æ•— (Code {upload_res.status_code})")
                return False

        # 3. è¼ªè©¢ç‹€æ…‹ (ğŸ”¥ åŠ å…¥æ–·ç·šé‡é€£æ©Ÿåˆ¶ ğŸ”¥)
        logger.info("â³ 3. ç­‰å¾…ä¼ºæœå™¨è§£æ...")
        query_url = f"{QUERY_BASE_URL}/{batch_id}"
        
        network_retry_count = 0
        MAX_RETRIES = 20  # æœ€å¤šå®¹è¨±é€£çºŒå¤±æ•— 20 æ¬¡ (ç´„ 10 åˆ†é˜æ–·ç¶²å®¹å¿)
        
        while True:
            try:
                # æŸ¥è©¢ç‹€æ…‹ (timeout=30)
                status_res = requests.get(query_url, headers=headers, timeout=30)
                status_res.raise_for_status()
                
                # æˆåŠŸé€£ç·šï¼Œé‡ç½®è¨ˆæ•¸å™¨
                network_retry_count = 0
                
                status_data = status_res.json().get("data", {})
                state = status_data.get("state")
                
                if state == "done":
                    break
                elif state == "failed":
                    logger.error("âŒ API è§£æä»»å‹™å›å ±å¤±æ•— (State: failed)")
                    return False
                else:
                    # æ­£åœ¨è™•ç†ä¸­ï¼Œæ­£å¸¸ç­‰å¾… 5 ç§’
                    time.sleep(5)

            except (requests.exceptions.RequestException, Exception) as e:
                network_retry_count += 1
                logger.warning(f"âš ï¸ ç¶²çµ¡é€£ç·šä¸ç©© ({network_retry_count}/{MAX_RETRIES}): {e}")
                
                if network_retry_count > MAX_RETRIES:
                    logger.error("âŒ é€£çºŒå¤šæ¬¡é€£ç·šå¤±æ•—ï¼Œæ”¾æ£„æ­¤æª”æ¡ˆã€‚")
                    return False
                
                # å¤±æ•—å¾Œä¼‘æ¯ 30 ç§’å†é‡è©¦ï¼Œçµ¦ç¶²çµ¡æ¢å¾©æ™‚é–“
                logger.info("ğŸ”„ ç¶²çµ¡ç•°å¸¸ï¼Œ30ç§’å¾Œå˜—è©¦é‡é€£...")
                time.sleep(30)

        # 4. ä¸‹è¼‰çµæœ
        logger.info("â¬‡ï¸ 4. ä¸‹è¼‰è§£æçµæœ...")
        result_links = status_data.get("links", [])
        content_json_url = None
        
        for link in result_links:
            if link.get("file_name", "").endswith("content_list.json"):
                content_json_url = link.get("url")
                break
        
        if not content_json_url and result_links:
             content_json_url = result_links[0].get("url")

        if content_json_url:
            # ä¸‹è¼‰ä¹Ÿè¦åŠ  timeout å’Œç°¡å–®é‡è©¦
            for _ in range(3):
                try:
                    content_res = requests.get(content_json_url, timeout=60)
                    content_res.raise_for_status()
                    extracted_data = content_res.json()
                    
                    with open(final_json_path, "w", encoding="utf-8") as f:
                        json.dump(extracted_data, f, ensure_ascii=False, indent=2)

                    logger.success(f"âœ… [API Mode] è§£ææˆåŠŸï¼")
                    logger.success(f"ğŸ’¾ å„²å­˜æ–¼: {final_json_path}")
                    return True
                except Exception as dl_err:
                    logger.warning(f"âš ï¸ ä¸‹è¼‰å¤±æ•—ï¼Œé‡è©¦ä¸­: {dl_err}")
                    time.sleep(5)
            
            logger.error("âŒ ä¸‹è¼‰çµæœå¤±æ•— (é‡è©¦ 3 æ¬¡å¾Œ)")
            return False
        else:
            logger.error("âŒ æ‰¾ä¸åˆ°çµæœä¸‹è¼‰éˆæ¥")
            return False

    except Exception as e:
        logger.exception(f"âš ï¸ API è™•ç†ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        return False

def main():
    # === è¨­å®šå€ ===
    input_dir = "./data/input/__enqueued__"
    
    # é€™æ˜¯ Step 2 è®€å–çš„ç›®éŒ„çµæ§‹
    step1_std_base_dir = "./data/input/step1_output" 
    
    FORCE_RERUN = False 
    EXCLUDE_FILES = ["sfc.pdf", "sfc_report.pdf"] 
    # ============

    if not os.path.exists(input_dir):
        logger.error(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥è³‡æ–™å¤¾: {input_dir}")
        return

    if not os.path.exists(step1_std_base_dir):
        os.makedirs(step1_std_base_dir)

    # æƒææ‰€æœ‰æª”æ¡ˆ
    all_entries = glob.glob(os.path.join(input_dir, "*"))
    files = []
    
    logger.info(f"ğŸ” æ­£åœ¨æƒæè³‡æ–™å¤¾: {input_dir}")
    
    for entry in all_entries:
        if os.path.isfile(entry):
            filename = os.path.basename(entry)
            if not filename.startswith("."):
                files.append(entry)

    if not files:
        logger.warning(f"ğŸ“‚ è³‡æ–™å¤¾å…§ç„¡æª”æ¡ˆ")
        return

    logger.info(f"ğŸ“¦ ç™¼ç¾ {len(files)} å€‹æª”æ¡ˆ...")
    
    success_count = 0
    fail_count = 0
    skipped_count = 0

    for i, file_path in enumerate(files):
        logger.info(f"\n[é€²åº¦: {i+1}/{len(files)}]")
        
        filename = os.path.basename(file_path)
        file_stem = os.path.splitext(filename)[0]

        # æ’é™¤æª¢æŸ¥
        if filename.lower() in EXCLUDE_FILES:
            logger.info(f"ğŸ›‘ è·³éæ’é™¤åå–®: {filename}")
            skipped_count += 1
            continue
        
        # æ–·é»çºŒå‚³æª¢æŸ¥
        expected_output = os.path.join(step1_std_base_dir, file_stem, "intermediate_result.json")
        if not FORCE_RERUN and os.path.exists(expected_output):
            logger.info(f"â­ï¸ æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³é: {filename}")
            skipped_count += 1
            success_count += 1
            continue
        
        # å‘¼å«è™•ç†å‡½å¼
        if process_single_file_via_api(file_path, step1_std_base_dir):
            success_count += 1
        else:
            fail_count += 1

    logger.info("\n" + "="*60)
    logger.info(f"ğŸ ä½œæ¥­å®Œæˆï¼æˆåŠŸ: {success_count} | è·³é: {skipped_count} | å¤±æ•—: {fail_count}")

if __name__ == "__main__":
    main()