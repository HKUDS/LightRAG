import os
import json
import time
import base64
import sys
import glob
from loguru import logger
from dotenv import load_dotenv  

# å¼·åˆ¶è¼‰å…¥ .env æª”æ¡ˆ
load_dotenv()

# === 1. è¨­å®šå€ (Configuration) ===
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY", "")
MODEL_NAME = "thudm/glm-4.1v-9b-thinking" 

# ğŸ”¥ [æ–°å¢] é»‘åå–®ï¼šå¡«å…¥ä¸æƒ³è™•ç†çš„æª”æ¡ˆåç¨± (è³‡æ–™å¤¾åç¨±/file_stem)
# ä¾‹å¦‚: ["SFC", "Another_Doc", "Old_Report"]
SKIP_FILES = [
    "SFC", 
    "Example_Doc_To_Skip"
]

# è¨­å®š Log ç›®éŒ„
LOG_DIR = "./logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

# è¨­å®š Loguru
log_file = os.path.join(LOG_DIR, f"step2_split_run_{time.strftime('%Y%m%d_%H%M%S')}.log")
logger.remove() 
logger.add(sys.stderr, level="INFO") 
logger.add(log_file, rotation="10 MB", level="DEBUG", encoding="utf-8")
logger.info(f"ğŸ“ Log æª”æ¡ˆå·²å»ºç«‹: {log_file}")

# ============

HAS_AI = False
ai_client = None

try:
    from openai import OpenAI
    if SILICONFLOW_API_KEY and "ä½ çš„_SILICONFLOW_KEY" not in SILICONFLOW_API_KEY:
        ai_client = OpenAI(
            api_key=SILICONFLOW_API_KEY,
            base_url="https://api.siliconflow.cn/v1"
        )
        HAS_AI = True
        logger.info(f"âœ… å·²å•Ÿç”¨ SiliconFlow AI ({MODEL_NAME})")
    else:
        logger.warning("âš ï¸ æœªå¡«å¯« SILICONFLOW_API_KEYï¼Œå°‡è·³é AI æè¿°åŠŸèƒ½")
except ImportError:
    logger.error("âš ï¸ ç¼ºå°‘ openai å¥—ä»¶")

def encode_image(image_path):
    if not os.path.exists(image_path): 
        logger.error(f"âŒ æ‰¾ä¸åˆ°åœ–ç‰‡: {image_path}")
        return None
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def call_vision_llm(img_path, mode="table", context_text=""):
    if not HAS_AI: return None
    
    base64_image = encode_image(img_path)
    if not base64_image: return None

    try:
        context_instruction = ""
        if context_text:
            context_instruction = f"""
            \n[Context Info]:
            The text surrounding this image/table is:
            " ... {context_text} ... "
            Instruction: Use this context to infer the title, subject, time period, or data units.
            """

        if mode == "table":
            system_prompt = "You are an expert OCR engine. Transcribe the table in the image into a clean Markdown table."
            user_msg = f"{context_instruction}\nTask: Output ONLY the markdown table content. Handle merged cells. No explanations."
        else: 
            system_prompt = "You are a helpful assistant describing images for a RAG system."
            user_msg = f"{context_instruction}\nTask: Provide a detailed description of this image. Extract key data points, trends, and the title."

        response = ai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": [
                    {"type": "text", "text": user_msg},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]}
            ],
            temperature=0.1, max_tokens=4096, stream=False
        )
        content = response.choices[0].message.content.strip()
        
        if "</think>" in content:
            content = content.split("</think>")[-1].strip()
            
        return content

    except Exception as e:
        logger.error(f"âŒ SiliconFlow API Error on {img_path}: {e}")
        return None

def main():
    # === è¨­å®šè·¯å¾‘ ===
    step1_base_dir = "./data/input/step1_output"
    output_base_dir = "./data/output/step2_output_granular"

    if not os.path.exists(step1_base_dir):
        logger.error(f"âŒ æ‰¾ä¸åˆ° Step 1 è¼¸å‡ºç›®éŒ„: {step1_base_dir}")
        return

    # æƒææ‰€æœ‰ input json
    all_json_files = glob.glob(os.path.join(step1_base_dir, "*", "intermediate_result.json"))
    
    if not all_json_files:
        logger.error(f"âŒ åœ¨ {step1_base_dir} æ‰¾ä¸åˆ°ä»»ä½• intermediate_result.json")
        return
        
    logger.info(f"ğŸ“¦ ç™¼ç¾ {len(all_json_files)} å€‹æª”æ¡ˆå¾…è™•ç†...")

    # === é€å€‹æª”æ¡ˆè™•ç† (Per-File Loop) ===
    for i, json_file_path in enumerate(all_json_files):
        # 1. æº–å‚™è·¯å¾‘å’Œè³‡æ–™å¤¾
        file_stem = os.path.basename(os.path.dirname(json_file_path))
        
        # === ğŸš« Blacklist Check (æ–°å¢æª¢æŸ¥é‚è¼¯) ===
        if file_stem in SKIP_FILES:
            logger.warning(f"ğŸš« [{i+1}/{len(all_json_files)}] è·³éé»‘åå–®æª”æ¡ˆ: {file_stem}")
            continue
        # ========================================

        current_base_dir = os.path.dirname(json_file_path)
        
        # å»ºç«‹å°ˆå±¬è¼¸å‡ºç›®éŒ„: output/doc_name/granular_content.json
        current_output_dir = os.path.join(output_base_dir, file_stem)
        current_output_path = os.path.join(current_output_dir, "granular_content.json")
        
        if not os.path.exists(current_output_dir):
            os.makedirs(current_output_dir)

        logger.info(f"\nğŸš€ [{i+1}/{len(all_json_files)}] æ­£åœ¨è™•ç†: {file_stem}")
        logger.info(f"   ğŸ“‚ è¼¸å‡ºä½ç½®: {current_output_path}")

        # 2. é‡å°ã€Œé€™å€‹æª”æ¡ˆã€çš„æ–·é»çºŒå‚³é‚è¼¯
        processed_blocks = [] # é€™æ¬¡åŸ·è¡Œè¦ç”¢ç”Ÿçš„å®Œæ•´åˆ—è¡¨
        existing_map = {}     # ç”¨ä¾†å¿«é€ŸæŸ¥æ‰¾èˆŠè³‡æ–™
        
        if os.path.exists(current_output_path):
            try:
                with open(current_output_path, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)
                    if isinstance(existing_data, list):
                        # å»ºç«‹ ID -> Block çš„å°ç…§è¡¨
                        for block in existing_data:
                            if "unique_id" in block:
                                existing_map[block["unique_id"]] = block
                        logger.info(f"   ğŸ”„ è¼‰å…¥èˆŠé€²åº¦: {len(existing_map)} ç­†è³‡æ–™ï¼Œå°‡æª¢æŸ¥å…§å®¹å®Œæ•´æ€§...")
            except Exception as e:
                logger.warning(f"   âš ï¸ è®€å–èˆŠæª”å¤±æ•—ï¼Œå°‡é‡æ–°é–‹å§‹: {e}")
                existing_map = {}

        # 3. è®€å–è¼¸å…¥èˆ‡è™•ç†
        with open(json_file_path, "r", encoding="utf-8") as f:
            content_list = json.load(f)

        stats = {"text": 0, "table": 0, "image": 0, "ai_processed": 0, "skipped": 0}

        for idx, item in enumerate(content_list):
            item_type = item.get('type')
            page_idx = item.get('page_idx', 0)
            
            raw_bbox = item.get('bbox') or item.get('rect')
            bbox = [int(b) for b in raw_bbox] if raw_bbox else None
            
            # ç”Ÿæˆ ID
            current_id = f"{file_stem}_{page_idx}_{str(bbox)}"

            # === ğŸ’¡ Skip Logic ===
            old_block = existing_map.get(current_id)
            should_skip = False

            if old_block:
                old_content = old_block.get("content", "").strip()
                
                # A: Text -> Skip
                if item_type == 'text':
                    should_skip = True
                
                # B: Image/Table -> Check Content Length
                elif item_type in ['table', 'image']:
                    if len(old_content) > 5:
                        should_skip = True
                    else:
                        logger.info(f"   âš ï¸ ç™¼ç¾èˆŠè³‡æ–™ä½†å…§å®¹ç‚ºç©ºï¼Œå°‡é‡æ–°åŸ·è¡Œ AI: P{page_idx+1} {item_type}")

            if should_skip:
                processed_blocks.append(old_block) # ç›´æ¥ä½¿ç”¨èˆŠçš„å€å¡Š
                stats["skipped"] += 1
                continue
            # =====================

            # å¦‚æœä¸ Skipï¼Œå°±æº–å‚™é€²è¡Œè™•ç†
            
            # æº–å‚™è·¯å¾‘
            rel_path = item.get('img_path', '')
            abs_img_path = None
            if rel_path:
                abs_img_path = os.path.join(current_base_dir, rel_path)

            block_data = {
                "type": item_type,
                "page": page_idx + 1,
                "bbox": bbox,
                "content": "",
                "original_img": rel_path,
                "source_file": file_stem,
                "unique_id": current_id 
            }

            # Context
            context_text = ""
            if idx > 0 and content_list[idx-1].get('type') == 'text':
                context_text += f"Pre: {content_list[idx-1].get('text', '')[-200:]}\n"

            # --- Process ---
            if item_type == 'text':
                text = item.get('text', '').strip()
                if text:
                    block_data["content"] = text
                    processed_blocks.append(block_data)
                    stats["text"] += 1

            elif item_type == 'table':
                logger.info(f"   ğŸ” [Table] P{page_idx+1}")
                content = item.get('table_body', '')
                if HAS_AI and abs_img_path and os.path.exists(abs_img_path):
                    ai_content = call_vision_llm(abs_img_path, mode="table", context_text=context_text)
                    if ai_content:
                        content = ai_content
                        stats["ai_processed"] += 1
                
                caption = "".join(item.get('table_caption', []))
                if caption: content = f"**Table Caption:** {caption}\n\n{content}"
                block_data["content"] = content
                processed_blocks.append(block_data)
                stats["table"] += 1

            elif item_type == 'image':
                logger.info(f"   ğŸ–¼ï¸ [Image] P{page_idx+1}")
                caption = "".join(item.get('image_caption', []))
                if HAS_AI and abs_img_path and os.path.exists(abs_img_path):
                    ai_desc = call_vision_llm(abs_img_path, mode="caption", context_text=context_text)
                    if ai_desc:
                        caption = f"{caption}\n**Image Description:** {ai_desc}".strip()
                        stats["ai_processed"] += 1
                
                block_data["content"] = caption
                processed_blocks.append(block_data)
                stats["image"] += 1

            # ğŸ’¾ Per-File Auto-Save
            new_processed_count = stats["text"] + stats["table"] + stats["image"]
            if new_processed_count > 0 and new_processed_count % 5 == 0:
                 with open(current_output_path, "w", encoding="utf-8") as f:
                    json.dump(processed_blocks, f, ensure_ascii=False, indent=2)

        # 4. å®Œæˆè©²æª”æ¡ˆï¼Œæœ€çµ‚å„²å­˜
        with open(current_output_path, "w", encoding="utf-8") as f:
            json.dump(processed_blocks, f, ensure_ascii=False, indent=2)

        logger.success(f"âœ… æ–‡ä»¶ {file_stem} è™•ç†å®Œæˆ")
        logger.info(f"   ğŸ“Š çµ±è¨ˆ: New={len(processed_blocks)-stats['skipped']} | Skipped (Reused)={stats['skipped']} | AI Calls={stats['ai_processed']}")

    logger.success("=" * 40)
    logger.success(f"ğŸ‰ æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œç•¢ï¼")

if __name__ == "__main__":
    main()