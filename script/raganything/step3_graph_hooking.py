import sys
import os
import json
import time
import glob
import asyncio 
from loguru import logger
from collections import defaultdict
from dotenv import load_dotenv
from functools import partial

# === å¼·åˆ¶åŠ å…¥æœ¬åœ°è·¯å¾‘ ===
current_path = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_path)))

if project_root not in sys.path:
    sys.path.insert(0, project_root)
# ========================

# è®€å– .env
load_dotenv()

# === ğŸ”¥ è¨­å®šå€ ===
SKIP_FILES = ["SFC", "Example_Doc_To_Skip"]

# å®šç¾©å“ªäº›æ–‡ä»¶æ˜¯ã€Œé«˜å¯ä¿¡åº¦ã€çš„ (å¯ä»¥æ ¹æ“š doc_label åˆ¤æ–·)
TRUSTED_KEYWORDS = ["Global Market Insights"] 
# =================

# å¼•å…¥ LightRAG
try:
    import lightrag 
    from lightrag import LightRAG
    from lightrag.utils import EmbeddingFunc
    from lightrag.llm.openai import azure_openai_complete, openai_embed
    logger.info("âœ… æˆåŠŸè¼‰å…¥ LightRAG")
except ImportError as e:
    logger.error(f"âŒ æ‰¾ä¸åˆ° LightRAG æˆ–ç›¸é—œæ¨¡çµ„: {e}")
    sys.exit(1)

# è¨­å®š Log
LOG_DIR = "./logs"
if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)
logger.remove()
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>", level="INFO")
logger.add(os.path.join(LOG_DIR, f"step3_graph_hook_{time.strftime('%Y%m%d_%H%M%S')}.log"), rotation="10 MB", encoding="utf-8")

# è¨­å®šè·¯å¾‘
WORKING_DIR = "./data/rag_storage"
STEP2_BASE_DIR = "./data/output/step2_output_granular"

# è‡ªå‹•é…ç½®è®€å–
ENV_LLM_MODEL = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o-mini") 
ENV_EMBED_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
SF_API_KEY = os.getenv("OPENAI_API_KEY") or os.getenv("EMBEDDING_BINDING_API_KEY") or os.getenv("SILICONFLOW_API_KEY")
SF_BASE_URL = os.getenv("OPENAI_BASE_URL") or os.getenv("EMBEDDING_BINDING_HOST") or "https://api.siliconflow.cn/v1"

if not SF_API_KEY:
    logger.error("âŒ æ‰¾ä¸åˆ° API Keyï¼è«‹æª¢æŸ¥ .env")
    sys.exit(1)

async def main():
    if not os.path.exists(STEP2_BASE_DIR):
        logger.error(f"âŒ æ‰¾ä¸åˆ° Step 2 è¼¸å‡ºç›®éŒ„: {STEP2_BASE_DIR}")
        return

    all_json_files = glob.glob(os.path.join(STEP2_BASE_DIR, "*", "granular_content.json"))
    if not all_json_files:
        logger.error("âŒ æ‰¾ä¸åˆ°ä»»ä½• granular_content.json")
        return

    if not os.path.exists(WORKING_DIR): os.makedirs(WORKING_DIR)
    
    logger.info("ğŸš€ åˆå§‹åŒ– LightRAG (Graph Hook Version)...")
    
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=azure_openai_complete,
        embedding_func=EmbeddingFunc(
            embedding_dim=1024, 
            max_token_size=512,
            func=partial(openai_embed.func, model=ENV_EMBED_MODEL, api_key=SF_API_KEY, base_url=SF_BASE_URL)
        ),
        chunk_token_size=512, 
        chunk_overlap_token_size=50,
        embedding_func_max_async=1, 
        max_parallel_insert=1 
    )

    logger.info("âš™ï¸ æ­£åœ¨åˆå§‹åŒ– Storage...")
    await rag.initialize_storages()

    logger.info(f"ğŸ“¦ ç™¼ç¾ {len(all_json_files)} ä»½æ–‡ä»¶ï¼Œé–‹å§‹æ‰¹é‡æ³¨å…¥...")
    total_files = len(all_json_files)
    
    for i, json_file_path in enumerate(all_json_files):
        doc_label = os.path.basename(os.path.dirname(json_file_path))
        
        if doc_label in SKIP_FILES:
            continue

        logger.info(f"\nğŸ“„ [File {i+1}/{total_files}] è™•ç†ä¸­: {doc_label}")
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºå¯ä¿¡æ–‡ä»¶
        is_trusted = any(k in doc_label for k in TRUSTED_KEYWORDS)
        if is_trusted:
            logger.info(f"   â­ è­˜åˆ¥ç‚ºå¯ä¿¡ä¾†æº (Graph Hook Enabled)")

        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                blocks = json.load(f)
        except Exception as e:
            logger.error(f"âŒ è®€å– JSON å¤±æ•— ({doc_label}): {e}")
            continue

        pages_map = defaultdict(str)
        for block in blocks:
            page_num = block.get('page', 'Unknown')
            content = block.get('content', '').strip()
            if not content: continue
            sep = "\n\n" if block.get('type') in ['table', 'image'] else "\n"
            pages_map[page_num] += f"{sep}{content}{sep}"

        sorted_pages = sorted(pages_map.items(), key=lambda x: int(x[0]) if isinstance(x[0], int) or str(x[0]).isdigit() else 9999)
        
        # === å‹•æ…‹åˆä½µ Buffer è¨­å®š ===
        TARGET_CHUNK_SIZE = 3000 
        buffer_content = ""
        start_page = -1
        end_page = -1
        chunk_count = 0
        total_pages = len(pages_map)

        for j, (page_num, page_content) in enumerate(sorted_pages):
            if len(page_content) < 10: continue
            
            if start_page == -1: start_page = page_num
            
            sep = f"\n\n--- Page {page_num} ---\n\n" if buffer_content else f"--- Page {page_num} ---\n"
            buffer_content += sep + page_content
            end_page = page_num

            is_last_page = (j == len(sorted_pages) - 1)
            
            if len(buffer_content) >= TARGET_CHUNK_SIZE or is_last_page:
                
                if start_page == end_page:
                    page_range_str = f"<Page {start_page}>"
                else:
                    page_range_str = f"<Page {start_page}-{end_page}>"
                
                source_id = f"{doc_label} {page_range_str}"
                final_text = f"Source: {source_id}\n\n{buffer_content}"

                try:
                    # 1. æ­£å¸¸æ’å…¥å…§å®¹
                    await rag.ainsert(final_text, file_paths=source_id)
                    chunk_count += 1
                    
                    # === ğŸ”¥ [Method 1] Graph Hook æ’å…¥ ===
                    # å¦‚æœæ˜¯å¯ä¿¡æ–‡ä»¶ï¼Œæ‰‹å‹•æ’å…¥ä¸€æ¢é«˜æ¬Šé‡é—œä¿‚
                    if is_trusted:
                        custom_kg = {
                            "entities": [
                                {
                                    "entity_name": "Verified Source",
                                    "entity_type": "QualityIndicator",
                                    "description": "Represents highly credible and verified information sources.",
                                    "source_id": "manual_trust_indicator" 
                                }
                            ],
                            "relationships": [
                                {
                                    "src_id": source_id, # å°‡é€™å€‹ Chunk é€£æ¥åˆ° Verified Source
                                    "tgt_id": "Verified Source",
                                    "description": "This content is from a verified high-credibility source.",
                                    "keywords": "verified, official, trusted",
                                    "weight": 10.0, # ğŸš€ è¶…é«˜æ¬Šé‡
                                    "source_id": "manual_trust_indicator"
                                }
                            ],
                            "chunks": [] # å…§å®¹å·²ç¶“åœ¨ ainsert è™•ç†äº†ï¼Œé€™è£¡ç•™ç©º
                        }
                        # æ³¨æ„ï¼šé€™è£¡ source_id ä¸ç”¨å¤ªåœ¨æ„ï¼Œä¸»è¦æ˜¯ç‚ºäº†å»ºç«‹é‚Š
                        await rag.ainsert_custom_kg(custom_kg)
                        logger.debug(f"      ğŸ”— å·²å»ºç«‹ Graph Hook: {source_id} -> Verified Source")
                    # ========================================

                    logger.info(f"     âœ… æ³¨å…¥å€å¡Š: {source_id} (Size: {len(buffer_content)})")

                except Exception as e:
                    logger.error(f"     âŒ æ³¨å…¥å¤±æ•— ({source_id}): {e}")

                # Buffer Reset Logic (Sliding Window)
                OVERLAP_SIZE = 500
                if len(page_content) > OVERLAP_SIZE and not is_last_page:
                    buffer_content = f"...(Context from Page {end_page})\n{page_content[-OVERLAP_SIZE:]}"
                else:
                    buffer_content = ""
                start_page = -1

        logger.success(f"âœ… æ–‡ä»¶ {doc_label} å®Œæˆï¼å…±ç”¢ç”Ÿ {chunk_count} å€‹åˆä½µå€å¡Š")

    logger.info("\n" + "="*40)
    logger.success(f"ğŸ‰ æ‰€æœ‰æ–‡ä»¶è™•ç†å®Œç•¢ï¼")
    logger.info("="*40)

if __name__ == "__main__":
    asyncio.run(main())