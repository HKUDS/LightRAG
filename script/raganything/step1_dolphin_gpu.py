import sys
import os
import json
import glob
import time
import math
import gc
import torch
from pdf2image import convert_from_path, pdfinfo_from_path
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from qwen_vl_utils import process_vision_info
from loguru import logger

# ==========================================
# ğŸ”¥ [ä½¿ç”¨è€…è¨­å®šå€] è«‹æ ¹æ“šä½ çš„ç’°å¢ƒä¿®æ”¹é€™è£¡
# ==========================================

# 1. Poppler è·¯å¾‘ (Windows å¿…å¡«)
# å¦‚æœä½ å·²ç¶“åŠ åˆ°ç³»çµ±ç’°å¢ƒè®Šæ•¸ï¼Œå¯ä»¥è¨­ç‚º None
POPPLER_BIN_PATH = r"C:\Users\sammi_hung\LightRAG\poppler-25.12.0\Library\bin"

# 2. è¼¸å…¥/è¼¸å‡ºè·¯å¾‘
INPUT_DIR = "./data/input/__enqueued__"
OUTPUT_DIR_BASE = "./data/input/step1_output"

# 3. æ¨¡å‹è¨­å®š
MODEL_ID = "ByteDance/Dolphin-v2"

# 4. æ•ˆèƒ½èª¿å„ª (é‡å° RTX 4060 8GB)
# 12000-14000 æ˜¯ 8GB VRAM çš„å®‰å…¨å€é–“
# 14000 tokens â‰ˆ 274è¬åƒç´  (ä¾‹å¦‚ 1400x1900)
MAX_VISUAL_TOKENS = 14000 

# æ¯å¹¾é å­˜æª”ä¸€æ¬¡ (é˜²æ­¢ç•¶æ©Ÿè³‡æ–™å…¨å¤±)
SAVE_INTERVAL = 5 

# ==========================================

# === 0. ç’°å¢ƒæº–å‚™ ===
# å°‡ Poppler åŠ å…¥ PATH
if POPPLER_BIN_PATH and os.path.exists(POPPLER_BIN_PATH):
    if POPPLER_BIN_PATH not in os.environ["PATH"]:
        os.environ["PATH"] += os.pathsep + POPPLER_BIN_PATH
        print(f"ğŸ”§ å·²å°‡ Poppler åŠ å…¥ PATH: {POPPLER_BIN_PATH}")

# è¨­å®š Logging
LOG_DIR = "./logs"
if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)
log_file = os.path.join(LOG_DIR, f"step1_dolphin_gpu_{time.strftime('%Y%m%d_%H%M%S')}.log")

logger.remove()
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>", level="INFO")
logger.add(log_file, rotation="10 MB", level="DEBUG", encoding="utf-8")
logger.info(f"ğŸ“ Log æª”æ¡ˆå·²å»ºç«‹: {log_file}")

# === 1. è¼”åŠ©å‡½å¼ ===

def resize_to_token_limit(image, max_tokens=14000):
    """
    æ™ºæ…§ç¸®åœ–ï¼šç¢ºä¿åœ–ç‰‡ä¸æœƒç”¢ç”Ÿéå¤š Token å°è‡´ OOM
    Qwen2.5-VL: 1 token â‰ˆ 14x14 pixels = 196 pixels
    """
    w, h = image.size
    total_pixels = w * h
    current_tokens = total_pixels / 196
    
    if current_tokens > max_tokens:
        scale = math.sqrt(max_tokens / current_tokens)
        new_w = int(w * scale)
        new_h = int(h * scale)
        logger.debug(f"ğŸ“‰ åœ–ç‰‡å£“ç¸®: {w}x{h} -> {new_w}x{new_h} (Tokens: {int(current_tokens)} -> ~{max_tokens})")
        return image.resize((new_w, new_h), Image.Resampling.LANCZOS)
    
    return image

def load_model():
    """
    è¼‰å…¥æ¨¡å‹ (4-bit é‡åŒ– + GPU åŠ é€Ÿ)
    """
    logger.info("="*60)
    logger.info(f"ğŸ“¥ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {MODEL_ID} (4-bit GPU Mode)...")
    
    if not torch.cuda.is_available():
        logger.critical("âŒ æª¢æ¸¬ä¸åˆ° GPUï¼è«‹ç¢ºèª CUDA æ˜¯å¦å®‰è£æ­£ç¢ºã€‚")
        sys.exit(1)

    gpu_name = torch.cuda.get_device_name(0)
    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    logger.info(f"ğŸ® GPU åµæ¸¬: {gpu_name} | VRAM: {vram_gb:.2f} GB")

    try:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            MODEL_ID,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        logger.success("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
        return model, processor
    except Exception as e:
        logger.critical(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        sys.exit(1)

def process_single_file(input_path, output_base, model, processor):
    filename = os.path.basename(input_path)
    file_stem = os.path.splitext(filename)[0]
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    current_out_dir = os.path.join(output_base, file_stem)
    if not os.path.exists(current_out_dir): os.makedirs(current_out_dir)
    final_json = os.path.join(current_out_dir, "intermediate_result.json")
    images_dir = os.path.join(current_out_dir, "images")
    if not os.path.exists(images_dir): os.makedirs(images_dir)

    logger.info("-" * 40)
    logger.info(f"ğŸš€ [Start] è™•ç†æª”æ¡ˆ: {filename}")
    
    start_time = time.time()

    # 1. ç²å–ç¸½é æ•¸ (ä¸è®€å–åœ–ç‰‡ï¼Œåªè®€ Metadataï¼Œæ¥µå¿«ä¸”çœ RAM)
    try:
        info = pdfinfo_from_path(input_path, poppler_path=POPPLER_BIN_PATH)
        total_pages = info["Pages"]
        logger.info(f"ğŸ“„ PDF ç¸½é æ•¸: {total_pages}")
    except Exception as e:
        logger.error(f"âŒ ç„¡æ³•è®€å– PDF è³‡è¨Š (å¯èƒ½æ˜¯è·¯å¾‘éŒ¯èª¤æˆ–æª”æ¡ˆææ¯€): {e}")
        return False

    parsed_results = []
    
    # å¦‚æœæœ‰èˆŠçš„é€²åº¦ï¼Œå¯ä»¥åœ¨é€™è£¡è¼‰å…¥ (Optional)
    # ...

    # 2. é€é è™•ç† (Page-by-Page)
    for i in range(1, total_pages + 1):
        page_start = time.time()
        logger.info(f"   ğŸ”„ æ­£åœ¨è™•ç† Page {i}/{total_pages}...")

        try:
            # ğŸ”¥ é—œéµå„ªåŒ–ï¼šåªè¼‰å…¥ã€Œé€™ä¸€é ã€
            # dpi=200 ä¿è­‰åŸå§‹ç´°ç¯€ï¼Œå¾ŒçºŒå†ç”¨ resize_to_token_limit ç¸®å°
            page_images = convert_from_path(
                input_path, 
                dpi=200, 
                first_page=i, 
                last_page=i, 
                poppler_path=POPPLER_BIN_PATH
            )
            
            if not page_images:
                logger.warning(f"âš ï¸ Page {i} è®€å–ç©ºç™½ï¼Œè·³éã€‚")
                continue
            
            image = page_images[0]
            
            # å„²å­˜åŸåœ– (æ–¹ä¾¿ Debug)
            img_filename = f"page_{i-1}.jpg" # 0-based index for saving
            image.save(os.path.join(images_dir, img_filename))

            # æ™ºæ…§ç¸®æ”¾ (ä¿è­· VRAM)
            image = resize_to_token_limit(image, max_tokens=MAX_VISUAL_TOKENS)

            # å»ºæ§‹ Prompt
            messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": "Read the text in the image word by word and transcribe it into Markdown format. Represent tables using Markdown syntax."}
                ]
            }]

            # æº–å‚™ Tensor
            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = process_vision_info(messages)
            
            inputs = processor(
                text=[text], 
                images=image_inputs, 
                videos=video_inputs, 
                padding=True, 
                return_tensors="pt"
            ).to(model.device)

            # æ¨ç† Generation
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=2048, # å¦‚æœé‚„çˆ†ï¼Œå¯é™è‡³ 1024
                    do_sample=False
                )

            # è§£ç¢¼ Decoding
            gen_ids_trimmed = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]
            md_text = processor.batch_decode(gen_ids_trimmed, skip_special_tokens=True)[0]

            # å­˜å…¥çµæœåˆ—è¡¨ (æ³¨æ„ page_idx è½‰ç‚º 0-based)
            parsed_results.append({
                "type": "text",
                "text": md_text,
                "page_idx": i - 1,
                "img_path": f"images/{img_filename}",
                "bbox": [0, 0, image.width, image.height]
            })

            dur = time.time() - page_start
            logger.info(f"     âœ… å®Œæˆ ({dur:.2f}s) | VRAM: {torch.cuda.memory_allocated()/1e9:.2f}GB")

            # ğŸ”¥ å®šæœŸå­˜æª” (Incremental Save)
            if i % SAVE_INTERVAL == 0 or i == total_pages:
                with open(final_json, "w", encoding="utf-8") as f:
                    json.dump(parsed_results, f, ensure_ascii=False, indent=2)
                logger.debug(f"ğŸ’¾ é€²åº¦å·²å„²å­˜ (Page {i})")

        except torch.cuda.OutOfMemoryError:
            logger.error(f"âŒ Page {i} OOM (é¡¯å­˜ä¸è¶³)ï¼è·³éæ­¤é ã€‚å»ºè­°é™ä½ MAX_VISUAL_TOKENSã€‚")
            torch.cuda.empty_cache()
        except Exception as e:
            logger.exception(f"âŒ Page {i} ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            # ğŸ”¥ æ¥µè‡´æ¸…ç†ï¼šç¢ºä¿æ¯ä¸€é è™•ç†å®Œéƒ½é‡‹æ”¾è³‡æº
            if 'inputs' in locals(): del inputs
            if 'generated_ids' in locals(): del generated_ids
            if 'image' in locals(): del image
            if 'page_images' in locals(): del page_images
            torch.cuda.empty_cache()
            gc.collect()

    logger.success(f"ğŸ‰ æª”æ¡ˆè™•ç†å®Œæˆï¼ç¸½è€—æ™‚: {time.time() - start_time:.2f}s")
    return True

def main():
    if not os.path.exists(INPUT_DIR):
        logger.error(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥ç›®éŒ„: {INPUT_DIR}")
        return
    if not os.path.exists(OUTPUT_DIR_BASE):
        os.makedirs(OUTPUT_DIR_BASE)

    # è¼‰å…¥æ¨¡å‹ (åªåšä¸€æ¬¡)
    model, processor = load_model()

    # æƒææª”æ¡ˆ
    all_files = glob.glob(os.path.join(INPUT_DIR, "*"))
    files = [f for f in all_files if os.path.isfile(f) and not os.path.basename(f).startswith(".")]
    
    logger.info(f"ğŸ“¦ ç™¼ç¾ {len(files)} å€‹æª”æ¡ˆï¼Œæº–å‚™é–‹å§‹...")

    for idx, file_path in enumerate(files):
        logger.info(f"\n[{idx+1}/{len(files)}] ----------------------------------------")
        
        # æ’é™¤é PDF (æš«æ™‚åªè™•ç† PDFï¼Œå¦‚æœæ˜¯åœ–ç‰‡å¯è‡ªè¡Œä¿®æ”¹)
        if not file_path.lower().endswith(".pdf"):
            logger.warning(f"â­ï¸ è·³éé PDF æª”æ¡ˆ: {file_path}")
            continue
            
        process_single_file(file_path, OUTPUT_DIR_BASE, model, processor)

    logger.success("\nğŸ æ‰€æœ‰ä»»å‹™åŸ·è¡Œå®Œç•¢ï¼")

if __name__ == "__main__":
    main()