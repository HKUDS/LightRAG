"""
Usage metering models for API responses.

These models track and report token usage and cost information
for LLM and embedding operations.
"""

import os
from datetime import date
from typing import Optional
from pydantic import BaseModel, Field


def calculate_estimated_cost(
    llm_prompt_tokens: int = 0,
    llm_completion_tokens: int = 0,
    embedding_tokens: int = 0,
) -> Optional[float]:
    """
    Calculate estimated cost based on token usage and configured pricing.

    Pricing is configured via environment variables:
    - LIGHTRAG_LLM_PRICE_PER_1K_INPUT: Cost per 1000 input tokens for LLM
    - LIGHTRAG_LLM_PRICE_PER_1K_OUTPUT: Cost per 1000 output tokens for LLM
    - LIGHTRAG_EMBEDDING_PRICE_PER_1K: Cost per 1000 tokens for embeddings

    Returns:
        Estimated cost in USD, or None if no pricing is configured.
    """
    llm_input_price = os.getenv("LIGHTRAG_LLM_PRICE_PER_1K_INPUT")
    llm_output_price = os.getenv("LIGHTRAG_LLM_PRICE_PER_1K_OUTPUT")
    embedding_price = os.getenv("LIGHTRAG_EMBEDDING_PRICE_PER_1K")

    # If no pricing is configured, return None
    if not any([llm_input_price, llm_output_price, embedding_price]):
        return None

    total_cost = 0.0

    # Calculate LLM input cost
    if llm_input_price and llm_prompt_tokens > 0:
        try:
            total_cost += (llm_prompt_tokens / 1000) * float(llm_input_price)
        except ValueError:
            pass

    # Calculate LLM output cost
    if llm_output_price and llm_completion_tokens > 0:
        try:
            total_cost += (llm_completion_tokens / 1000) * float(llm_output_price)
        except ValueError:
            pass

    # Calculate embedding cost
    if embedding_price and embedding_tokens > 0:
        try:
            total_cost += (embedding_tokens / 1000) * float(embedding_price)
        except ValueError:
            pass

    # Return None if no cost was calculated (all values were zero or invalid)
    return total_cost if total_cost > 0 else None


class LLMUsageInfo(BaseModel):
    """LLM token usage details for a request."""

    prompt_tokens: int = Field(
        default=0,
        description="Number of input tokens sent to the LLM",
        ge=0,
    )
    completion_tokens: int = Field(
        default=0,
        description="Number of output tokens generated by the LLM",
        ge=0,
    )
    total_tokens: int = Field(
        default=0,
        description="Total tokens (prompt + completion)",
        ge=0,
    )
    calls: int = Field(
        default=0,
        description="Number of LLM API calls made",
        ge=0,
    )
    model: Optional[str] = Field(
        default=None,
        description="Model identifier used for LLM calls",
    )


class EmbeddingUsageInfo(BaseModel):
    """Embedding token usage details for a request."""

    tokens: int = Field(
        default=0,
        description="Number of tokens processed for embeddings",
        ge=0,
    )
    calls: int = Field(
        default=0,
        description="Number of embedding API calls made",
        ge=0,
    )
    model: Optional[str] = Field(
        default=None,
        description="Model identifier used for embedding calls",
    )


class UsageInfo(BaseModel):
    """Per-request usage information included in API responses."""

    llm: Optional[LLMUsageInfo] = Field(
        default=None,
        description="LLM usage details (null if no LLM calls were made)",
    )
    embedding: Optional[EmbeddingUsageInfo] = Field(
        default=None,
        description="Embedding usage details (null if no embedding calls were made)",
    )
    estimated_cost_usd: Optional[float] = Field(
        default=None,
        description="Estimated cost in USD (only present if pricing is configured)",
    )

    @classmethod
    def from_token_tracker(
        cls,
        token_tracker,
        estimated_cost: Optional[float] = None,
    ) -> "UsageInfo":
        """
        Create UsageInfo from a TokenTracker instance.

        Args:
            token_tracker: TokenTracker instance with usage data
            estimated_cost: Optional pre-calculated cost estimate

        Returns:
            UsageInfo instance with usage data from tracker
        """
        llm_usage = None
        embedding_usage = None

        # Get LLM usage if available
        llm_data = token_tracker.get_llm_usage()
        if llm_data.get("call_count", 0) > 0:
            llm_usage = LLMUsageInfo(
                prompt_tokens=llm_data.get("prompt_tokens", 0),
                completion_tokens=llm_data.get("completion_tokens", 0),
                total_tokens=llm_data.get("total_tokens", 0),
                calls=llm_data.get("call_count", 0),
                model=llm_data.get("model"),
            )

        # Get embedding usage if available
        embedding_data = token_tracker.get_embedding_usage()
        if embedding_data.get("call_count", 0) > 0:
            embedding_usage = EmbeddingUsageInfo(
                tokens=embedding_data.get("total_tokens", 0),
                calls=embedding_data.get("call_count", 0),
                model=embedding_data.get("model"),
            )

        return cls(
            llm=llm_usage,
            embedding=embedding_usage,
            estimated_cost_usd=estimated_cost,
        )


class QueryTokenUsage(BaseModel):
    """Flat token usage structure for Cleo billing integration.

    This model provides a simplified, flat structure for token usage reporting,
    designed specifically for external billing systems like Cleo. It maps directly
    to billing rate tables without requiring nested object traversal.
    """

    llm_model: Optional[str] = Field(
        default=None,
        description="Model ID used for response generation (null if no LLM call made)",
    )
    llm_input_tokens: int = Field(
        default=0,
        description="Total input tokens sent to LLM (system prompt + context + query)",
        ge=0,
    )
    llm_output_tokens: int = Field(
        default=0,
        description="Tokens in generated response",
        ge=0,
    )
    embedding_model: Optional[str] = Field(
        default=None,
        description="Model ID used for query embedding",
    )
    embedding_tokens: int = Field(
        default=0,
        description="Tokens used to embed the query",
        ge=0,
    )

    @classmethod
    def from_token_tracker(cls, token_tracker) -> "QueryTokenUsage":
        """Create QueryTokenUsage from a TokenTracker instance.

        Args:
            token_tracker: TokenTracker instance with usage data

        Returns:
            QueryTokenUsage instance with flat token usage data
        """
        llm_data = token_tracker.get_llm_usage()
        embedding_data = token_tracker.get_embedding_usage()

        return cls(
            llm_model=llm_data.get("model"),
            llm_input_tokens=llm_data.get("prompt_tokens", 0),
            llm_output_tokens=llm_data.get("completion_tokens", 0),
            embedding_model=embedding_data.get("model"),
            embedding_tokens=embedding_data.get("total_tokens", 0),
        )


class UsageAggregateResponse(BaseModel):
    """Aggregated usage statistics for a workspace over a time period."""

    workspace: str = Field(
        description="Workspace identifier",
    )
    start_date: date = Field(
        description="Start of the aggregation period",
    )
    end_date: date = Field(
        description="End of the aggregation period",
    )
    llm: Optional[dict] = Field(
        default=None,
        description="Aggregated LLM usage statistics",
    )
    embedding: Optional[dict] = Field(
        default=None,
        description="Aggregated embedding usage statistics",
    )
    total_estimated_cost_usd: Optional[float] = Field(
        default=None,
        description="Total estimated cost for the period (if pricing configured)",
    )
    request_count: int = Field(
        default=0,
        description="Number of requests in the period",
        ge=0,
    )
