# PRD: LLM Post-Processing Cache System for LightRAG

## 1. Executive Summary

### Problem Statement
The LightRAG implementation currently makes 75-100 LLM calls per document, resulting in significant costs (~$110/month). The chunk-level post-processing system, while effective, repeatedly processes identical extraction results when documents are reprocessed, wasting tokens and money.

### Solution
Implement a conditional caching system with dependency hashing that caches post-processing results based on extraction output, reducing redundant LLM calls while maintaining data freshness.

### Expected Impact
- 60-80% reduction in post-processing LLM calls
- ~$40-60/month cost savings
- 70% faster document reprocessing

## 2. User Stories

### As a Developer
- I want to reprocess documents without redundant LLM calls so that I can iterate quickly and save costs
- I want to invalidate cache when extraction logic changes so that results remain accurate
- I want visibility into cache performance so that I can optimize the system

### As a System Administrator
- I want automatic cache management so that storage doesn't grow unbounded
- I want cache versioning so that logic updates don't serve stale results

## 3. Functional Requirements

### 3.1 Cache Key Generation
```python
Requirements:
- Generate deterministic hash from extraction results
- Include post-processing version
- Include chunk identifier
- Support for optional chunk content hashing
```

### 3.2 Cache Operations
```python
Requirements:
- Store: Save post-processing results with TTL
- Retrieve: Get cached results if valid
- Invalidate: Remove entries on demand
- Exists: Check cache presence without retrieval
```

### 3.3 Cache Invalidation
```python
Triggers:
- POST_PROCESSING_VERSION change
- force_reprocess flag
- TTL expiration
- Manual invalidation command
```

### 3.4 Monitoring & Logging
```python
Metrics:
- Cache hit rate
- Storage size
- Token savings
- Processing time reduction
```

## 4. Technical Specifications

### 4.1 Architecture
```python
┌─────────────────┐
│ Chunk Extractor │
└────────┬────────┘
         │ Relationships
         ▼
┌─────────────────┐     ┌─────────────┐
│ Cache Key Gen   │────▶│ Cache Store │
└────────┬────────┘     └──────┬──────┘
         │                     │
         ▼                     │
┌─────────────────┐           │
│ Cache Lookup    │◀──────────┘
└────────┬────────┘
         │ Hit/Miss
         ▼
┌─────────────────┐
│ LLM Post-Proc   │ (Only on miss)
└─────────────────┘
```

### 4.2 Implementation Details

#### Cache Key Generator
```python
class PostProcessingCacheKeyGenerator:
    def __init__(self, version: str = "1.0"):
        self.version = version

    def generate_key(
        self,
        chunk_id: str,
        relationships: List[Relationship],
        include_content_hash: bool = False,
        chunk_content: Optional[str] = None
    ) -> str:
        # Serialize relationships deterministically
        rel_data = sorted([{
            'src': r.src_id,
            'tgt': r.tgt_id,
            'type': r.relationship_type,
            'weight': round(r.weight, 4),  # Avoid float precision issues
            'desc_hash': hashlib.md5(r.description.encode()).hexdigest()[:8]
        } for r in relationships], key=lambda x: f"{x['src']}-{x['tgt']}")

        # Create base hash
        base_string = json.dumps({
            'version': self.version,
            'chunk_id': chunk_id,
            'relationships': rel_data,
            'rel_count': len(relationships)
        }, sort_keys=True)

        base_hash = hashlib.sha256(base_string.encode()).hexdigest()[:16]

        # Optionally include content hash
        if include_content_hash and chunk_content:
            content_hash = hashlib.md5(chunk_content.encode()).hexdigest()[:8]
            return f"pp_v{self.version}_{chunk_id}_{base_hash}_{content_hash}"

        return f"pp_v{self.version}_{chunk_id}_{base_hash}"
```

#### Cache Manager
```python
class PostProcessingCacheManager:
    def __init__(
        self,
        cache_backend: str = "redis",
        default_ttl: int = 7 * 24 * 3600,  # 7 days
        max_cache_size_gb: float = 10.0
    ):
        self.cache = self._init_cache(cache_backend)
        self.default_ttl = default_ttl
        self.max_size = max_cache_size_gb * 1024 * 1024 * 1024
        self.metrics = CacheMetrics()

    async def get_or_process(
        self,
        chunk_id: str,
        relationships: List[Relationship],
        process_func: Callable,
        force_reprocess: bool = False,
        ttl: Optional[int] = None
    ) -> PostProcessingResult:
        # Generate cache key
        cache_key = self.key_generator.generate_key(chunk_id, relationships)

        # Check cache unless forced
        if not force_reprocess:
            cached = await self.cache.get(cache_key)
            if cached:
                self.metrics.record_hit(cache_key)
                logger.info(f"Cache HIT: {cache_key}")
                return PostProcessingResult.from_cache(cached)

        # Process and cache
        self.metrics.record_miss(cache_key)
        logger.info(f"Cache MISS: {cache_key} - Processing...")

        result = await process_func(chunk_id, relationships)

        # Store in cache
        await self.cache.set(
            cache_key,
            result.to_cache(),
            ttl=ttl or self.default_ttl
        )

        # Check cache size
        await self._enforce_size_limit()

        return result
```

#### Metrics Tracking
```python
class CacheMetrics:
    def __init__(self):
        self.hits = 0
        self.misses = 0
        self.token_savings = 0
        self.time_savings_ms = 0

    def record_hit(self, cache_key: str, tokens_saved: int = 500):
        self.hits += 1
        self.token_savings += tokens_saved
        self.time_savings_ms += 2000  # Avg LLM call time

    def get_hit_rate(self) -> float:
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

    def get_cost_savings(self, cost_per_1k_tokens: float = 0.01) -> float:
        return (self.token_savings / 1000) * cost_per_1k_tokens
```

### 4.3 Configuration
```yaml
# config/cache_settings.yaml
post_processing_cache:
  enabled: true
  backend: "redis"  # or "postgres", "memory"
  version: "1.0"

  ttl:
    default: 604800  # 7 days
    minimum: 3600    # 1 hour
    maximum: 2592000 # 30 days

  size_limits:
    max_total_size_gb: 10
    max_entry_size_mb: 10
    eviction_policy: "lru"

  monitoring:
    log_level: "info"
    metrics_enabled: true
    report_interval: 3600
```

## 5. Integration Points

### 5.1 Existing Code Changes
```python
# In lightrag/kg/post_process/post_process.py

async def post_process_chunk_with_cache(
    chunk: Dict,
    relationships: List[Relationship],
    config: Config
) -> List[Relationship]:
    """Wrapper around existing post_process_chunk with caching"""

    cache_manager = get_cache_manager(config)

    async def _process():
        # Existing post-processing logic
        return await llm_post_process_chunk(chunk, relationships, config)

    result = await cache_manager.get_or_process(
        chunk_id=chunk['chunk_id'],
        relationships=relationships,
        process_func=_process,
        force_reprocess=config.get('force_reprocess', False)
    )

    return result.relationships
```

## 6. Testing Requirements

### 6.1 Unit Tests
```python
def test_cache_key_deterministic():
    """Same inputs produce same key"""

def test_cache_key_changes_with_relationships():
    """Different relationships produce different keys"""

def test_cache_invalidation_on_version_change():
    """Version change invalidates cache"""

def test_metrics_tracking():
    """Metrics accurately track hits/misses"""
```

### 6.2 Integration Tests
```python
def test_full_document_processing_with_cache():
    """Process document twice, verify cache usage"""

def test_cache_size_enforcement():
    """Verify eviction when size limit reached"""
```

## 7. Rollout Plan

### Phase 1: Development (Week 1)
- Implement cache key generator
- Implement cache manager
- Add metrics tracking

### Phase 2: Testing (Week 2)
- Unit tests
- Integration tests
- Performance benchmarking

### Phase 3: Gradual Rollout (Week 3)
- Enable for 10% of documents
- Monitor metrics
- Fix any issues

### Phase 4: Full Deployment (Week 4)
- Enable for all documents
- Document usage
- Training for team

## 8. Success Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Cache Hit Rate | >60% | After 1 week |
| Cost Reduction | >40% | Monthly |
| Processing Speed | 3x faster | For cached docs |
| Storage Usage | <10GB | Ongoing |

## 9. Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Stale cache serves wrong results | High | Version tracking, TTL limits |
| Cache storage grows too large | Medium | Size limits, eviction policy |
| Cache backend failure | Medium | Fallback to direct processing |
| Complex debugging | Low | Comprehensive logging |

## 10. Future Enhancements

1. **Distributed Caching**: Support Redis Cluster for scalability
2. **Smart TTL**: Adjust TTL based on document change frequency
3. **Compression**: Store compressed results to save space
4. **Warming**: Pre-populate cache during off-hours
5. **Analytics Dashboard**: Visual cache performance metrics

## 11. Appendix: Example Usage

```python
# Developer usage
result = await lightrag.process_document(
    "document.md",
    force_reprocess=False  # Use cache
)

# Admin commands
lightrag cache clear --pattern "pp_v1.0_*"
lightrag cache stats
lightrag cache warm --documents "./docs/*.md"
```

This caching system will significantly reduce your LLM costs while maintaining the quality of your post-processing system!
