### This is sample file of .env


### Server Configuration
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='TLL Graph UI'
WEBUI_DESCRIPTION="Simple and Fast Graph Based RAG System"
OLLAMA_EMULATING_MODEL_TAG=latest

# Gunicorn Multi-Worker Configuration
WORKERS=4
WORKER_TIMEOUT=120
WORKER_MAX_REQUESTS=1000
WORKER_MAX_REQUESTS_JITTER=50
PRELOAD_APP=true

# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

### Login Configuration
# AUTH_ACCOUNTS='admin:admin123,user1:pass456'
# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server
# TOKEN_EXPIRE_HOURS=48
# GUEST_TOKEN_EXPIRE_HOURS=24
# JWT_ALGORITHM=HS256

### API-Key to access LightRAG Server API
# LIGHTRAG_API_KEY=your-secure-api-key-here
# WHITELIST_PATHS=/health,/api/*

### Optional SSL Configuration
# SSL=true
# SSL_CERTFILE=/path/to/cert.pem
# SSL_KEYFILE=/path/to/key.pem

### Directory Configuration (defaults to current working directory)
### Should not be set if deploy by docker (Set by Dockerfile instead of .env)
### Default value is ./inputs and ./rag_storage
# INPUT_DIR=<absolute_path_for_doc_input_dir>
# WORKING_DIR=<absolute_path_for_working_dir>

### Max nodes return from grap retrieval
# MAX_GRAPH_NODES=1000

### Logging level
# LOG_LEVEL=INFO
# VERBOSE=False
# LOG_MAX_BYTES=10485760
# LOG_BACKUP_COUNT=5
### Logfile location (defaults to current working directory)
# LOG_DIR=/path/to/log/directory

### Settings for RAG query
# HISTORY_TURNS=3
# COSINE_THRESHOLD=0.2
# TOP_K=60
# MAX_TOKEN_TEXT_CHUNK=4000
# MAX_TOKEN_RELATION_DESC=4000
# MAX_TOKEN_ENTITY_DESC=4000

### Dynamic Thresholding Configuration
ENABLE_DYNAMIC_THRESHOLDS=true
MIN_THRESHOLD=0.2
THRESHOLD_FLOOR=0.1
THRESHOLD_CEILING=0.4
THRESHOLD_PERCENTILE=10.0

### Entity and ralation summarization configuration
### Language: English, Chinese, French, German ...
SUMMARY_LANGUAGE=English
### Number of duplicated entities/edges to trigger LLM re-summary on merge ( at least 3 is recommented)
# FORCE_LLM_SUMMARY_ON_MERGE=6
### Max tokens for entity/relations description after merge
# MAX_TOKEN_SUMMARY=500

### Number of parallel processing documents(Less than MAX_ASYNC/2 is recommended)
# MAX_PARALLEL_INSERT=2
### Chunk size for document splitting, 500~1500 is recommended
# CHUNK_SIZE=1200
# CHUNK_OVERLAP_SIZE=100

### Chunk-Level Relationship Post-Processing Configuration
### Enable chunk-level relationship validation for improved accuracy and performance
# ENABLE_CHUNK_POST_PROCESSING=false
### Maximum number of relationships to validate per chunk (recommended: 50)
# CHUNK_VALIDATION_BATCH_SIZE=50
### Timeout in seconds for chunk-level validation (recommended: 30)
# CHUNK_VALIDATION_TIMEOUT=30
### Enable detailed logging of relationship validation changes
# LOG_VALIDATION_CHANGES=false

### Enhanced Relationship Quality Filter Configuration
### Enable type-specific relationship filtering with intelligent categorization
# ENABLE_ENHANCED_RELATIONSHIP_FILTER=true
### Log detailed classification results for each relationship (can be verbose)
# LOG_RELATIONSHIP_CLASSIFICATION=false
### Track filter performance metrics by relationship type
# RELATIONSHIP_FILTER_PERFORMANCE_TRACKING=true
### Enable console logging for enhanced filter (set to true for debugging)
# ENHANCED_FILTER_CONSOLE_LOGGING=false
### Monitoring mode: classify and log relationships but don't actually filter them
# ENHANCED_FILTER_MONITORING_MODE=false

### LLM Configuration
ENABLE_LLM_CACHE=true
ENABLE_LLM_CACHE_FOR_EXTRACT=true
### Time out in seconds for LLM, None for infinite timeout
TIMEOUT=0
### Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0.1
### Max concurrency requests of LLM
MAX_ASYNC=10
### MAX_TOKENS: max tokens send to LLM for entity relation summaries (less than context size of the model)
### MAX_TOKENS: set as num_ctx option for Ollama by API Server
MAX_TOKENS=16000
### LLM Binding type: openai, ollama, lollms, azure_openai
LLM_BINDING=openai
LLM_MODEL=gpt-4.1-mini
LLM_BINDING_HOST=https://api.openai.com/v1
LLM_BINDING_API_KEY=your_openai_api_key_here
### Optional for Azure
# AZURE_OPENAI_API_VERSION=2024-08-01-preview
# AZURE_OPENAI_DEPLOYMENT=gpt-4o

### Embedding Configuration
### Embedding Binding type: openai, ollama, lollms, azure_openai
EMBEDDING_BINDING=anthropic
EMBEDDING_MODEL=voyage-3-large
EMBEDDING_BINDING_HOST=https://api.voyageai.com/v1
EMBEDDING_DIM=1024
MAX_EMBED_TOKENS=32000
EMBEDDING_BINDING_API_KEY=your_voyage_api_key_here
# If the embedding service is deployed within the same Docker stack, use host.docker.internal instead of localhost
# EMBEDDING_BINDING_HOST=http://localhost:11434
### Num of chunks send to Embedding in single request
# EMBEDDING_BATCH_NUM=32
### Max concurrency requests for Embedding
# EMBEDDING_FUNC_MAX_ASYNC=16
### Maximum tokens sent to Embedding for each chunk (no longer in use?)
# MAX_EMBED_TOKENS=8192
### Optional for Azure
# AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large
# AZURE_EMBEDDING_API_VERSION=2023-05-15

### Data storage selection
LIGHTRAG_KV_STORAGE=PGKVStorage
LIGHTRAG_VECTOR_STORAGE=PGVectorStorage
LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
LIGHTRAG_GRAPH_STORAGE=Neo4JStorage

### TiDB Configuration (Deprecated)
# TIDB_HOST=localhost
# TIDB_PORT=4000
# TIDB_USER=your_username
# TIDB_PASSWORD='your_password'
# TIDB_DATABASE=your_database
### separating all data from difference Lightrag instances(deprecating)
# TIDB_WORKSPACE=default

### PostgreSQL Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=your_postgres_username
POSTGRES_PASSWORD='your_postgres_password'
POSTGRES_DATABASE=postgres
POSTGRES_MAX_CONNECTIONS=12
### separating all data from difference Lightrag instances(deprecating)
# POSTGRES_WORKSPACE=default

### Neo4j Configuration
NEO4J_URI=bolt://localhost:7689
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password
NEO4J_DATABASE=neo4j

# Neo4j Connection Pool Settings for Multi-Worker Support
NEO4J_MAX_CONNECTION_POOL_SIZE=100
NEO4J_CONNECTION_TIMEOUT=30.0
NEO4J_CONNECTION_ACQUISITION_TIMEOUT=60.0
NEO4J_MAX_TRANSACTION_RETRY_TIME=30.0

### Independent AGM Configuration(not for AMG embedded in PostreSQL)
# AGE_POSTGRES_DB=
# AGE_POSTGRES_USER=
# AGE_POSTGRES_PASSWORD=
# AGE_POSTGRES_HOST=
# AGE_POSTGRES_PORT=8529

# AGE Graph Name(apply to PostgreSQL and independent AGM)
### AGE_GRAPH_NAME is precated
# AGE_GRAPH_NAME=lightrag

### MongoDB Configuration
MONGO_URI=mongodb://root:root@localhost:27017/
MONGO_DATABASE=LightRAG
### separating all data from difference Lightrag instances(deprecating)
# MONGODB_GRAPH=false

### Milvus Configuration
MILVUS_URI=http://localhost:19530
MILVUS_DB_NAME=lightrag
# MILVUS_USER=root
# MILVUS_PASSWORD=your_password
# MILVUS_TOKEN=your_token

### Qdrant
QDRANT_URL=http://localhost:16333
# QDRANT_API_KEY=your-api-key

### Redis
REDIS_URI=redis://localhost:6379
NEO4J_DATABASE=neo4j

### Chunk Post-Processing Configuration
# ENABLE_CHUNK_POST_PROCESSING=false
# LOG_VALIDATION_CHANGES=false
# CHUNK_VALIDATION_BATCH_SIZE=50
# CHUNK_VALIDATION_TIMEOUT=30

### LLM Post-Processing Configuration
# ENABLE_LLM_POST_PROCESSING=false

### Enhanced Relationship Filter Configuration
# ENABLE_ENHANCED_RELATIONSHIP_FILTER=false
