{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.tencent.com/pypi/simple\n",
      "/Users/llp/opensource/LightRAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement lmdeploy (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for lmdeploy\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from lightrag.utils import logger\n",
    "# 添加父目录到路径，以便导入LightRAG包\n",
    "# 在Jupyter Notebook中使用os.getcwd()代替__file__\n",
    "notebook_dir = Path(os.getcwd())\n",
    "# 假设notebook在tests目录下\n",
    "parent_dir = notebook_dir.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.utils import logger, EmbeddingFunc\n",
    "from lightrag.llm.openai import (\n",
    "    gpt_4o_mini_complete,\n",
    "    gpt_4o_complete,\n",
    "    openai_embed,\n",
    "    openai_complete_if_cache\n",
    ")\n",
    "from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "from lightrag.llm.siliconcloud import siliconcloud_embedding\n",
    "from lightrag.chunking import set_hierarchical_chunking_config, get_hierarchical_chunking_config\n",
    "\n",
    "\n",
    "# 添加父目录到路径，以便导入LightRAG包\n",
    "# 在Jupyter Notebook中使用os.getcwd()代替__file__\n",
    "notebook_dir = Path(os.getcwd())\n",
    "# 假设notebook在tests目录下\n",
    "parent_dir = notebook_dir.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "%cd /Users/llp/opensource/LightRAG\n",
    "\n",
    "WORKING_DIR = \"data/TechnicalDemo\"\n",
    "\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "# 在导入部分添加\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 在代码的开头加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n",
    ") -> str:\n",
    "    return await openai_complete_if_cache(\n",
    "        \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "        \n",
    "        # \"Qwen/Qwen2.5-32B-Instruct\",\n",
    "        # \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "        # \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n",
    "        base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "# async def embedding_func(texts: list[str]) -> np.ndarray:\n",
    "#     return await siliconcloud_embedding(\n",
    "#         texts,\n",
    "#         model=\"netease-youdao/bce-embedding-base_v1\",\n",
    "#         api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n",
    "#         max_token_size=512,\n",
    "#     )\n",
    "\n",
    "async def embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    # 添加批处理大小限制，最大批大小为32\n",
    "    max_batch_size = 32\n",
    "    \n",
    "    # 如果输入小于或等于最大批大小，直接处理\n",
    "    if len(texts) <= max_batch_size:\n",
    "        return await siliconcloud_embedding(\n",
    "            texts,\n",
    "            model=\"netease-youdao/bce-embedding-base_v1\",\n",
    "            api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n",
    "            max_token_size=512,\n",
    "        )\n",
    "    \n",
    "    # 如果输入超过最大批大小，分批处理\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), max_batch_size):\n",
    "        batch_texts = texts[i:i+max_batch_size]\n",
    "        batch_embeddings = await siliconcloud_embedding(\n",
    "            batch_texts,\n",
    "            model=\"BAAI/bge-m3\",\n",
    "            api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n",
    "            max_token_size=512,\n",
    "        )\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # 合并所有批次的结果\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "technical_doc = open(os.path.join(\"tests\", \"technical_manual.md\")).read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightrag.base import ChunkingMode\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "\n",
    "\n",
    "async def get_embedding_dim():\n",
    "    test_embedding = await embedding_func([\"test\"])\n",
    "    return len(test_embedding[0])\n",
    "\n",
    "async def initialize_rag():\n",
    "    # Detect embedding dimension\n",
    "    embedding_dimension = await get_embedding_dim()\n",
    "    print(f\"Detected embedding dimension: {embedding_dimension}\")\n",
    "\n",
    "    # Initialize LightRAG with domain\n",
    "    rag = LightRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        llm_model_func=llm_model_func,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=embedding_dimension,\n",
    "            max_token_size=8192,\n",
    "            func=embedding_func,\n",
    "        ),\n",
    "        # 初始化时可以设置domain\n",
    "        domain=\"technical_manual\",\n",
    "    )\n",
    "\n",
    "    technical_manual_config = {\n",
    "        \"language\": \"中文\",\n",
    "        \"entity_types\": [\"部件\", \"故障原因\", \"故障现象\", \"维修步骤\"],\n",
    "        \"tuple_delimiter\": \"<|>\",\n",
    "        \"record_delimiter\": \"##\",\n",
    "        \"completion_delimiter\": \"<|COMPLETE|>\"\n",
    "    }\n",
    "    set_hierarchical_chunking_config(\n",
    "        heading_levels=3,        # 处理到 #### 级别标题\n",
    "        parent_level=2,          # ### 级别标题作为父文档\n",
    "        preprocess_attachments=False,  # 不预处理附件标题\n",
    "    )\n",
    "    rag.register_domain(\"technical_manual\", technical_manual_config)\n",
    "\n",
    "    rag.chunking_mode = ChunkingMode.HIREARCHIACL\n",
    "    # 导入并初始化pipeline状态\n",
    "    await initialize_pipeline_status()\n",
    " \n",
    "    # 初始化存储\n",
    "    await rag.initialize_storages()\n",
    "    \n",
    "    return rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Process 13161 Shared-Data created for Single Process\n",
      "INFO: Process 13161 initializing storage statuses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected embedding dimension: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'data/TechnicalDemo/vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'data/TechnicalDemo/vdb_relationships.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'data/TechnicalDemo/vdb_chunks.json'} 0 data\n",
      "INFO: Process 13161 Pipeline namespace initialized\n",
      "INFO: Process 13161 initialized updated flags for namespace: [full_docs]\n",
      "INFO: Process 13161 ready to initialize storage namespace: [full_docs]\n",
      "INFO: Process 13161 initialized updated flags for namespace: [text_chunks]\n",
      "INFO: Process 13161 ready to initialize storage namespace: [text_chunks]\n",
      "INFO: Process 13161 initialized updated flags for namespace: [entities]\n",
      "INFO: Process 13161 initialized updated flags for namespace: [relationships]\n",
      "INFO: Process 13161 initialized updated flags for namespace: [chunks]\n",
      "INFO: Process 13161 initialized updated flags for namespace: [chunk_entity_relation]\n",
      "INFO: Process 13161 initialized updated flags for namespace: [llm_response_cache]\n",
      "INFO: Process 13161 ready to initialize storage namespace: [llm_response_cache]\n",
      "INFO: Process 13161 initialized updated flags for namespace: [doc_status]\n",
      "INFO: Process 13161 ready to initialize storage namespace: [doc_status]\n",
      "INFO: Process 13161 storage namespace already initialized: [full_docs]\n",
      "INFO: Process 13161 storage namespace already initialized: [text_chunks]\n",
      "INFO: Process 13161 storage namespace already initialized: [llm_response_cache]\n",
      "INFO: Process 13161 storage namespace already initialized: [doc_status]\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'data/TechnicalDemo/vdb_chunks.json'} 0 data\n",
      "INFO: == Lock == Process 13161: Acquiring lock 'graph_db_lock' (async=True)\n",
      "INFO: == Lock == Process 13161: Lock 'graph_db_lock' acquired (async=True)\n",
      "INFO: == Lock == Process 13161: Releasing lock 'graph_db_lock' (async=True)\n",
      "INFO: == Lock == Process 13161: Lock 'graph_db_lock' released (async=True)\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'data/TechnicalDemo/vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'data/TechnicalDemo/vdb_relationships.json'} 0 data\n"
     ]
    }
   ],
   "source": [
    "rag = await initialize_rag()\n",
    "await rag.ainsert(technical_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模糊查询函数\n",
    "def find_closest_entity(query, entities):\n",
    "    \"\"\"查找最接近的实体\"\"\"\n",
    "    matches = []\n",
    "    for entity in entities:\n",
    "        # 移除引号和大写处理\n",
    "        clean_entity = entity.replace('\"', '')\n",
    "        if query.lower() in clean_entity.lower():\n",
    "            matches.append(entity)\n",
    "    return matches\n",
    "\n",
    "# 查看已提取的所有实体\n",
    "all_nodes = list(rag.chunk_entity_relation_graph._graph.nodes())\n",
    "print(\"知识图谱中的所有实体:\")\n",
    "for node in all_nodes:\n",
    "    print(node)\n",
    "# 使用模糊匹配\n",
    "search_term = \"空调噪音\"\n",
    "possible_matches = find_closest_entity(search_term, all_nodes)\n",
    "\n",
    "if possible_matches:\n",
    "    print(f\"找到可能匹配的实体: {possible_matches}\")\n",
    "    tech_entity = possible_matches[0].replace('\"', '')  # 使用第一个匹配项\n",
    "    print(f\"\\n=== 查询技术实体: {tech_entity} ===\")\n",
    "    tech_entity_info = await rag.get_entity_info(tech_entity)\n",
    "    # ... 其余代码保持不变\n",
    "else:\n",
    "    print(f\"未找到与 '{search_term}' 匹配的实体\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询实体和关系\n",
    "# 查询实体和关系\n",
    "medical_entity = \"糖尿病\"\n",
    "tech_entity = \"CRH380A\"\n",
    "\n",
    "# 查询医疗实体\n",
    "print(f\"\\n=== 查询医疗实体: {medical_entity} ===\")\n",
    "medical_entity_info = await rag.get_entity_info(medical_entity)\n",
    "\n",
    "if medical_entity_info[\"graph_data\"]:\n",
    "    print(f\"类型: {medical_entity_info['graph_data'].get('entity_type', 'N/A')}\")\n",
    "    print(f\"描述: {medical_entity_info['graph_data'].get('description', 'N/A')}\")\n",
    "    \n",
    "    # 使用正确的方法获取相关节点和边\n",
    "    # 格式化实体名称\n",
    "    formatted_entity = f'\"{medical_entity.upper()}\"'\n",
    "    \n",
    "    # 获取所有出边（从该实体指向其他实体的关系）\n",
    "    try:\n",
    "        # 获取与该实体相连的所有边\n",
    "        neighbors = list(rag.chunk_entity_relation_graph._graph.neighbors(formatted_entity))\n",
    "        \n",
    "        if neighbors:\n",
    "            print(\"\\n相关关系:\")\n",
    "            for neighbor in neighbors:\n",
    "                # 获取边的属性\n",
    "                edge_data = rag.chunk_entity_relation_graph._graph.get_edge_data(formatted_entity, neighbor)\n",
    "                if edge_data:\n",
    "                    print(f\"- 目标实体: {neighbor}\")\n",
    "                    print(f\"  描述: {edge_data.get('description', 'N/A')}\")\n",
    "                    print(f\"  关键词: {edge_data.get('keywords', 'N/A')}\")\n",
    "                    print(f\"  权重: {edge_data.get('weight', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"没有找到相关关系\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"获取关系时出错: {e}\")\n",
    "else:\n",
    "    print(f\"未找到实体: {medical_entity}\")\n",
    "\n",
    "# 查询技术实体    \n",
    "print(f\"\\n=== 查询技术实体: {tech_entity} ===\")\n",
    "tech_entity_info = await rag.get_entity_info(tech_entity)\n",
    "\n",
    "if tech_entity_info[\"graph_data\"]:\n",
    "    print(f\"类型: {tech_entity_info['graph_data'].get('entity_type', 'N/A')}\")\n",
    "    print(f\"描述: {tech_entity_info['graph_data'].get('description', 'N/A')}\")\n",
    "    \n",
    "    # 使用正确的方法获取相关节点和边\n",
    "    # 格式化实体名称\n",
    "    formatted_entity = f'\"{tech_entity.upper()}\"'\n",
    "    \n",
    "    # 获取所有出边（从该实体指向其他实体的关系）\n",
    "    try:\n",
    "        # 获取与该实体相连的所有边\n",
    "        neighbors = list(rag.chunk_entity_relation_graph._graph.neighbors(formatted_entity))\n",
    "        \n",
    "        if neighbors:\n",
    "            print(\"\\n相关关系:\")\n",
    "            for neighbor in neighbors:\n",
    "                # 获取边的属性\n",
    "                edge_data = rag.chunk_entity_relation_graph._graph.get_edge_data(formatted_entity, neighbor)\n",
    "                if edge_data:\n",
    "                    print(f\"- 目标实体: {neighbor}\")\n",
    "                    print(f\"  描述: {edge_data.get('description', 'N/A')}\")\n",
    "                    print(f\"  关键词: {edge_data.get('keywords', 'N/A')}\")\n",
    "                    print(f\"  权重: {edge_data.get('weight', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"没有找到相关关系\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"获取关系时出错: {e}\")\n",
    "else:\n",
    "    print(f\"未找到实体: {tech_entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param (QueryParam): 查询执行的配置参数。可通过param.mode指定查询模式:\n",
    "#     - naive: 基础查询模式,直接从文本块中检索相关内容\n",
    "#     - hierarchical: 层次化查询模式,考虑文档的层次结构关系进行检索\n",
    "#     - local: 基于本地知识图谱的查询模式\n",
    "#     - global: 基于全局知识图谱的查询模式  \n",
    "#     - hybrid: 混合查询模式,同时使用文本检索和知识图谱\n",
    "\n",
    "\n",
    "tech_manual_query_param = QueryParam(\n",
    "    mode=\"naive\",  # 混合知识图谱和向量查询\n",
    "    top_k=5,  # 检索更少但更精确的结果\n",
    "    disable_response_cache=True,\n",
    ")\n",
    "\n",
    "tech_system_prompt = \"\"\"你是一个专业的技术维修顾问。\n",
    "回答问题时请注意：\n",
    "1. 严格遵循技术参数（电压、压力、电流等数值）\n",
    "2. 清晰区分不同设备型号的特定要求\n",
    "3. 按正确顺序列出维修步骤\n",
    "4. 引用相应的手册章节\n",
    "请仅基于提供的文档内容回答，不要臆测信息。\"\"\"\n",
    "\n",
    "response = rag.query(\n",
    "    \"车门状态指示灯 (例如：开门指示灯、关门指示灯、故障指示灯) 显示错误或不亮\", \n",
    "    param=tech_manual_query_param,\n",
    "    # system_prompt=tech_system_prompt\n",
    ")\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_manual_query_param = QueryParam(\n",
    "    mode=\"hierarchical\",  # 混合知识图谱和向量查询\n",
    "    top_k=5,  # 检索更少但更精确的结果\n",
    "    disable_response_cache=True,\n",
    ")\n",
    "\n",
    "response = rag.query(\n",
    "    \"车门状态指示灯 (例如：开门指示灯、关门指示灯、故障指示灯) 显示错误或不亮\", \n",
    "    param=tech_manual_query_param,\n",
    "    # system_prompt=tech_system_prompt\n",
    ")\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_manual_query_param = QueryParam(\n",
    "    mode=\"local\",  # 混合知识图谱和向量查询\n",
    "    top_k=5,  # 检索更少但更精确的结果\n",
    "    disable_response_cache=True,\n",
    ")\n",
    "\n",
    "response = rag.query(\n",
    "    \"出现噪声过大时的处理方法\", \n",
    "    param=tech_manual_query_param,\n",
    "    # system_prompt=tech_system_prompt\n",
    ")\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_manual_query_param = QueryParam(\n",
    "    mode=\"global\",  # 混合知识图谱和向量查询\n",
    "    top_k=5,  # 检索更少但更精确的结果\n",
    "    disable_response_cache=True,\n",
    ")\n",
    "\n",
    "response = rag.query(\n",
    "    \"出现噪声过大时的处理方法\", \n",
    "    param=tech_manual_query_param,\n",
    "    # system_prompt=tech_system_prompt\n",
    ")\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_manual_query_param = QueryParam(\n",
    "    mode=\"mix\",  # 混合知识图谱和向量查询\n",
    "    top_k=5,  # 检索更少但更精确的结果\n",
    "    disable_response_cache=True,\n",
    ")\n",
    "\n",
    "response = rag.query(\n",
    "    \"出现噪声过大时的处理方法\", \n",
    "    param=tech_manual_query_param,\n",
    "    # system_prompt=tech_system_prompt\n",
    ")\n",
    "display(Markdown(response))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
