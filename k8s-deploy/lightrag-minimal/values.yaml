# LightRAG Minimal Configuration
# Matches docker-compose.minimal.yml setup

replicaCount: 1

image:
  repository: ghcr.io/hkuds/lightrag
  tag: 1.3.8
  pullPolicy: IfNotPresent

nameOverride: ""
fullnameOverride: ""

service:
  type: ClusterIP
  port: 9621
  targetPort: 9621

ingress:
  enabled: false
  className: ""
  clusterName: ""
  annotations: {}
  hosts:
    - host: lightrag-minimal.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

# Resource limits and requests
resources:
  limits:
    cpu: 2000m
    memory: 4Gi
  requests:
    cpu: 500m
    memory: 1Gi

# Persistence for data volumes
persistence:
  enabled: true
  ragStorage:
    accessMode: ReadWriteOnce
    size: 20Gi
    storageClass: ""
  inputs:
    accessMode: ReadWriteOnce
    size: 10Gi
    storageClass: ""

# PostgreSQL configuration (embedded chart with pgvector)
postgresql:
  enabled: true
  # Use pgvector image instead of standard PostgreSQL
  image:
    registry: docker.io
    repository: pgvector/pgvector
    tag: pg16
  auth:
    database: lightrag
    username: lightrag_user
    password: ""
    existingSecret: ""
    secretKeys:
      userPasswordKey: postgres-password
  primary:
    persistence:
      enabled: true
      size: 20Gi
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 250m
        memory: 512Mi
    initdb:
      scripts:
        00-pgvector.sql: |
          CREATE EXTENSION IF NOT EXISTS vector;

# LightRAG Environment Configuration
# This matches the minimal docker-compose setup
env:
  # Server configuration
  HOST: "0.0.0.0"
  PORT: "9621"

  # Web UI configuration
  WEBUI_TITLE: "LightRAG - Graph Knowledge Base"
  WEBUI_DESCRIPTION: "Simple and Fast Graph Based RAG System for Apolo Documentation"

  # LLM configuration (OpenRouter)
  LLM_BINDING: "openai"
  LLM_MODEL: "openai/gpt-4o"
  LLM_BINDING_HOST: "https://openrouter.ai/api/v1"
  # LLM_BINDING_API_KEY: Set via secret
  # OPENAI_API_KEY: Set via secret

  # Embedding configuration (Gemini)
  EMBEDDING_BINDING: "openai"
  EMBEDDING_MODEL: "gemini-embedding-001"
  EMBEDDING_DIM: "3072"
  EMBEDDING_BINDING_HOST: "https://generativelanguage.googleapis.com/v1beta/openai/"
  # EMBEDDING_BINDING_API_KEY: Set via secret

  # Storage configuration - Minimal setup
  LIGHTRAG_KV_STORAGE: "PGKVStorage"
  LIGHTRAG_VECTOR_STORAGE: "PGVectorStorage"
  LIGHTRAG_DOC_STATUS_STORAGE: "PGDocStatusStorage"
  LIGHTRAG_GRAPH_STORAGE: "NetworkXStorage" # Local storage, no external DB needed

  # PostgreSQL connection (internal service)
  POSTGRES_HOST: '{{ include "lightrag-minimal.fullname" . }}-postgresql'
  POSTGRES_PORT: "5432"
  POSTGRES_USER: "lightrag_user"
  POSTGRES_DATABASE: "lightrag"
  POSTGRES_WORKSPACE: "default"

  # LightRAG Processing Configuration
  ENABLE_LLM_CACHE: true
  ENABLE_LLM_CACHE_FOR_EXTRACT: true
  TIMEOUT: 240
  TEMPERATURE: 0
  MAX_ASYNC: 4
  MAX_TOKENS: 32768
  CHUNK_SIZE: 1200
  CHUNK_OVERLAP_SIZE: 100
  TOP_K: 60

# Secret configuration for API keys
secrets:
  # Create a secret with your API keys
  openaiApiKey: "" # Legacy field, kept for backward compatibility
  llmApiKey: "" # API key for LLM service (e.g., OpenRouter)
  embeddingApiKey: "" # API key for embedding service (e.g., Google Gemini)
  lightragApiKey: ""

auth:
  enabled: false
  accounts: []
  tokenSecret: ""


# Node selector and affinity
nodeSelector: {}
tolerations: []
affinity: {}

# Security context
securityContext: {}
podSecurityContext: {}

# Service account
serviceAccount:
  create: true
  annotations: {}
  name: ""

# Pod annotations
podAnnotations: {}

# Auto scaling (disabled by default for minimal setup)
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Health checks
healthCheck:
  enabled: true
  path: "/health"
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 5
