<div align="center">

<div style="margin: 20px 0;">
  <img src="./assets/logo.png" width="120" height="120" alt="LightRAG Logo" style="border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);">
</div>

# ğŸš€ LightRAG: ç®€å•ä¸”å¿«é€Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶

<div align="center">
    <a href="https://trendshift.io/repositories/13043" target="_blank"><img src="https://trendshift.io/api/badge/repositories/13043" alt="HKUDS%2FLightRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>

<div align="center">
  <div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"></div>
</div>

<div align="center">
  <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;">
    <p>
      <a href='https://github.com/HKUDS/LightRAG'><img src='https://img.shields.io/badge/ğŸ”¥é¡¹ç›®-ä¸»é¡µ-00d9ff?style=for-the-badge&logo=github&logoColor=white&labelColor=1a1a2e'></a>
      <a href='https://arxiv.org/abs/2410.05779'><img src='https://img.shields.io/badge/ğŸ“„arXiv-2410.05779-ff6b6b?style=for-the-badge&logo=arxiv&logoColor=white&labelColor=1a1a2e'></a>
      <a href="https://github.com/HKUDS/LightRAG/stargazers"><img src='https://img.shields.io/github/stars/HKUDS/LightRAG?color=00d9ff&style=for-the-badge&logo=star&logoColor=white&labelColor=1a1a2e' /></a>
    </p>
    <p>
      <img src="https://img.shields.io/badge/ğŸPython-3.10-4ecdc4?style=for-the-badge&logo=python&logoColor=white&labelColor=1a1a2e">
      <a href="https://pypi.org/project/lightrag-hku/"><img src="https://img.shields.io/pypi/v/lightrag-hku.svg?style=for-the-badge&logo=pypi&logoColor=white&labelColor=1a1a2e&color=ff6b6b"></a>
    </p>
    <p>
      <a href="https://discord.gg/yF2MmDJyGJ"><img src="https://img.shields.io/badge/ğŸ’¬Discord-ç¤¾åŒº-7289da?style=for-the-badge&logo=discord&logoColor=white&labelColor=1a1a2e"></a>
      <a href="https://github.com/HKUDS/LightRAG/issues/285"><img src="https://img.shields.io/badge/ğŸ’¬å¾®ä¿¡ç¾¤-äº¤æµ-07c160?style=for-the-badge&logo=wechat&logoColor=white&labelColor=1a1a2e"></a>
    </p>
    <p>
      <a href="README-zh.md"><img src="https://img.shields.io/badge/ğŸ‡¨ğŸ‡³ä¸­æ–‡ç‰ˆ-1a1a2e?style=for-the-badge"></a>
      <a href="README.md"><img src="https://img.shields.io/badge/ğŸ‡ºğŸ‡¸English-1a1a2e?style=for-the-badge"></a>
    </p>
    <p>
      <a href="https://pepy.tech/projects/lightrag-hku"><img src="https://static.pepy.tech/personalized-badge/lightrag-hku?period=total&units=INTERNATIONAL_SYSTEM&left_color=BLACK&right_color=GREEN&left_text=downloads"></a>
    </p>
  </div>
</div>

</div>

<div align="center" style="margin: 30px 0;">
  <img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="800">
</div>

<div align="center" style="margin: 30px 0;">
    <img src="./README.assets/b2aaf634151b4706892693ffb43d9093.png" width="800" alt="LightRAG Diagram">
</div>

---

<div align="center">
  <table>
    <tr>
      <td style="vertical-align: middle;">
        <img src="./assets/LiteWrite.png"
             width="56"
             height="56"
             alt="LiteWrite"
             style="border-radius: 12px;" />
      </td>
      <td style="vertical-align: middle; padding-left: 12px;">
        <a href="https://litewrite.ai">
          <img src="https://img.shields.io/badge/ğŸš€%20LiteWrite-AI%20åŸç”Ÿ%20LaTeX%20ç¼–è¾‘å™¨-ff6b6b?style=for-the-badge&logoColor=white&labelColor=1a1a2e">
        </a>
      </td>
    </tr>
  </table>
</div>

---

## ğŸ‰ æ–°é—»
- [2025.11]ğŸ¯[æ–°åŠŸèƒ½]: é›†æˆäº† **RAGAS è¯„ä¼°**å’Œ **Langfuse è¿½è¸ª**ã€‚æ›´æ–°äº† API ä»¥åœ¨æŸ¥è¯¢ç»“æœä¸­è¿”å›å¬å›ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒä¸Šä¸‹æ–‡ç²¾åº¦æŒ‡æ ‡ã€‚
- [2025.10]ğŸ¯[å¯æ‰©å±•æ€§å¢å¼º]: æ¶ˆé™¤äº†å¤„ç†ç“¶é¢ˆï¼Œä»¥é«˜æ•ˆæ”¯æŒ**å¤§è§„æ¨¡æ•°æ®é›†**ã€‚
- [2025.09]ğŸ¯[æ–°åŠŸèƒ½]: æ˜¾è‘—æå‡äº† Qwen3-30B-A3B ç­‰**å¼€æº LLM** çš„çŸ¥è¯†å›¾è°±æå–å‡†ç¡®æ€§ã€‚
- [2025.08]ğŸ¯[æ–°åŠŸèƒ½]: ç°å·²æ”¯æŒ **Reranker**ï¼Œæ˜¾è‘—æå‡æ··åˆæŸ¥è¯¢æ€§èƒ½ï¼ˆå·²è®¾ä¸ºé»˜è®¤æŸ¥è¯¢æ¨¡å¼ï¼‰ã€‚
- [2025.08]ğŸ¯[æ–°åŠŸèƒ½]: æ·»åŠ äº†**æ–‡æ¡£åˆ é™¤**åŠŸèƒ½ï¼Œå¹¶æ”¯æŒè‡ªåŠ¨é‡æ–°ç”ŸæˆçŸ¥è¯†å›¾è°±ï¼Œä»¥ç¡®ä¿æœ€ä½³æŸ¥è¯¢æ€§èƒ½ã€‚
- [2025.06]ğŸ¯[æ–°å‘å¸ƒ]: æˆ‘ä»¬çš„å›¢é˜Ÿå‘å¸ƒäº† [RAG-Anything](https://github.com/HKUDS/RAG-Anything) â€”â€” ä¸€ä¸ªç”¨äºæ— ç¼å¤„ç†æ–‡æœ¬ã€å›¾åƒã€è¡¨æ ¼å’Œæ–¹ç¨‹å¼çš„**å…¨åŠŸèƒ½å¤šæ¨¡æ€ RAG** ç³»ç»Ÿã€‚
- [2025.06]ğŸ¯[æ–°åŠŸèƒ½]: LightRAG ç°å·²é›†æˆ [RAG-Anything](https://github.com/HKUDS/RAG-Anything)ï¼Œæ”¯æŒå…¨é¢çš„å¤šæ¨¡æ€æ•°æ®å¤„ç†ï¼Œå®ç°å¯¹ PDFã€å›¾åƒã€Office æ–‡æ¡£ã€è¡¨æ ¼å’Œå…¬å¼ç­‰å¤šç§æ ¼å¼çš„æ— ç¼æ–‡æ¡£è§£æå’Œ RAG èƒ½åŠ›ã€‚è¯¦è§[å¤šæ¨¡æ€æ–‡æ¡£å¤„ç†éƒ¨åˆ†](https://github.com/HKUDS/LightRAG/?tab=readme-ov-file#multimodal-document-processing-rag-anything-integration)ã€‚
- [2025.03]ğŸ¯[æ–°åŠŸèƒ½]: LightRAG ç°å·²æ”¯æŒå¼•ç”¨åŠŸèƒ½ï¼Œå®ç°äº†å‡†ç¡®çš„æºå½’å› å’Œå¢å¼ºçš„æ–‡æ¡£å¯è¿½æº¯æ€§ã€‚
- [2025.02]ğŸ¯[æ–°åŠŸèƒ½]: ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ MongoDB ä½œä¸ºä¸€ä½“åŒ–å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼Œå®ç°ç»Ÿä¸€çš„æ•°æ®ç®¡ç†ã€‚
- [2025.02]ğŸ¯[æ–°å‘å¸ƒ]: æˆ‘ä»¬çš„å›¢é˜Ÿå‘å¸ƒäº† [VideoRAG](https://github.com/HKUDS/VideoRAG) â€”â€” ä¸€ä¸ªç”¨äºç†è§£è¶…é•¿ä¸Šä¸‹æ–‡è§†é¢‘çš„ RAG ç³»ç»Ÿã€‚
- [2025.01]ğŸ¯[æ–°å‘å¸ƒ]: æˆ‘ä»¬çš„å›¢é˜Ÿå‘å¸ƒäº† [MiniRAG](https://github.com/HKUDS/MiniRAG)ï¼Œä½¿ç”¨å°å‹æ¨¡å‹ç®€åŒ– RAGã€‚
- [2025.01]ğŸ¯ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ PostgreSQL ä½œä¸ºä¸€ä½“åŒ–å­˜å‚¨è§£å†³æ–¹æ¡ˆè¿›è¡Œæ•°æ®ç®¡ç†ã€‚
- [2024.11]ğŸ¯[æ–°èµ„æº]: LightRAG çš„ç»¼åˆæŒ‡å—ç°å·²åœ¨ [LearnOpenCV](https://learnopencv.com/lightrag) ä¸Šå‘å¸ƒ â€”â€” æ¢ç´¢æ·±å…¥çš„æ•™ç¨‹å’Œæœ€ä½³å®è·µã€‚éå¸¸æ„Ÿè°¢åšå®¢ä½œè€…çš„æ°å‡ºè´¡çŒ®ï¼
- [2024.11]ğŸ¯[æ–°åŠŸèƒ½]: æ¨å‡º LightRAG WebUI â€”â€” ä¸€ä¸ªå…è®¸æ‚¨é€šè¿‡ç›´è§‚çš„ Web ç•Œé¢æ’å…¥ã€æŸ¥è¯¢å’Œå¯è§†åŒ– LightRAG çŸ¥è¯†çš„ä»ªè¡¨æ¿ã€‚
- [2024.11]ğŸ¯[æ–°åŠŸèƒ½]: ç°åœ¨æ‚¨å¯ä»¥[ä½¿ç”¨ Neo4J è¿›è¡Œå­˜å‚¨](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#using-neo4j-for-storage) â€”â€” å¼€å¯å›¾æ•°æ®åº“æ”¯æŒã€‚
- [2024.10]ğŸ¯[æ–°åŠŸèƒ½]: æˆ‘ä»¬æ·»åŠ äº† [LightRAG ä»‹ç»è§†é¢‘](https://youtu.be/oageL-1I0GE) çš„é“¾æ¥ â€”â€” æ¼”ç¤º LightRAG çš„å„é¡¹åŠŸèƒ½ã€‚æ„Ÿè°¢ä½œè€…çš„æ°å‡ºè´¡çŒ®ï¼
- [2024.10]ğŸ¯[æ–°é¢‘é“]: æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª [Discord é¢‘é“](https://discord.gg/yF2MmDJyGJ)ï¼ğŸ’¬ æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒºè¿›è¡Œåˆ†äº«ã€è®¨è®ºå’Œåä½œï¼ ğŸ‰ğŸ‰
- [2024.10]ğŸ¯[æ–°åŠŸèƒ½]: LightRAG ç°åœ¨æ”¯æŒ [Ollama æ¨¡å‹](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start)ï¼

<details>
  <summary style="font-size: 1.4em; font-weight: bold; cursor: pointer; display: list-item;">
    ç®—æ³•æµç¨‹å›¾
  </summary>

![LightRAGç´¢å¼•æµç¨‹å›¾](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-VectorDB-Json-KV-Store-Indexing-Flowchart-scaled.jpg)
*å›¾1ï¼šLightRAGç´¢å¼•æµç¨‹å›¾ - å›¾ç‰‡æ¥æºï¼š[Source](https://learnopencv.com/lightrag/)*
![LightRAGæ£€ç´¢å’ŒæŸ¥è¯¢æµç¨‹å›¾](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-Querying-Flowchart-Dual-Level-Retrieval-Generation-Knowledge-Graphs-scaled.jpg)
*å›¾2ï¼šLightRAGæ£€ç´¢å’ŒæŸ¥è¯¢æµç¨‹å›¾ - å›¾ç‰‡æ¥æºï¼š[Source](https://learnopencv.com/lightrag/)*

</details>

## å®‰è£…

> **ğŸ’¡ ä½¿ç”¨ uv è¿›è¡ŒåŒ…ç®¡ç†**: æœ¬é¡¹ç›®ä½¿ç”¨ [uv](https://docs.astral.sh/uv/) è¿›è¡Œå¿«é€Ÿå¯é çš„ Python åŒ…ç®¡ç†ã€‚
> é¦–å…ˆå®‰è£… uv: `curl -LsSf https://astral.sh/uv/install.sh | sh` (Unix/macOS) æˆ– `powershell -c "irm https://astral.sh/uv/install.ps1 | iex"` (Windows)
>
> **æ³¨æ„**ï¼šå¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ pipï¼Œä½†ä¸ºäº†è·å¾—æ›´å¥½çš„æ€§èƒ½ and æ›´å¯é çš„ä¾èµ–ç®¡ç†ï¼Œå»ºè®®ä½¿ç”¨ uvã€‚
>
> **ğŸ“¦ ç¦»çº¿éƒ¨ç½²**: å¯¹äºç¦»çº¿æˆ–éš”ç¦»ç¯å¢ƒï¼Œè¯·å‚é˜…[ç¦»çº¿éƒ¨ç½²æŒ‡å—](./docs/OfflineDeployment.md)ï¼Œäº†è§£é¢„å®‰è£…æ‰€æœ‰ä¾èµ–é¡¹å’Œç¼“å­˜æ–‡ä»¶çš„è¯´æ˜ã€‚

### å®‰è£…LightRAGæœåŠ¡å™¨

LightRAGæœåŠ¡å™¨æ—¨åœ¨æä¾›Web UIå’ŒAPIæ”¯æŒã€‚Web UIä¾¿äºæ–‡æ¡£ç´¢å¼•ã€çŸ¥è¯†å›¾è°±æ¢ç´¢å’Œç®€å•çš„RAGæŸ¥è¯¢ç•Œé¢ã€‚LightRAGæœåŠ¡å™¨è¿˜æä¾›å…¼å®¹Ollamaçš„æ¥å£ï¼Œæ—¨åœ¨å°†LightRAGæ¨¡æ‹Ÿä¸ºOllamaèŠå¤©æ¨¡å‹ã€‚è¿™ä½¿å¾—AIèŠå¤©æœºå™¨äººï¼ˆå¦‚Open WebUIï¼‰å¯ä»¥è½»æ¾è®¿é—®LightRAGã€‚

* ä»PyPIå®‰è£…

```bash
### ä½¿ç”¨ uv å®‰è£… LightRAG æœåŠ¡å™¨ï¼ˆä½œä¸ºå·¥å…·ï¼Œæ¨è)
uv tool install "lightrag-hku[api]"

### æˆ–ä½¿ç”¨ pip
# python -m venv .venv
# source .venv/bin/activate  # Windows: .venv\Scripts\activate
# pip install "lightrag-hku[api]"

### æ„å»ºå‰ç«¯ä»£ç 
cd lightrag_webui
bun install --frozen-lockfile
bun run build
cd ..

# é…ç½® env æ–‡ä»¶
cp env.example .env  # ä½¿ç”¨ä½ çš„LLMå’ŒEmbeddingæ¨¡å‹è®¿é—®å‚æ•°æ›´æ–°.envæ–‡ä»¶
# å¯åŠ¨API-WebUIæœåŠ¡
lightrag-server
```

æ³¨æ„ï¼šè¿è¡Œ `lightrag-server` æ—¶ä¼šä»å½“å‰å·¥ä½œç›®å½•è¯»å– `.env`ã€‚å¦‚æœä½ æ˜¯é€šè¿‡å…¨å±€å·¥å…·å®‰è£…ï¼ˆä¾‹å¦‚ `uv tool install "lightrag-hku[api]"`ï¼‰ï¼Œæœ¬åœ°ç›®å½•ä¸ä¼šåŒ…å« `env.example`ã€‚æ­¤æ—¶è¯·æ‰‹åŠ¨åˆ›å»º `.env`ï¼Œæˆ–ä»æºç ä»“åº“ä¸­å¤åˆ¶ `env.example`ã€‚

* ä»æºä»£ç å®‰è£…

```bash
git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG

# ä½¿ç”¨ uv (æ¨è)
# æ³¨æ„: uv sync ä¼šè‡ªåŠ¨åœ¨ .venv/ ç›®å½•åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv sync --extra api
source .venv/bin/activate  # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (Linux/macOS)
# Windows ç³»ç»Ÿ: .venv\Scripts\activate

### æˆ–ä½¿ç”¨ pip å’Œè™šæ‹Ÿç¯å¢ƒ
# python -m venv .venv
# source .venv/bin/activate  # Windows: .venv\Scripts\activate
# pip install -e ".[api]"

# æ„å»ºå‰ç«¯ä»£ç 
cd lightrag_webui
bun install --frozen-lockfile
bun run build
cd ..

# é…ç½® env æ–‡ä»¶
cp env.example .env  # ä½¿ç”¨ä½ çš„LLMå’ŒEmbeddingæ¨¡å‹è®¿é—®å‚æ•°æ›´æ–°.envæ–‡ä»¶
# å¯åŠ¨API-WebUIæœåŠ¡
lightrag-server
```

* ä½¿ç”¨ Docker Compose å¯åŠ¨ LightRAG æœåŠ¡å™¨

```bash
git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
cp env.example .env  # ä½¿ç”¨ä½ çš„LLMå’ŒEmbeddingæ¨¡å‹è®¿é—®å‚æ•°æ›´æ–°.envæ–‡ä»¶
# modify LLM and Embedding settings in .env
docker compose up
```

> åœ¨æ­¤è·å–LightRAG dockeré•œåƒå†å²ç‰ˆæœ¬: [LightRAG Docker Images]( https://github.com/HKUDS/LightRAG/pkgs/container/lightrag)

### å®‰è£…LightRAG Core

* ä»æºä»£ç å®‰è£…ï¼ˆæ¨èï¼‰

```bash
cd LightRAG
# æ³¨æ„: uv sync ä¼šè‡ªåŠ¨åœ¨ .venv/ ç›®å½•åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv sync
source .venv/bin/activate  # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (Linux/macOS)
# Windows ç³»ç»Ÿ: .venv\Scripts\activate

# æˆ–: pip install -e .
```

* ä»PyPIå®‰è£…

```bash
uv pip install lightrag-hku
# æˆ–: pip install lightrag-hku
```

## å¿«é€Ÿå¼€å§‹

### LightRAGçš„LLMåŠé…å¥—æŠ€æœ¯æ ˆè¦æ±‚

LightRAGå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›è¦æ±‚è¿œé«˜äºä¼ ç»ŸRAGï¼Œå› ä¸ºå®ƒéœ€è¦LLMæ‰§è¡Œæ–‡æ¡£ä¸­çš„å®ä½“å…³ç³»æŠ½å–ä»»åŠ¡ã€‚é…ç½®åˆé€‚çš„Embeddingå’ŒRerankeræ¨¡å‹å¯¹æé«˜æŸ¥è¯¢è¡¨ç°ä¹Ÿè‡³å…³é‡è¦ã€‚

- **LLMé€‰å‹**ï¼š
  - æ¨èé€‰ç”¨å‚æ•°é‡è‡³å°‘ä¸º32Bçš„LLMã€‚
  - ä¸Šä¸‹æ–‡é•¿åº¦è‡³å°‘ä¸º32KBï¼Œæ¨èè¾¾åˆ°64KBã€‚
  - åœ¨æ–‡æ¡£ç´¢å¼•é˜¶æ®µä¸å»ºè®®é€‰æ‹©æ¨ç†æ¨¡å‹ã€‚
  - åœ¨æŸ¥è¯¢é˜¶æ®µå»ºè®®é€‰æ‹©æ¯”ç´¢å¼•é˜¶æ®µèƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹ï¼Œä»¥è¾¾åˆ°æ›´é«˜çš„æŸ¥è¯¢æ•ˆæœã€‚
- **Embeddingæ¨¡å‹**ï¼š
  - é«˜æ€§èƒ½çš„Embeddingæ¨¡å‹å¯¹RAGè‡³å…³é‡è¦ã€‚
  - æ¨èä½¿ç”¨ä¸»æµçš„å¤šè¯­è¨€Embeddingæ¨¡å‹ï¼Œä¾‹å¦‚ï¼šBAAI/bge-m3 å’Œ text-embedding-3-largeã€‚
  - **é‡è¦æç¤º**ï¼šåœ¨æ–‡æ¡£ç´¢å¼•å‰å¿…é¡»ç¡®å®šä½¿ç”¨çš„Embeddingæ¨¡å‹ï¼Œä¸”åœ¨æ–‡æ¡£æŸ¥è¯¢é˜¶æ®µå¿…é¡»æ²¿ç”¨ä¸ç´¢å¼•é˜¶æ®µç›¸åŒçš„æ¨¡å‹ã€‚æœ‰äº›å­˜å‚¨ï¼ˆä¾‹å¦‚PostgreSQLï¼‰åœ¨é¦–æ¬¡å»ºç«‹æ•°è¡¨çš„æ—¶å€™éœ€è¦ç¡®å®šå‘é‡ç»´åº¦ï¼Œå› æ­¤æ›´æ¢Embeddingæ¨¡å‹åéœ€è¦åˆ é™¤å‘é‡ç›¸å…³åº“è¡¨ï¼Œä»¥ä¾¿è®©LightRAGé‡å»ºæ–°çš„åº“è¡¨ã€‚
- **Rerankeræ¨¡å‹é…ç½®**ï¼š
  - é…ç½®Rerankeræ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—æå‡LightRAGçš„æ£€ç´¢æ•ˆæœã€‚
  - å¯ç”¨Rerankeræ¨¡å‹åï¼Œæ¨èå°†â€œmixæ¨¡å¼â€è®¾ä¸ºé»˜è®¤æŸ¥è¯¢æ¨¡å¼ã€‚
  - æ¨èé€‰ç”¨ä¸»æµçš„Rerankeræ¨¡å‹ï¼Œä¾‹å¦‚ï¼šBAAI/bge-reranker-v2-m3 æˆ– Jina ç­‰æœåŠ¡å•†æä¾›çš„æ¨¡å‹ã€‚

### ä½¿ç”¨LightRAGæœåŠ¡å™¨

**æœ‰å…³LightRAGæœåŠ¡å™¨çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[LightRAGæœåŠ¡å™¨](./lightrag/api/README.md)ã€‚**

### ä½¿ç”¨LightRAG Core

LightRAGæ ¸å¿ƒåŠŸèƒ½çš„ç¤ºä¾‹ä»£ç è¯·å‚è§`examples`ç›®å½•ã€‚æ‚¨è¿˜å¯å‚ç…§[è§†é¢‘](https://www.youtube.com/watch?v=g21royNJ4fw)è§†é¢‘å®Œæˆç¯å¢ƒé…ç½®ã€‚è‹¥å·²æŒæœ‰OpenAI APIå¯†é’¥ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿è¡Œæ¼”ç¤ºä»£ç ï¼š

```bash
### you should run the demo code with project folder
cd LightRAG
### provide your API-KEY for OpenAI
export OPENAI_API_KEY="sk-...your_opeai_key..."
### download the demo document of "A Christmas Carol" by Charles Dickens
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt > ./book.txt
### run the demo code
python examples/lightrag_openai_demo.py
```

å¦‚éœ€æµå¼å“åº”ç¤ºä¾‹çš„å®ç°ä»£ç ï¼Œè¯·å‚é˜… `examples/lightrag_openai_compatible_demo.py`ã€‚è¿è¡Œå‰ï¼Œè¯·ç¡®ä¿æ ¹æ®éœ€æ±‚ä¿®æ”¹ç¤ºä¾‹ä»£ç ä¸­çš„LLMåŠåµŒå…¥æ¨¡å‹é…ç½®ã€‚

**æ³¨æ„1**ï¼šåœ¨è¿è¡Œdemoç¨‹åºçš„æ—¶å€™éœ€è¦æ³¨æ„ï¼Œä¸åŒçš„æµ‹è¯•ç¨‹åºå¯èƒ½ä½¿ç”¨çš„æ˜¯ä¸åŒçš„embeddingæ¨¡å‹ï¼Œæ›´æ¢ä¸åŒçš„embedingæ¨¡å‹çš„æ—¶å€™éœ€è¦æŠŠæ¸…ç©ºæ•°æ®ç›®å½•ï¼ˆ`./dickens`ï¼‰ï¼Œå¦åˆ™å±‚åºæ‰§è¡Œä¼šå‡ºé”™ã€‚å¦‚æœä½ æƒ³ä¿ç•™LLMç¼“å­˜ï¼Œå¯ä»¥åœ¨æ¸…é™¤æ•°æ®ç›®å½•æ—¶ä¿ç•™`kv_store_llm_response_cache.json`æ–‡ä»¶ã€‚

**æ³¨æ„2**ï¼šå®˜æ–¹æ”¯æŒçš„ç¤ºä¾‹ä»£ç ä»…ä¸º `lightrag_openai_demo.py` å’Œ `lightrag_openai_compatible_demo.py` ä¸¤ä¸ªæ–‡ä»¶ã€‚å…¶ä»–ç¤ºä¾‹æ–‡ä»¶å‡ä¸ºç¤¾åŒºè´¡çŒ®å†…å®¹ï¼Œå°šæœªç»è¿‡å®Œæ•´æµ‹è¯•ä¸ä¼˜åŒ–ã€‚

## ä½¿ç”¨LightRAG Coreè¿›è¡Œç¼–ç¨‹

> âš ï¸ **å¦‚æœæ‚¨å¸Œæœ›å°†LightRAGé›†æˆåˆ°æ‚¨çš„é¡¹ç›®ä¸­ï¼Œå»ºè®®æ‚¨ä½¿ç”¨LightRAG Serveræä¾›çš„REST API**ã€‚LightRAG Coreé€šå¸¸ç”¨äºåµŒå…¥å¼åº”ç”¨ï¼Œæˆ–ä¾›å¸Œæœ›è¿›è¡Œç ”ç©¶ä¸è¯„ä¼°çš„å­¦è€…ä½¿ç”¨ã€‚

### âš ï¸ é‡è¦ï¼šåˆå§‹åŒ–è¦æ±‚

LightRAG åœ¨ä½¿ç”¨å‰éœ€è¦æ˜¾å¼åˆå§‹åŒ–ã€‚ åˆ›å»º LightRAG å®ä¾‹åï¼Œæ‚¨å¿…é¡»è°ƒç”¨ await rag.initialize_storages()ï¼Œå¦åˆ™å°†å‡ºç°é”™è¯¯ã€‚

### ä¸€ä¸ªç®€å•ç¨‹åº

ä»¥ä¸‹Pythonä»£ç ç‰‡æ®µæ¼”ç¤ºäº†å¦‚ä½•åˆå§‹åŒ–LightRAGã€æ’å…¥æ–‡æœ¬å¹¶è¿›è¡ŒæŸ¥è¯¢ï¼š

```python
import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed
from lightrag.utils import setup_logger

setup_logger("lightrag", level="INFO")

WORKING_DIR = "./rag_storage"
if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
    )
    # IMPORTANT: Both initialization calls are required!
    await rag.initialize_storages()  # Initialize storage backends
    return rag

async def main():
    try:
        # åˆå§‹åŒ–RAGå®ä¾‹
        rag = await initialize_rag()
        await rag.ainsert("Your text")

        # æ‰§è¡Œæ··åˆæ£€ç´¢
        mode = "hybrid"
        print(
          await rag.aquery(
              "What are the top themes in this story?",
              param=QueryParam(mode=mode)
          )
        )

    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        if rag:
            await rag.finalize_storages()

if __name__ == "__main__":
    asyncio.run(main())
```

é‡è¦è¯´æ˜ï¼š

- è¿è¡Œè„šæœ¬å‰è¯·å…ˆå¯¼å‡ºä½ çš„OPENAI_API_KEYç¯å¢ƒå˜é‡ã€‚
- è¯¥ç¨‹åºä½¿ç”¨LightRAGçš„é»˜è®¤å­˜å‚¨è®¾ç½®ï¼Œæ‰€æœ‰æ•°æ®å°†æŒä¹…åŒ–åœ¨WORKING_DIR/rag_storageç›®å½•ä¸‹ã€‚
- è¯¥ç¤ºä¾‹ä»…å±•ç¤ºäº†åˆå§‹åŒ–LightRAGå¯¹è±¡çš„æœ€ç®€å•æ–¹å¼ï¼šæ³¨å…¥embeddingå’ŒLLMå‡½æ•°ï¼Œå¹¶åœ¨åˆ›å»ºLightRAGå¯¹è±¡ååˆå§‹åŒ–å­˜å‚¨å’Œç®¡é“çŠ¶æ€ã€‚

### LightRAGåˆå§‹åŒ–å‚æ•°

ä»¥ä¸‹æ˜¯å®Œæ•´çš„LightRAGå¯¹è±¡åˆå§‹åŒ–å‚æ•°æ¸…å•ï¼š

<details>
<summary> å‚æ•° </summary>

| **å‚æ•°** | **ç±»å‹** | **è¯´æ˜** | **é»˜è®¤å€¼** |
| -------------- | ---------- | ----------------- | ------------- |
| **working_dir** | `str` | å­˜å‚¨ç¼“å­˜çš„ç›®å½• | `lightrag_cache+timestamp` |
| **workspace** | str | ç”¨äºä¸åŒ LightRAG å®ä¾‹ä¹‹é—´æ•°æ®éš”ç¦»çš„å·¥ä½œåŒºåç§° | |
| **kv_storage** | `str` | Storage type for documents and text chunks. Supported types: `JsonKVStorage`,`PGKVStorage`,`RedisKVStorage`,`MongoKVStorage` | `JsonKVStorage` |
| **vector_storage** | `str` | Storage type for embedding vectors. Supported types: `NanoVectorDBStorage`,`PGVectorStorage`,`MilvusVectorDBStorage`,`ChromaVectorDBStorage`,`FaissVectorDBStorage`,`MongoVectorDBStorage`,`QdrantVectorDBStorage` | `NanoVectorDBStorage` |
| **graph_storage** | `str` | Storage type for graph edges and nodes. Supported types: `NetworkXStorage`,`Neo4JStorage`,`PGGraphStorage`,`AGEStorage` | `NetworkXStorage` |
| **doc_status_storage** | `str` | Storage type for documents process status. Supported types: `JsonDocStatusStorage`,`PGDocStatusStorage`,`MongoDocStatusStorage` | `JsonDocStatusStorage` |
| **chunk_token_size** | `int` | æ‹†åˆ†æ–‡æ¡£æ—¶æ¯ä¸ªå—çš„æœ€å¤§ä»¤ç‰Œå¤§å° | `1200` |
| **chunk_overlap_token_size** | `int` | æ‹†åˆ†æ–‡æ¡£æ—¶ä¸¤ä¸ªå—ä¹‹é—´çš„é‡å ä»¤ç‰Œå¤§å° | `100` |
| **tokenizer** | `Tokenizer` | ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸º tokensï¼ˆæ•°å­—ï¼‰ä»¥åŠä½¿ç”¨éµå¾ª TokenizerInterface åè®®çš„ .encode() å’Œ .decode() å‡½æ•°å°† tokens è½¬æ¢å›æ–‡æœ¬çš„å‡½æ•°ã€‚ å¦‚æœæ‚¨ä¸æŒ‡å®šï¼Œå®ƒå°†ä½¿ç”¨é»˜è®¤çš„ Tiktoken tokenizerã€‚ | `TiktokenTokenizer` |
| **tiktoken_model_name** | `str` | å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯é»˜è®¤çš„ Tiktoken tokenizerï¼Œé‚£ä¹ˆè¿™æ˜¯è¦ä½¿ç”¨çš„ç‰¹å®š Tiktoken æ¨¡å‹çš„åç§°ã€‚å¦‚æœæ‚¨æä¾›è‡ªå·±çš„ tokenizerï¼Œåˆ™å¿½ç•¥æ­¤è®¾ç½®ã€‚ | `gpt-4o-mini` |
| **entity_extract_max_gleaning** | `int` | å®ä½“æå–è¿‡ç¨‹ä¸­çš„å¾ªç¯æ¬¡æ•°ï¼Œé™„åŠ å†å²æ¶ˆæ¯ | `1` |
| **node_embedding_algorithm** | `str` | èŠ‚ç‚¹åµŒå…¥ç®—æ³•ï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰ | `node2vec` |
| **node2vec_params** | `dict` | èŠ‚ç‚¹åµŒå…¥çš„å‚æ•° | `{"dimensions": 1536,"num_walks": 10,"walk_length": 40,"window_size": 2,"iterations": 3,"random_seed": 3,}` |
| **embedding_func** | `EmbeddingFunc` | ä»æ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡çš„å‡½æ•° | `openai_embed` |
| **embedding_batch_num** | `int` | åµŒå…¥è¿‡ç¨‹çš„æœ€å¤§æ‰¹é‡å¤§å°ï¼ˆæ¯æ‰¹å‘é€å¤šä¸ªæ–‡æœ¬ï¼‰ | `32` |
| **embedding_func_max_async** | `int` | æœ€å¤§å¹¶å‘å¼‚æ­¥åµŒå…¥è¿›ç¨‹æ•° | `16` |
| **llm_model_func** | `callable` | LLMç”Ÿæˆçš„å‡½æ•° | `gpt_4o_mini_complete` |
| **llm_model_name** | `str` | ç”¨äºç”Ÿæˆçš„LLMæ¨¡å‹åç§° | `meta-llama/Llama-3.2-1B-Instruct` |
| **summary_context_size** | `int` | åˆå¹¶å®ä½“å…³ç³»æ‘˜è¦æ—¶é€ç»™LLMçš„æœ€å¤§ä»¤ç‰Œæ•° | `10000`ï¼ˆç”±ç¯å¢ƒå˜é‡ SUMMARY_MAX_CONTEXT è®¾ç½®ï¼‰ |
| **summary_max_tokens** | `int` | åˆå¹¶å®ä½“å…³ç³»æè¿°çš„æœ€å¤§ä»¤ç‰Œæ•°é•¿åº¦ | `500`ï¼ˆç”±ç¯å¢ƒå˜é‡ SUMMARY_MAX_TOKENS è®¾ç½®ï¼‰ |
| **llm_model_max_async** | `int` | æœ€å¤§å¹¶å‘å¼‚æ­¥LLMè¿›ç¨‹æ•° | `4`ï¼ˆé»˜è®¤å€¼ç”±ç¯å¢ƒå˜é‡MAX_ASYNCæ›´æ”¹ï¼‰ |
| **llm_model_kwargs** | `dict` | LLMç”Ÿæˆçš„é™„åŠ å‚æ•° | |
| **vector_db_storage_cls_kwargs** | `dict` | å‘é‡æ•°æ®åº“çš„é™„åŠ å‚æ•°ï¼Œå¦‚è®¾ç½®èŠ‚ç‚¹å’Œå…³ç³»æ£€ç´¢çš„é˜ˆå€¼ | cosine_better_than_threshold: 0.2ï¼ˆé»˜è®¤å€¼ç”±ç¯å¢ƒå˜é‡COSINE_THRESHOLDæ›´æ”¹ï¼‰ |
| **enable_llm_cache** | `bool` | å¦‚æœä¸º`TRUE`ï¼Œå°†LLMç»“æœå­˜å‚¨åœ¨ç¼“å­˜ä¸­ï¼›é‡å¤çš„æç¤ºè¿”å›ç¼“å­˜çš„å“åº” | `TRUE` |
| **enable_llm_cache_for_entity_extract** | `bool` | å¦‚æœä¸º`TRUE`ï¼Œå°†å®ä½“æå–çš„LLMç»“æœå­˜å‚¨åœ¨ç¼“å­˜ä¸­ï¼›é€‚åˆåˆå­¦è€…è°ƒè¯•åº”ç”¨ç¨‹åº | `TRUE` |
| **addon_params** | `dict` | é™„åŠ å‚æ•°ï¼Œä¾‹å¦‚`{"language": "Simplified Chinese", "entity_types": ["organization", "person", "location", "event"]}`ï¼šè®¾ç½®ç¤ºä¾‹é™åˆ¶ã€è¾“å‡ºè¯­è¨€å’Œæ–‡æ¡£å¤„ç†çš„æ‰¹é‡å¤§å° | language: English` |
| **embedding_cache_config** | `dict` | é—®ç­”ç¼“å­˜çš„é…ç½®ã€‚åŒ…å«ä¸‰ä¸ªå‚æ•°ï¼š`enabled`ï¼šå¸ƒå°”å€¼ï¼Œå¯ç”¨/ç¦ç”¨ç¼“å­˜æŸ¥æ‰¾åŠŸèƒ½ã€‚å¯ç”¨æ—¶ï¼Œç³»ç»Ÿå°†åœ¨ç”Ÿæˆæ–°ç­”æ¡ˆä¹‹å‰æ£€æŸ¥ç¼“å­˜çš„å“åº”ã€‚`similarity_threshold`ï¼šæµ®ç‚¹å€¼ï¼ˆ0-1ï¼‰ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼ã€‚å½“æ–°é—®é¢˜ä¸ç¼“å­˜é—®é¢˜çš„ç›¸ä¼¼åº¦è¶…è¿‡æ­¤é˜ˆå€¼æ—¶ï¼Œå°†ç›´æ¥è¿”å›ç¼“å­˜çš„ç­”æ¡ˆè€Œä¸è°ƒç”¨LLMã€‚`use_llm_check`ï¼šå¸ƒå°”å€¼ï¼Œå¯ç”¨/ç¦ç”¨LLMç›¸ä¼¼åº¦éªŒè¯ã€‚å¯ç”¨æ—¶ï¼Œåœ¨è¿”å›ç¼“å­˜ç­”æ¡ˆä¹‹å‰ï¼Œå°†ä½¿ç”¨LLMä½œä¸ºäºŒæ¬¡æ£€æŸ¥æ¥éªŒè¯é—®é¢˜ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ | é»˜è®¤ï¼š`{"enabled": False, "similarity_threshold": 0.95, "use_llm_check": False}` |

</details>

### æŸ¥è¯¢å‚æ•°

ä½¿ç”¨QueryParamæ§åˆ¶ä½ çš„æŸ¥è¯¢è¡Œä¸ºï¼š

```python
class QueryParam:
    """Configuration parameters for query execution in LightRAG."""

    mode: Literal["local", "global", "hybrid", "naive", "mix", "bypass"] = "global"
    """Specifies the retrieval mode:
    - "local": Focuses on context-dependent information.
    - "global": Utilizes global knowledge.
    - "hybrid": Combines local and global retrieval methods.
    - "naive": Performs a basic search without advanced techniques.
    - "mix": Integrates knowledge graph and vector retrieval.
    """

    only_need_context: bool = False
    """If True, only returns the retrieved context without generating a response."""

    only_need_prompt: bool = False
    """If True, only returns the generated prompt without producing a response."""

    response_type: str = "Multiple Paragraphs"
    """Defines the response format. Examples: 'Multiple Paragraphs', 'Single Paragraph', 'Bullet Points'."""

    stream: bool = False
    """If True, enables streaming output for real-time responses."""

    top_k: int = int(os.getenv("TOP_K", "60"))
    """Number of top items to retrieve. Represents entities in 'local' mode and relationships in 'global' mode."""

    chunk_top_k: int = int(os.getenv("CHUNK_TOP_K", "20"))
    """Number of text chunks to retrieve initially from vector search and keep after reranking.
    If None, defaults to top_k value.
    """

    max_entity_tokens: int = int(os.getenv("MAX_ENTITY_TOKENS", "6000"))
    """Maximum number of tokens allocated for entity context in unified token control system."""

    max_relation_tokens: int = int(os.getenv("MAX_RELATION_TOKENS", "8000"))
    """Maximum number of tokens allocated for relationship context in unified token control system."""

    max_total_tokens: int = int(os.getenv("MAX_TOTAL_TOKENS", "30000"))
    """Maximum total tokens budget for the entire query context (entities + relations + chunks + system prompt)."""

    # History messages are only sent to LLM for context, not used for retrieval
    conversation_history: list[dict[str, str]] = field(default_factory=list)
    """Stores past conversation history to maintain context.
    Format: [{"role": "user/assistant", "content": "message"}].
    """

    # Deprecated (ids filter lead to potential hallucination effects)
    ids: list[str] | None = None
    """List of ids to filter the results."""

    model_func: Callable[..., object] | None = None
    """Optional override for the LLM model function to use for this specific query.
    If provided, this will be used instead of the global model function.
    This allows using different models for different query modes.
    """

    user_prompt: str | None = None
    """User-provided prompt for the query.
    Addition instructions for LLM. If provided, this will be inject into the prompt template.
    It's purpose is the let user customize the way LLM generate the response.
    """

    enable_rerank: bool = True
    """Enable reranking for retrieved text chunks. If True but no rerank model is configured, a warning will be issued.
    Default is True to enable reranking when rerank model is available.
    """
```

> top_kçš„é»˜è®¤å€¼å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡TOP_Kæ›´æ”¹ã€‚

### LLM and Embeddingæ³¨å…¥

LightRAG éœ€è¦åˆ©ç”¨LLMå’ŒEmbedingæ¨¡å‹æ¥å®Œæˆæ–‡æ¡£ç´¢å¼•å’ŒçŸ¥è¯†åº“æŸ¥è¯¢å·¥ä½œã€‚åœ¨åˆå§‹åŒ–LightRAGçš„æ—¶å€™éœ€è¦æŠŠé˜¶æ®µï¼Œéœ€è¦æŠŠLLMå’ŒEmbeddingçš„æ“ä½œå‡½æ•°æ³¨å…¥åˆ°å¯¹è±¡ä¸­ï¼š

<details>
<summary> <b>ä½¿ç”¨ç±»OpenAIçš„API</b> </summary>

* LightRAGè¿˜æ”¯æŒç±»OpenAIçš„èŠå¤©/åµŒå…¥APIï¼š

```python
import os
import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.openai import openai_complete_if_cache, openai_embed

async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -> str:
    return await openai_complete_if_cache(
        "solar-mini",
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv("UPSTAGE_API_KEY"),
        base_url="https://api.upstage.ai/v1/solar",
        **kwargs
    )

@wrap_embedding_func_with_attrs(embedding_dim=4096, max_token_size=8192, model_name="solar-embedding-1-large-query")
async def embedding_func(texts: list[str]) -> np.ndarray:
    return await openai_embed.func(
        texts,
        model="solar-embedding-1-large-query",
        api_key=os.getenv("UPSTAGE_API_KEY"),
        base_url="https://api.upstage.ai/v1/solar"
    )

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        embedding_func=embedding_func  # ç›´æ¥ä¼ å…¥è£…é¥°åçš„å‡½æ•°
    )

    await rag.initialize_storages()
    return rag
```

> **å…³äºåµŒå…¥å‡½æ•°å°è£…çš„é‡è¦è¯´æ˜ï¼š**
>
> `EmbeddingFunc` ä¸èƒ½åµŒå¥—å°è£…ã€‚å·²ç»è¢« `@wrap_embedding_func_with_attrs` è£…é¥°è¿‡çš„åµŒå…¥å‡½æ•°ï¼ˆå¦‚ `openai_embed`ã€`ollama_embed` ç­‰ï¼‰ä¸èƒ½å†æ¬¡ä½¿ç”¨ `EmbeddingFunc()` å°è£…ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨åˆ›å»ºè‡ªå®šä¹‰åµŒå…¥å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬è°ƒç”¨ `xxx_embed.func`ï¼ˆåº•å±‚æœªå°è£…çš„å‡½æ•°ï¼‰è€Œä¸æ˜¯ç›´æ¥è°ƒç”¨ `xxx_embed`ã€‚

</details>

<details>
<summary> <b>ä½¿ç”¨ Hugging Face æ¨¡å‹</b> </summary>

* å¦‚æœæ‚¨æƒ³ä½¿ç”¨ Hugging Face æ¨¡å‹ï¼Œåªéœ€è¦æŒ‰å¦‚ä¸‹æ–¹å¼è®¾ç½® LightRAGï¼š

å‚è§ `lightrag_hf_demo.py`

```python
from functools import partial
from transformers import AutoTokenizer, AutoModel

# Pre-load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
embed_model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

# ä½¿ç”¨ Hugging Face æ¨¡å‹åˆå§‹åŒ– LightRAG
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=hf_model_complete,  # ä½¿ç”¨ Hugging Face æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
    llm_model_name='meta-llama/Llama-3.1-8B-Instruct',  # Hugging Face çš„æ¨¡å‹åç§°
    # ä½¿ç”¨ Hugging Face åµŒå…¥å‡½æ•°
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        max_token_size=2048,
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        func=partial(
            hf_embed.func,  # ä½¿ç”¨ .func è®¿é—®åº•å±‚æœªå°è£…çš„å‡½æ•°
            tokenizer=tokenizer,
            embed_model=embed_model
        )
    ),
)
```

</details>

<details>
<summary> <b>ä½¿ç”¨Ollamaæ¨¡å‹</b> </summary>

**ç»¼è¿°**

å¦‚æœæ‚¨æƒ³ä½¿ç”¨Ollamaæ¨¡å‹ï¼Œæ‚¨éœ€è¦æ‹‰å–è®¡åˆ’ä½¿ç”¨çš„æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ï¼Œä¾‹å¦‚`nomic-embed-text`ã€‚

ç„¶åæ‚¨åªéœ€è¦æŒ‰å¦‚ä¸‹æ–¹å¼è®¾ç½®LightRAGï¼š

```python
import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.ollama import ollama_model_complete, ollama_embed

@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=8192, model_name="nomic-embed-text")
async def embedding_func(texts: list[str]) -> np.ndarray:
    return await ollama_embed.func(texts, embed_model="nomic-embed-text")

# Initialize LightRAG with Ollama model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name='your_model_name', # Your model name
    embedding_func=embedding_func,  # Pass the decorated function directly
)
```

* **å¢åŠ ä¸Šä¸‹æ–‡å¤§å°**

ä¸ºäº†ä½¿ LightRAG æ­£å¸¸å·¥ä½œï¼Œä¸Šä¸‹æ–‡å¤§å°è‡³å°‘éœ€è¦ 32k tokensã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒOllama æ¨¡å‹çš„ä¸Šä¸‹æ–‡å¤§å°ä¸º 8kã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼ä¹‹ä¸€æ¥å®ç°ï¼š

* **åœ¨ Modelfile ä¸­å¢åŠ  `num_ctx` å‚æ•°**

1. æ‹‰å–æ¨¡å‹ï¼š

```bash
ollama pull qwen2
```

2. æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶ï¼š

```bash
ollama show --modelfile qwen2 > Modelfile
```

3. ç¼–è¾‘ Modelfileï¼Œæ·»åŠ ä»¥ä¸‹è¡Œï¼š

```bash
PARAMETER num_ctx 32768
```

4. åˆ›å»ºä¿®æ”¹åçš„æ¨¡å‹ï¼š

```bash
ollama create -f Modelfile qwen2m
```

* **é€šè¿‡ Ollama API è®¾ç½® `num_ctx`**

æ‚¨å¯ä»¥ä½¿ç”¨ `llm_model_kwargs` å‚æ•°æ¥é…ç½® Ollamaï¼š

```python
import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.ollama import ollama_model_complete, ollama_embed

@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=8192, model_name="nomic-embed-text")
async def embedding_func(texts: list[str]) -> np.ndarray:
    return await ollama_embed.func(texts, embed_model="nomic-embed-text")

rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # ä½¿ç”¨ Ollama æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
    llm_model_name='your_model_name', # æ‚¨çš„æ¨¡å‹åç§°
    llm_model_kwargs={"options": {"num_ctx": 32768}},
    embedding_func=embedding_func,  # ç›´æ¥ä¼ å…¥è£…é¥°åçš„å‡½æ•°
)
```

> **å…³äºåµŒå…¥å‡½æ•°å°è£…çš„é‡è¦è¯´æ˜ï¼š**
>
> `EmbeddingFunc` ä¸èƒ½åµŒå¥—å°è£…ã€‚å·²ç»è¢« `@wrap_embedding_func_with_attrs` è£…é¥°è¿‡çš„åµŒå…¥å‡½æ•°ï¼ˆå¦‚ `openai_embed`ã€`ollama_embed` ç­‰ï¼‰ä¸èƒ½å†æ¬¡ä½¿ç”¨ `EmbeddingFunc()` å°è£…ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨åˆ›å»ºè‡ªå®šä¹‰åµŒå…¥å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬è°ƒç”¨ `xxx_embed.func`ï¼ˆåº•å±‚æœªå°è£…çš„å‡½æ•°ï¼‰è€Œä¸æ˜¯ç›´æ¥è°ƒç”¨ `xxx_embed`ã€‚

* **ä½æ˜¾å­˜ GPU**

å¦‚æœè¦åœ¨ä½æ˜¾å­˜ GPU ä¸Šè¿è¡Œæ­¤å®éªŒï¼Œæ‚¨åº”è¯¥é€‰æ‹©è¾ƒå°çš„æ¨¡å‹å¹¶è°ƒæ•´ä¸Šä¸‹æ–‡çª—å£ï¼ˆå¢åŠ ä¸Šä¸‹æ–‡ä¼šå¢åŠ å†…å­˜æ¶ˆè€—ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€å—æ”¹è£…çš„ 6GB æ˜¾å­˜çš„æŒ–çŸ¿ GPU ä¸Šè¿è¡Œæ­¤ Ollama ç¤ºä¾‹ï¼Œéœ€è¦åœ¨ä½¿ç”¨ `gemma2:2b` æ—¶å°†ä¸Šä¸‹æ–‡å¤§å°è®¾ç½®ä¸º 26kã€‚å®ƒèƒ½å¤Ÿåœ¨ `book.txt` ä¸­æ‰¾åˆ° 197 ä¸ªå®ä½“å’Œ 19 ä¸ªå…³ç³»ã€‚

</details>

<details>
<summary> <b>LlamaIndex</b> </summary>

LightRAG æ”¯æŒä¸ LlamaIndex é›†æˆï¼ˆ`llm/llama_index_impl.py`ï¼‰ï¼š

- é€šè¿‡ LlamaIndex ä¸ OpenAI å’Œå…¶ä»–æä¾›å•†é›†æˆ
- è¯¦ç»†è®¾ç½®è¯·å‚é˜… [LlamaIndex æ–‡æ¡£](https://developers.llamaindex.ai/python/framework/) æˆ– [ç¤ºä¾‹](examples/unofficial-sample/)

**ç¤ºä¾‹ç”¨æ³•**

```python
# ä½¿ç”¨ LlamaIndex ç›´æ¥è®¿é—® OpenAI
import asyncio
from lightrag import LightRAG
from lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from lightrag.utils import setup_logger

# ä¸º LightRAG è®¾ç½®æ—¥å¿—å¤„ç†å™¨
setup_logger("lightrag", level="INFO")

async def initialize_rag():
    rag = LightRAG(
        working_dir="your/path",
        llm_model_func=llama_index_complete_if_cache,  # ä¸ LlamaIndex å…¼å®¹çš„è¡¥å…¨å‡½æ•°
        embedding_func=EmbeddingFunc(    # ä¸ LlamaIndex å…¼å®¹çš„åµŒå…¥å‡½æ•°
            embedding_dim=1536,
            max_token_size=2048,
            model_name=embed_model,
            func=partial(llama_index_embed.func, embed_model=embed_model)  # ä½¿ç”¨ .func è®¿é—®æœªå°è£…çš„åŸå§‹å‡½æ•°
        ),
    )

    await rag.initialize_storages()
    return rag

def main():
    # åˆå§‹åŒ– RAG å®ä¾‹
    rag = asyncio.run(initialize_rag())

    with open("./book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # æ‰§è¡Œæœ´ç´ æœç´¢
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="naive"))
    )

    # æ‰§è¡Œæœ¬åœ°æœç´¢
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="local"))
    )

    # æ‰§è¡Œå…¨å±€æœç´¢
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="global"))
    )

    # æ‰§è¡Œæ··åˆæœç´¢
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="hybrid"))
    )

if __name__ == "__main__":
    main()
```

**è¯¦ç»†æ–‡æ¡£å’Œç¤ºä¾‹è¯·å‚é˜…ï¼š**

- [LlamaIndex æ–‡æ¡£](https://developers.llamaindex.ai/python/framework/)
- [ç›´æ¥ä½¿ç”¨ OpenAI ç¤ºä¾‹](examples/unofficial-sample/lightrag_llamaindex_direct_demo.py)
- [LiteLLM ä»£ç†ç¤ºä¾‹](examples/unofficial-sample/lightrag_llamaindex_litellm_demo.py)
- [LiteLLM ä»£ç†ä¸ Opik é›†æˆç¤ºä¾‹](examples/unofficial-sample/lightrag_llamaindex_litellm_opik_demo.py)

</details>

<details>
<summary> <b>ä½¿ç”¨ Azure OpenAI æ¨¡å‹</b> </summary>

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ Azure OpenAI æ¨¡å‹ï¼Œæ‚¨åªéœ€è¦æŒ‰å¦‚ä¸‹æ–¹å¼è®¾ç½® LightRAGï¼š

```python
import os
import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.azure_openai import azure_openai_complete_if_cache, azure_openai_embed

# é…ç½®ç”Ÿæˆæ¨¡å‹
async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -> str:
    return await azure_openai_complete_if_cache(
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
        deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
        **kwargs
    )

# é…ç½®åµŒå…¥æ¨¡å‹
@wrap_embedding_func_with_attrs(
    embedding_dim=1536,
    max_token_size=8192,
    model_name=os.getenv("AZURE_OPENAI_EMBEDDING_MODEL")
)
async def embedding_func(texts: list[str]) -> np.ndarray:
    return await azure_openai_embed.func(
        texts,
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
        deployment_name=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME")
    )

rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=llm_model_func,
    embedding_func=embedding_func
)
```

</details>

<details>
<summary> <b>ä½¿ç”¨ Google Gemini æ¨¡å‹</b> </summary>

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ Google Gemini æ¨¡å‹ï¼Œæ‚¨åªéœ€è¦æŒ‰å¦‚ä¸‹æ–¹å¼è®¾ç½® LightRAGï¼š

```python
import os
import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.gemini import gemini_complete, gemini_embed

# é…ç½®ç”Ÿæˆæ¨¡å‹
async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -> str:
    return await gemini_complete(
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-1.5-flash",
        **kwargs
    )

# é…ç½®åµŒå…¥æ¨¡å‹
@wrap_embedding_func_with_attrs(
    embedding_dim=768,
    max_token_size=2048,
    model_name="models/text-embedding-004"
)
async def embedding_func(texts: list[str]) -> np.ndarray:
    return await gemini_embed.func(
        texts,
        api_key=os.getenv("GEMINI_API_KEY"),
        model="models/text-embedding-004"
    )

rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=llm_model_func,
    llm_model_name="gemini-2.0-flash",
    embedding_func=embedding_func
)
```

</details>

### Rerank å‡½æ•°æ³¨å…¥

ä¸ºäº†æå‡æ£€ç´¢è´¨é‡ï¼Œå¯ä»¥åŸºäºæ›´æœ‰æ•ˆçš„ç›¸å…³æ€§è¯„åˆ†æ¨¡å‹å¯¹æ–‡æ¡£è¿›è¡Œé‡æ–°æ’åºã€‚`rerank.py` æ–‡ä»¶æä¾›äº†ä¸‰ä¸ª Reranker æœåŠ¡å•†çš„é©±åŠ¨å‡½æ•°ï¼š

* **Cohere / vLLM**: `cohere_rerank`
* **Jina AI**: `jina_rerank`
* **é˜¿é‡Œäº‘**: `ali_rerank`

æ‚¨å¯ä»¥å°†å…¶ä¸­ä¸€ä¸ªå‡½æ•°æ³¨å…¥åˆ° LightRAG å¯¹è±¡çš„ `rerank_model_func` å±æ€§ä¸­ã€‚è¿™å°†ä½¿ LightRAG çš„æŸ¥è¯¢å‡½æ•°èƒ½å¤Ÿä½¿ç”¨æ³¨å…¥çš„å‡½æ•°å¯¹æ£€ç´¢åˆ°çš„æ–‡æœ¬å—è¿›è¡Œé‡æ–°æ’åºã€‚è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒ `examples/rerank_example.py` æ–‡ä»¶ã€‚

### User Prompt ä¸ Query çš„åŒºåˆ«

ä½¿ç”¨ LightRAG è¿›è¡Œå†…å®¹æŸ¥è¯¢æ—¶ï¼Œåº”é¿å…å°†æœç´¢è¿‡ç¨‹ä¸ä¸ç›¸å…³çš„è¾“å‡ºå¤„ç†æ··åˆåœ¨ä¸€èµ·ï¼Œå› ä¸ºè¿™ä¼šæ˜¾è‘—å½±å“æŸ¥è¯¢æ•ˆæœã€‚QueryParam ä¸­çš„ `user_prompt` å‚æ•°ä¸“é—¨ç”¨äºè§£å†³æ­¤é—®é¢˜ - å®ƒä¸å‚ä¸ RAG æ£€ç´¢é˜¶æ®µï¼Œè€Œæ˜¯åœ¨æŸ¥è¯¢å®ŒæˆåæŒ‡å¯¼ LLM å¦‚ä½•å¤„ç†æ£€ç´¢åˆ°çš„ç»“æœã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```python
# åˆ›å»ºæŸ¥è¯¢å‚æ•°
query_param = QueryParam(
    mode = "hybrid",  # å…¶ä»–æ¨¡å¼ï¼šlocal, global, hybrid, mix, naive
    user_prompt = "å¯¹äºå›¾è¡¨ï¼Œä½¿ç”¨ mermaid æ ¼å¼ï¼ŒèŠ‚ç‚¹åç§°ä½¿ç”¨è‹±æ–‡æˆ–æ‹¼éŸ³ï¼Œæ˜¾ç¤ºæ ‡ç­¾ä½¿ç”¨ä¸­æ–‡",
)

# æŸ¥è¯¢å¹¶å¤„ç†
response_default = rag.query(
    "è¯·ä¸ºæ–¯å…‹é²å¥‡ç»˜åˆ¶äººç‰©å…³ç³»å›¾",
    param=query_param
)
print(response_default)
```

### æ’å…¥

<details>
  <summary> <b> åŸºæœ¬æ’å…¥ </b></summary>

```python
# åŸºæœ¬æ’å…¥
rag.insert("æ–‡æœ¬")
```

</details>

<details>
  <summary> <b> æ‰¹é‡æ’å…¥ </b></summary>

```python
# åŸºæœ¬æ‰¹é‡æ’å…¥ï¼šä¸€æ¬¡æ’å…¥å¤šä¸ªæ–‡æœ¬
rag.insert(["æ–‡æœ¬1", "æ–‡æœ¬2",...])

# è‡ªå®šä¹‰æ‰¹é‡å¤§å°é…ç½®çš„æ‰¹é‡æ’å…¥
rag = LightRAG(
    ...
    working_dir=WORKING_DIR,
    max_parallel_insert = 4
)

rag.insert(["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3", ...])  # æ–‡æ¡£å°†ä»¥æ¯æ‰¹ 4 ä¸ªçš„æ–¹å¼å¤„ç†
```

`max_parallel_insert` å‚æ•°å†³å®šäº†æ–‡æ¡£ç´¢å¼•ç®¡é“ä¸­å¹¶å‘å¤„ç†çš„æ–‡æ¡£æ•°é‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤å€¼ä¸º **2**ã€‚æˆ‘ä»¬å»ºè®®å°†æ­¤è®¾ç½®ä¿æŒåœ¨ **10 ä»¥ä¸‹**ï¼Œå› ä¸ºæ€§èƒ½ç“¶é¢ˆé€šå¸¸åœ¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤„ç†èƒ½åŠ›ã€‚

</details>

<details>
  <summary> <b> å¸¦ ID æ’å…¥ </b></summary>

å¦‚æœæ‚¨æƒ³ä¸ºæ–‡æ¡£æä¾›è‡ªå®šä¹‰ IDï¼Œæ–‡æ¡£æ•°é‡å’Œ ID æ•°é‡å¿…é¡»ç›¸åŒã€‚

```python
# æ’å…¥å•ä¸ªæ–‡æœ¬ï¼Œå¹¶ä¸ºå…¶æä¾› ID
rag.insert("æ–‡æœ¬1", ids=["æ–‡æœ¬1çš„ID"])

# æ’å…¥å¤šä¸ªæ–‡æœ¬ï¼Œå¹¶ä¸ºå®ƒä»¬æä¾› ID
rag.insert(["æ–‡æœ¬1", "æ–‡æœ¬2",...], ids=["æ–‡æœ¬1çš„ID", "æ–‡æœ¬2çš„ID"])
```

</details>

<details>
  <summary><b>ä½¿ç”¨ç®¡é“æ’å…¥</b></summary>

`apipeline_enqueue_documents` å’Œ `apipeline_process_enqueue_documents` å‡½æ•°å…è®¸æ‚¨å°†æ–‡æ¡£å¢é‡æ’å…¥åˆ°å›¾ä¸­ã€‚è¿™å¯¹äºå¸Œæœ›åœ¨åå°å¤„ç†æ–‡æ¡£åŒæ—¶å…è®¸ä¸»çº¿ç¨‹ç»§ç»­æ‰§è¡Œçš„åœºæ™¯éå¸¸æœ‰ç”¨ã€‚

```python
rag = LightRAG(..)

await rag.apipeline_enqueue_documents(input)
# åœ¨å¾ªç¯ä¸­çš„ä¾‹ç¨‹
await rag.apipeline_process_enqueue_documents(input)
```

</details>

<details>
  <summary><b>å¤šæ–‡ä»¶ç±»å‹æ”¯æŒæ’å…¥</b></summary>

`textract` æ”¯æŒè¯»å– TXTã€DOCXã€PPTXã€CSV å’Œ PDF ç­‰æ–‡ä»¶ç±»å‹ã€‚

```python
import textract

file_path = 'TEXT.pdf'
text_content = textract.process(file_path)

rag.insert(text_content.decode('utf-8'))
```

</details>

<details>
  <summary><b>å¼•ç”¨åŠŸèƒ½</b></summary>

é€šè¿‡æä¾›æ–‡ä»¶è·¯å¾„ï¼Œç³»ç»Ÿå¯ä»¥ç¡®ä¿æ¥æºå¯ä»¥è¿½æº¯åˆ°åŸå§‹æ–‡æ¡£ã€‚

```python
# å®šä¹‰æ–‡æ¡£åŠå…¶æ–‡ä»¶è·¯å¾„
documents = ["æ–‡æ¡£å†…å®¹ 1", "æ–‡æ¡£å†…å®¹ 2"]
file_paths = ["path/to/doc1.txt", "path/to/doc2.txt"]

# å¸¦æ–‡ä»¶è·¯å¾„æ’å…¥æ–‡æ¡£
rag.insert(documents, file_paths=file_paths)
```

</details>

### å­˜å‚¨æ–¹æ¡ˆ

LightRAG ä½¿ç”¨ 4 ç§ç±»å‹çš„å­˜å‚¨æ¥æ»¡è¶³ä¸åŒç”¨é€”ï¼š

* KV_STORAGEï¼šLLM å“åº”ç¼“å­˜ã€æ–‡æœ¬å—ã€æ–‡æ¡£ä¿¡æ¯
* VECTOR_STORAGEï¼šå®ä½“å‘é‡ã€å…³ç³»å‘é‡ã€æ–‡æœ¬å—å‘é‡
* GRAPH_STORAGEï¼šå®ä½“å…³ç³»å›¾
* DOC_STATUS_STORAGEï¼šæ–‡æ¡£ç´¢å¼•çŠ¶æ€

æ¯ç§å­˜å‚¨ç±»å‹éƒ½æœ‰å¤šç§å®ç°ï¼š

* KV_STORAGE æ”¯æŒçš„å®ç°ï¼š

```
JsonKVStorage    JsonFileï¼ˆé»˜è®¤ï¼‰
PGKVStorage      Postgres
RedisKVStorage   Redis
MongoKVStorage   MongoDB
```

* GRAPH_STORAGE æ”¯æŒçš„å®ç°ï¼š

```
NetworkXStorage      NetworkXï¼ˆé»˜è®¤ï¼‰
Neo4JStorage         Neo4J
PGGraphStorage       PostgreSQL with AGE æ’ä»¶
MemgraphStorage      Memgraph
```

> æµ‹è¯•è¡¨æ˜ï¼ŒNeo4J åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„æ€§èƒ½ä¼˜äºå¸¦æœ‰ AGE æ’ä»¶çš„ PostgreSQLã€‚

* VECTOR_STORAGE æ”¯æŒçš„å®ç°ï¼š

```
NanoVectorDBStorage         NanoVectorï¼ˆé»˜è®¤ï¼‰
PGVectorStorage             Postgres
MilvusVectorDBStorage       Milvus
FaissVectorDBStorage        Faiss
QdrantVectorDBStorage       Qdrant
MongoVectorDBStorage        MongoDB
```

* DOC_STATUS_STORAGE æ”¯æŒçš„å®ç°ï¼š

```
JsonDocStatusStorage        JsonFileï¼ˆé»˜è®¤ï¼‰
PGDocStatusStorage          Postgres
MongoDocStatusStorage       MongoDB
```

å„å­˜å‚¨ç±»å‹çš„ç¤ºä¾‹è¿æ¥é…ç½®å¯åœ¨ä»“åº“ä¸­çš„ `env.example` æ–‡ä»¶é‡Œæ‰¾åˆ°ã€‚è¿æ¥å­—ç¬¦ä¸²ä¸­çš„æ•°æ®åº“å®ä¾‹éœ€è¦æ‚¨é¢„å…ˆåœ¨æ•°æ®åº“æœåŠ¡å™¨ä¸Šåˆ›å»ºã€‚LightRAG ä»…è´Ÿè´£åœ¨æ•°æ®åº“å®ä¾‹ä¸­åˆ›å»ºè¡¨ï¼Œä¸è´Ÿè´£åˆ›å»ºæ•°æ®åº“å®ä¾‹æœ¬èº«ã€‚å¦‚æœä½¿ç”¨ Redis ä½œä¸ºå­˜å‚¨ï¼Œè¯·è®°ä½é…ç½® Redis çš„è‡ªåŠ¨æ•°æ®æŒä¹…åŒ–è§„åˆ™ï¼Œå¦åˆ™ Redis æœåŠ¡é‡å¯åæ•°æ®å°†ä¼šä¸¢å¤±ã€‚å¦‚æœä½¿ç”¨ PostgreSQLï¼Œå»ºè®®ä½¿ç”¨ 16.6 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚

<details>
<summary> <b>ä½¿ç”¨ Neo4J å­˜å‚¨</b> </summary>

* å¯¹äºç”Ÿäº§çº§åœºæ™¯ï¼Œæ‚¨å¾ˆå¯èƒ½éœ€è¦ä½¿ç”¨ä¼ä¸šçº§è§£å†³æ–¹æ¡ˆ
* ç”¨äºçŸ¥è¯†å›¾è°±å­˜å‚¨ã€‚æ¨èåœ¨ Docker ä¸­è¿è¡Œ Neo4J è¿›è¡Œæ— ç¼æœ¬åœ°æµ‹è¯•ã€‚
* å‚è§ï¼šhttps://hub.docker.com/_/neo4j

```python
export NEO4J_URI="neo4j://localhost:7687"
export NEO4J_USERNAME="neo4j"
export NEO4J_PASSWORD="password"
export NEO4J_DATABASE="neo4j" #<----------- ä½¿ç”¨ neo4j ç¤¾åŒºç‰ˆ docker é•œåƒæ—¶æ•°æ®åº“å®ä¾‹å¿…é¡»ä¸ºneo4j

# ä¸º LightRAG è®¾ç½®æ—¥å¿—
setup_logger("lightrag", level="INFO")

# å¯åŠ¨é¡¹ç›®æ—¶ï¼Œè¯·ç¡®ä¿é€šè¿‡æŒ‡å®š graph_storage="Neo4JStorage" æ¥è¦†ç›–é»˜è®¤çš„ KG: NetworkXã€‚
# ä½¿ç”¨ Neo4J å®ç°åˆå§‹åŒ– LightRAGã€‚
async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=gpt_4o_mini_complete,  # ä½¿ç”¨ gpt_4o_mini_complete LLM æ¨¡å‹
        graph_storage="Neo4JStorage", #<-----------è¦†ç›– KG é»˜è®¤å€¼
    )

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    await rag.initialize_storages()
    # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†çš„ç®¡é“çŠ¶æ€
    return rag
```

å‚è§ test_neo4j.py è·å–å¯è¿è¡Œçš„ç¤ºä¾‹ã€‚

</details>

<details>
<summary> <b>ä½¿ç”¨ PostgreSQL å­˜å‚¨</b> </summary>

å¯¹äºç”Ÿäº§çº§åœºæ™¯ï¼Œæ‚¨å¾ˆå¯èƒ½éœ€è¦ä½¿ç”¨ä¼ä¸šçº§è§£å†³æ–¹æ¡ˆã€‚PostgreSQL å¯ä»¥ä¸ºæ‚¨æä¾›ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œä½œä¸º KV å­˜å‚¨ã€VectorDBï¼ˆpgvectorï¼‰å’Œ GraphDBï¼ˆapache AGEï¼‰ã€‚æ”¯æŒ PostgreSQL 16.6 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚

* PostgreSQL å¾ˆè½»é‡ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦æ’ä»¶çš„å®Œæ•´äºŒè¿›åˆ¶å‘è¡Œç‰ˆå¯ä»¥å‹ç¼©åˆ° 40MBï¼šå‚è€ƒ [Windows Release](https://github.com/ShanGor/apache-age-windows/releases/tag/PG17%2Fv1.5.0-rc0)ï¼ŒLinux/Mac ä¹Ÿå¾ˆå®¹æ˜“å®‰è£…ã€‚
* å¦‚æœæ‚¨å–œæ¬¢ dockerï¼Œå»ºè®®åˆå­¦è€…ä½¿ç”¨æ­¤é•œåƒä»¥é¿å…å‡ºç°é—®é¢˜ï¼ˆé»˜è®¤ç”¨æˆ·å¯†ç ï¼šrag/ragï¼‰ï¼šhttps://hub.docker.com/r/gzdaniel/postgres-for-rag
* å¦‚ä½•å¼€å§‹ï¼Ÿå‚è€ƒï¼š[examples/lightrag_gemini_postgres_demo.py](https://github.com/HKUDS/LightRAG/blob/main/examples/lightrag_gemini_postgres_demo.py)
* å¯¹äºé«˜æ€§èƒ½å›¾æ•°æ®åº“éœ€æ±‚ï¼Œæ¨èä½¿ç”¨ Neo4jï¼Œå› ä¸º Apache AGE çš„æ€§èƒ½ä¸å¤Ÿç†æƒ³ã€‚

</details>

<details>
<summary> <b>ä½¿ç”¨ Faiss å­˜å‚¨</b> </summary>

åœ¨ä½¿ç”¨ Faiss å‘é‡æ•°æ®åº“ä¹‹å‰ï¼Œæ‚¨å¿…é¡»æ‰‹åŠ¨å®‰è£… `faiss-cpu` æˆ– `faiss-gpu`ã€‚

- å®‰è£…æ‰€éœ€ä¾èµ–ï¼š

```
pip install faiss-cpu
```

å¦‚æœæ‚¨æœ‰ GPU æ”¯æŒï¼Œä¹Ÿå¯ä»¥å®‰è£… `faiss-gpu`ã€‚

- è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ `sentence-transformers`ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ `3072` ç»´åº¦çš„ `OpenAIEmbedding` æ¨¡å‹ã€‚

```python
async def embedding_func(texts: list[str]) -> np.ndarray:
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(texts, convert_to_numpy=True)
    return embeddings

# ä½¿ç”¨ LLM æ¨¡å‹å‡½æ•°å’ŒåµŒå…¥å‡½æ•°åˆå§‹åŒ– LightRAG
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=llm_model_func,
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        max_token_size=2048,
        model_name="all-MiniLM-L6-v2",
        func=embedding_func,
    ),
    vector_storage="FaissVectorDBStorage",
    vector_db_storage_cls_kwargs={
        "cosine_better_than_threshold": 0.3  # æ‚¨æœŸæœ›çš„é˜ˆå€¼
    }
)
```

</details>

<details>
<summary> <b>ä½¿ç”¨ Memgraph å­˜å‚¨</b> </summary>

* Memgraph æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å†…å­˜å›¾æ•°æ®åº“ï¼Œå…¼å®¹ Neo4j Bolt åè®®ã€‚
* æ‚¨å¯ä»¥ä½¿ç”¨ Docker åœ¨æœ¬åœ°è¿è¡Œ Memgraph è¿›è¡Œç®€å•æµ‹è¯•ï¼š
* å‚è§ï¼šhttps://memgraph.com/download

```python
export MEMGRAPH_URI="bolt://localhost:7687"

# ä¸º LightRAG è®¾ç½®æ—¥å¿—
setup_logger("lightrag", level="INFO")

# å¯åŠ¨é¡¹ç›®æ—¶ï¼Œé€šè¿‡æŒ‡å®š kg="MemgraphStorage" æ¥è¦†ç›–é»˜è®¤çš„ KG: NetworkXã€‚

# æ³¨æ„ï¼šé»˜è®¤è®¾ç½®ä½¿ç”¨ NetworkX
# ä½¿ç”¨ Memgraph å®ç°åˆå§‹åŒ– LightRAGã€‚
async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=gpt_4o_mini_complete,  # ä½¿ç”¨ gpt_4o_mini_complete LLM æ¨¡å‹
        graph_storage="MemgraphStorage", #<-----------è¦†ç›– KG é»˜è®¤å€¼
    )

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    await rag.initialize_storages()
    # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†çš„ç®¡é“çŠ¶æ€
    return rag
```

</details>

<details>
<summary> <b>ä½¿ç”¨ MongoDB å­˜å‚¨</b> </summary>

MongoDB ä¸º LightRAG æä¾›äº†ä¸€ç«™å¼å­˜å‚¨è§£å†³æ–¹æ¡ˆã€‚MongoDB æä¾›åŸç”Ÿçš„ KV å­˜å‚¨å’Œå‘é‡å­˜å‚¨ã€‚LightRAG ä½¿ç”¨ MongoDB é›†åˆæ¥å®ç°ç®€å•çš„å›¾å­˜å‚¨ã€‚MongoDB å®˜æ–¹çš„å‘é‡æœç´¢åŠŸèƒ½ï¼ˆ`$vectorSearch`ï¼‰ç›®å‰éœ€è¦å…¶å®˜æ–¹äº‘æœåŠ¡ MongoDB Atlasã€‚æ­¤åŠŸèƒ½æ— æ³•åœ¨è‡ªæ‰˜ç®¡çš„ MongoDB Community/Enterprise ç‰ˆæœ¬ä¸Šä½¿ç”¨ã€‚

</details>

<details>
<summary> <b>ä½¿ç”¨ Redis å­˜å‚¨</b> </summary>

LightRAG æ”¯æŒä½¿ç”¨ Redis ä½œä¸º KV å­˜å‚¨ã€‚ä½¿ç”¨ Redis å­˜å‚¨æ—¶ï¼Œéœ€è¦æ³¨æ„æŒä¹…åŒ–é…ç½®å’Œå†…å­˜ä½¿ç”¨é…ç½®ã€‚ä»¥ä¸‹æ˜¯æ¨èçš„ Redis é…ç½®ï¼š

```
save 900 1
save 300 10
save 60 1000
stop-writes-on-bgsave-error yes
maxmemory 4gb
maxmemory-policy noeviction
maxclients 500
```

</details>

### LightRAG å®ä¾‹ä¹‹é—´çš„æ•°æ®éš”ç¦»

`workspace` å‚æ•°ç¡®ä¿ä¸åŒ LightRAG å®ä¾‹ä¹‹é—´çš„æ•°æ®éš”ç¦»ã€‚ä¸€æ—¦åˆå§‹åŒ–ï¼Œ`workspace` æ˜¯ä¸å¯å˜çš„ï¼Œæ— æ³•æ›´æ”¹ã€‚ä»¥ä¸‹æ˜¯ä¸åŒç±»å‹å­˜å‚¨å®ç°å·¥ä½œåŒºçš„æ–¹å¼ï¼š

- **å¯¹äºåŸºäºæœ¬åœ°æ–‡ä»¶çš„æ•°æ®åº“ï¼Œé€šè¿‡å·¥ä½œåŒºå­ç›®å½•å®ç°æ•°æ®éš”ç¦»**ï¼š`JsonKVStorage`ã€`JsonDocStatusStorage`ã€`NetworkXStorage`ã€`NanoVectorDBStorage`ã€`FaissVectorDBStorage`ã€‚
- **å¯¹äºä»¥é›†åˆæ–¹å¼å­˜å‚¨æ•°æ®çš„æ•°æ®åº“ï¼Œé€šè¿‡åœ¨é›†åˆåç§°å‰æ·»åŠ å·¥ä½œåŒºå‰ç¼€æ¥å®ç°**ï¼š`RedisKVStorage`ã€`RedisDocStatusStorage`ã€`MilvusVectorDBStorage`ã€`MongoKVStorage`ã€`MongoDocStatusStorage`ã€`MongoVectorDBStorage`ã€`MongoGraphStorage`ã€`PGGraphStorage`ã€‚
- **å¯¹äº Qdrant å‘é‡æ•°æ®åº“ï¼Œé€šè¿‡åŸºäº payload çš„åˆ†åŒºå®ç°æ•°æ®éš”ç¦»ï¼ˆQdrant æ¨èçš„å¤šç§Ÿæˆ·æ–¹æ³•ï¼‰**ï¼š`QdrantVectorDBStorage` ä½¿ç”¨å¸¦æœ‰ payload è¿‡æ»¤çš„å…±äº«é›†åˆï¼Œå®ç°æ— é™çš„å·¥ä½œåŒºå¯æ‰©å±•æ€§ã€‚
- **å¯¹äºå…³ç³»å‹æ•°æ®åº“ï¼Œé€šè¿‡åœ¨è¡¨ä¸­æ·»åŠ  `workspace` å­—æ®µå®ç°é€»è¾‘æ•°æ®åˆ†ç¦»**ï¼š`PGKVStorage`ã€`PGVectorStorage`ã€`PGDocStatusStorage`ã€‚
- **å¯¹äº Neo4j å›¾æ•°æ®åº“ï¼Œé€šè¿‡æ ‡ç­¾å®ç°é€»è¾‘æ•°æ®éš”ç¦»**ï¼š`Neo4JStorage`

ä¸ºäº†ä¿æŒä¸æ—§æ•°æ®çš„å…¼å®¹æ€§ï¼Œå½“æœªé…ç½®å·¥ä½œåŒºæ—¶ï¼ŒPostgreSQL éå›¾å­˜å‚¨çš„é»˜è®¤å·¥ä½œåŒºä¸º `default`ï¼ŒPostgreSQL AGE å›¾å­˜å‚¨çš„é»˜è®¤å·¥ä½œåŒºä¸º nullï¼ŒNeo4j å›¾å­˜å‚¨çš„é»˜è®¤å·¥ä½œåŒºä¸º `base`ã€‚å¯¹äºæ‰€æœ‰å¤–éƒ¨å­˜å‚¨ï¼Œç³»ç»Ÿæä¾›ä¸“ç”¨çš„å·¥ä½œåŒºç¯å¢ƒå˜é‡æ¥è¦†ç›–é€šç”¨çš„ `WORKSPACE` ç¯å¢ƒå˜é‡é…ç½®ã€‚è¿™äº›å­˜å‚¨ç‰¹å®šçš„å·¥ä½œåŒºç¯å¢ƒå˜é‡åŒ…æ‹¬ï¼š`REDIS_WORKSPACE`ã€`MILVUS_WORKSPACE`ã€`QDRANT_WORKSPACE`ã€`MONGODB_WORKSPACE`ã€`POSTGRES_WORKSPACE`ã€`NEO4J_WORKSPACE`ã€‚

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
æœ‰å…³åœ¨å•ä¸ªåº”ç”¨ç¨‹åºä¸­ç®¡ç†å¤šä¸ªéš”ç¦»çŸ¥è¯†åº“ï¼ˆä¾‹å¦‚ï¼Œå°†"ä¹¦ç±"å†…å®¹ä¸"äººåŠ›èµ„æºæ”¿ç­–"åˆ†å¼€ï¼‰çš„å®é™…æ¼”ç¤ºï¼Œè¯·å‚é˜… [Workspace Demo](examples/lightrag_gemini_workspace_demo.py)ã€‚

### AGENTS.md -- æŒ‡å¯¼ç¼–ç ä»£ç†

AGENTS.md æ˜¯ä¸€ç§ç®€å•ã€å¼€æ”¾çš„æ ¼å¼ï¼Œç”¨äºæŒ‡å¯¼ç¼–ç ä»£ç†ï¼ˆhttps://agents.md/ï¼‰ã€‚å®ƒæ˜¯ä¸€ä¸ªä¸“é—¨çš„ã€å¯é¢„æµ‹çš„åœ°æ–¹ï¼Œç”¨äºæä¾›ä¸Šä¸‹æ–‡å’ŒæŒ‡ä»¤ï¼Œå¸®åŠ© AI ç¼–ç ä»£ç†åœ¨ LightRAG é¡¹ç›®ä¸Šå·¥ä½œã€‚ä¸åŒçš„ AI ç¼–ç å™¨ä¸åº”å•ç‹¬ç»´æŠ¤å„è‡ªçš„æŒ‡å¯¼æ–‡ä»¶ã€‚å¦‚æœä»»ä½• AI ç¼–ç å™¨æ— æ³•è‡ªåŠ¨è¯†åˆ« AGENTS.mdï¼Œå¯ä»¥ä½¿ç”¨ç¬¦å·é“¾æ¥ä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚å»ºç«‹ç¬¦å·é“¾æ¥åï¼Œå¯ä»¥é€šè¿‡é…ç½®æœ¬åœ°çš„ `.gitignore_global` æ¥é˜²æ­¢å®ƒä»¬è¢«æäº¤åˆ° Git ä»“åº“ã€‚

## ç¼–è¾‘å®ä½“å’Œå…³ç³»

LightRAG ç°åœ¨æ”¯æŒå…¨é¢çš„çŸ¥è¯†å›¾è°±ç®¡ç†åŠŸèƒ½ï¼Œå…è®¸æ‚¨åœ¨çŸ¥è¯†å›¾è°±ä¸­åˆ›å»ºã€ç¼–è¾‘å’Œåˆ é™¤å®ä½“å’Œå…³ç³»ã€‚

<details>
  <summary> <b> åˆ›å»ºå®ä½“å’Œå…³ç³» </b></summary>

```python
# åˆ›å»ºæ–°å®ä½“
entity = rag.create_entity("Google", {
    "description": "Google æ˜¯ä¸€å®¶ä¸“æ³¨äºäº’è”ç½‘ç›¸å…³æœåŠ¡å’Œäº§å“çš„è·¨å›½ç§‘æŠ€å…¬å¸ã€‚",
    "entity_type": "company"
})

# åˆ›å»ºå¦ä¸€ä¸ªå®ä½“
product = rag.create_entity("Gmail", {
    "description": "Gmail æ˜¯ Google å¼€å‘çš„ç”µå­é‚®ä»¶æœåŠ¡ã€‚",
    "entity_type": "product"
})

# åˆ›å»ºå®ä½“ä¹‹é—´çš„å…³ç³»
relation = rag.create_relation("Google", "Gmail", {
    "description": "Google å¼€å‘å’Œè¿è¥ Gmailã€‚",
    "keywords": "develops operates service",
    "weight": 2.0
})
```

</details>

<details>
  <summary> <b> æ‰‹åŠ¨ä¿®æ”¹å®ä½“ä¸å…³ç³» </b></summary>

```python
# Edit an existing entity
updated_entity = rag.edit_entity("Google", {
    "description": "Google is a subsidiary of Alphabet Inc., founded in 1998.",
    "entity_type": "tech_company"
})

# Rename an entity (with all its relationships properly migrated)
renamed_entity = rag.edit_entity("Gmail", {
    "entity_name": "Google Mail",
    "description": "Google Mail (formerly Gmail) is an email service."
})

# Edit a relation between entities
updated_relation = rag.edit_relation("Google", "Google Mail", {
    "description": "Google created and maintains Google Mail service.",
    "keywords": "creates maintains email service",
    "weight": 3.0
})
```

æ‰€æœ‰æ“ä½œå‡æä¾›åŒæ­¥å’Œå¼‚æ­¥ä¸¤ä¸ªç‰ˆæœ¬ã€‚å¼‚æ­¥ç‰ˆæœ¬å¸¦æœ‰ "a" å‰ç¼€ï¼ˆä¾‹å¦‚ï¼š`acreate_entity`ã€`aedit_relation`ï¼‰ã€‚

</details>

<details>
  <summary> <b> æ’å…¥è‡ªå®šä¹‰çŸ¥è¯†å›¾è°± </b></summary>

```python
custom_kg = {
        "chunks": [
            {
                "content": "Alice and Bob are collaborating on quantum computing research.",
                "source_id": "doc-1",
                "file_path": "test_file",
            }
        ],
        "entities": [
            {
                "entity_name": "Alice",
                "entity_type": "person",
                "description": "Alice is a researcher specializing in quantum physics.",
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "entity_name": "Bob",
                "entity_type": "person",
                "description": "Bob is a mathematician.",
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "entity_name": "Quantum Computing",
                "entity_type": "technology",
                "description": "Quantum computing utilizes quantum mechanical phenomena for computation.",
                "source_id": "doc-1",
                "file_path": "test_file"
            }
        ],
        "relationships": [
            {
                "src_id": "Alice",
                "tgt_id": "Bob",
                "description": "Alice and Bob are research partners.",
                "keywords": "collaboration research",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "src_id": "Alice",
                "tgt_id": "Quantum Computing",
                "description": "Alice conducts research on quantum computing.",
                "keywords": "research expertise",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "src_id": "Bob",
                "tgt_id": "Quantum Computing",
                "description": "Bob researches quantum computing.",
                "keywords": "research application",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            }
        ]
    }

rag.insert_custom_kg(custom_kg)
```

</details>

<details>
  <summary> <b>å…¶å®ƒå®ä½“ä¸å…³ç³»æ“ä½œ</b></summary>

- **create_entity**ï¼šåˆ›å»ºå…·æœ‰æŒ‡å®šå±æ€§çš„æ–°å®ä½“
- **edit_entity**ï¼šæ›´æ–°ç°æœ‰å®ä½“çš„å±æ€§æˆ–é‡å‘½åå®ƒ
- **create_relation**ï¼šåœ¨ç°æœ‰å®ä½“ä¹‹é—´åˆ›å»ºæ–°å…³ç³»
- **edit_relation**ï¼šæ›´æ–°ç°æœ‰å…³ç³»çš„å±æ€§

è¿™äº›æ“ä½œåœ¨å›¾æ•°æ®åº“å’Œå‘é‡æ•°æ®åº“ç»„ä»¶ä¹‹é—´ä¿æŒæ•°æ®ä¸€è‡´æ€§ï¼Œç¡®ä¿æ‚¨çš„çŸ¥è¯†å›¾è°±ä¿æŒè¿è´¯ã€‚

</details>

## åˆ é™¤åŠŸèƒ½

LightRAG æä¾›äº†å…¨é¢çš„åˆ é™¤èƒ½åŠ›ï¼Œå…è®¸æ‚¨åˆ é™¤æ–‡æ¡£ã€å®ä½“å’Œå…³ç³»ã€‚

<details>
<summary> <b>åˆ é™¤å®ä½“</b> </summary>

æ‚¨å¯ä»¥é€šè¿‡å®ä½“åç§°åˆ é™¤å®ä½“åŠå…¶æ‰€æœ‰å…³è”å…³ç³»ï¼š

```python
# åˆ é™¤å®ä½“åŠå…¶æ‰€æœ‰å…³ç³»ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
rag.delete_by_entity("Google")

# å¼‚æ­¥ç‰ˆæœ¬
await rag.adelete_by_entity("Google")
```

åˆ é™¤å®ä½“æ—¶ï¼š
- ä»çŸ¥è¯†å›¾è°±ä¸­ç§»é™¤è¯¥å®ä½“èŠ‚ç‚¹
- åˆ é™¤æ‰€æœ‰å…³è”çš„å…³ç³»
- ä»å‘é‡æ•°æ®åº“ä¸­ç§»é™¤ç›¸å…³çš„åµŒå…¥å‘é‡
- ä¿æŒçŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§

</details>

<details>
<summary> <b>åˆ é™¤å…³ç³»</b> </summary>

æ‚¨å¯ä»¥åˆ é™¤ä¸¤ä¸ªç‰¹å®šå®ä½“ä¹‹é—´çš„å…³ç³»ï¼š

```python
# åˆ é™¤ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„å…³ç³»ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
rag.delete_by_relation("Google", "Gmail")

# å¼‚æ­¥ç‰ˆæœ¬
await rag.adelete_by_relation("Google", "Gmail")
```

åˆ é™¤å…³ç³»æ—¶ï¼š
- ç§»é™¤æŒ‡å®šçš„å…³ç³»è¾¹
- ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤è¯¥å…³ç³»çš„åµŒå…¥å‘é‡
- ä¿ç•™å®ä½“èŠ‚ç‚¹åŠå…¶å®ƒå…³ç³»

</details>

<details>
<summary> <b>é€šè¿‡æ–‡æ¡£ ID åˆ é™¤</b> </summary>

æ‚¨å¯ä»¥é€šè¿‡æ–‡æ¡£ ID åˆ é™¤æ•´ä¸ªæ–‡æ¡£åŠå…¶æ‰€æœ‰ç›¸å…³çš„çŸ¥è¯†ï¼š

```python
# é€šè¿‡æ–‡æ¡£ ID åˆ é™¤ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
await rag.adelete_by_doc_id("doc-12345")
```

é€šè¿‡æ–‡æ¡£ ID åˆ é™¤æ—¶çš„ä¼˜åŒ–å¤„ç†ï¼š
- **æ™ºèƒ½æ¸…ç†**ï¼šè‡ªåŠ¨è¯†åˆ«å¹¶åˆ é™¤ä»…å±äºè¯¥æ–‡æ¡£çš„å®ä½“å’Œå…³ç³»
- **ä¿ç•™å…±äº«çŸ¥è¯†**ï¼šå¦‚æœå®ä½“æˆ–å…³ç³»åœ¨å…¶ä»–æ–‡æ¡£ä¸­ä¹Ÿå­˜åœ¨ï¼Œåˆ™ä¼šä¿ç•™å¹¶é‡æ–°æ„å»ºå…¶æè¿°
- **ç¼“å­˜ä¼˜åŒ–**ï¼šæ¸…ç†ç›¸å…³çš„ LLM ç¼“å­˜ä»¥å‡å°‘å­˜å‚¨å¼€é”€
- **å¢é‡é‡å»º**ï¼šä»å‰©ä½™æ–‡æ¡£ä¸­é‡æ–°æ„å»ºå—å½±å“çš„å®ä½“å’Œå…³ç³»æè¿°

åˆ é™¤è¿‡ç¨‹åŒ…æ‹¬ï¼š
1. åˆ é™¤ä¸è¯¥æ–‡æ¡£ç›¸å…³çš„æ‰€æœ‰æ–‡æœ¬å—
2. è¯†åˆ«å¹¶åˆ é™¤ä»…å±äºè¯¥æ–‡æ¡£çš„å®ä½“å’Œå…³ç³»
3. é‡æ–°æ„å»ºåœ¨å…¶ä»–æ–‡æ¡£ä¸­ä»å­˜åœ¨çš„å®ä½“å’Œå…³ç³»
4. æ›´æ–°æ‰€æœ‰ç›¸å…³çš„å‘é‡ç´¢å¼•
5. æ¸…ç†æ–‡æ¡£çŠ¶æ€è®°å½•

æ³¨æ„ï¼šç”±äºæ¶‰åŠå¤æ‚çš„çŸ¥è¯†å›¾è°±é‡æ„è¿‡ç¨‹ï¼Œé€šè¿‡æ–‡æ¡£ ID åˆ é™¤æ˜¯ä¸€ä¸ªå¼‚æ­¥æ“ä½œã€‚

</details>

**é‡è¦æé†’ï¼š**

1. **ä¸å¯é€†æ“ä½œ**ï¼šæ‰€æœ‰åˆ é™¤æ“ä½œéƒ½æ˜¯ä¸å¯é€†çš„ï¼Œè¯·è°¨æ…ä½¿ç”¨
2. **æ€§èƒ½è€ƒè™‘**ï¼šåˆ é™¤å¤§é‡æ•°æ®å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ–‡æ¡£ ID åˆ é™¤
3. **æ•°æ®ä¸€è‡´æ€§**ï¼šåˆ é™¤æ“ä½œä¼šè‡ªåŠ¨ç»´æŠ¤çŸ¥è¯†å›¾è°±ä¸å‘é‡æ•°æ®åº“ä¹‹é—´çš„ä¸€è‡´æ€§
4. **å¤‡ä»½å»ºè®®**ï¼šåœ¨æ‰§è¡Œé‡è¦åˆ é™¤æ“ä½œå‰ï¼Œè¯·è€ƒè™‘å¤‡ä»½æ•°æ®

**æ‰¹é‡åˆ é™¤å»ºè®®ï¼š**
- å¯¹äºæ‰¹é‡åˆ é™¤æ“ä½œï¼Œå»ºè®®ä½¿ç”¨å¼‚æ­¥æ–¹æ³•ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
- å¯¹äºå¤§è§„æ¨¡åˆ é™¤ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†ä»¥é¿å…ç³»ç»Ÿè´Ÿè½½è¿‡é«˜

## å®ä½“åˆå¹¶

<details>
<summary> <b>åˆå¹¶å®ä½“åŠå…¶å…³ç³»</b> </summary>

LightRAG ç°åœ¨æ”¯æŒå°†å¤šä¸ªå®ä½“åˆå¹¶ä¸ºå•ä¸ªå®ä½“ï¼Œå¹¶è‡ªåŠ¨å¤„ç†æ‰€æœ‰å…³ç³»ï¼š

```python
# åŸºç¡€å®ä½“åˆå¹¶
rag.merge_entities(
    source_entities=["Artificial Intelligence", "AI", "Machine Intelligence"],
    target_entity="AI Technology"
)
```

ä½¿ç”¨è‡ªå®šä¹‰åˆå¹¶ç­–ç•¥ï¼š

```python
# ä¸ºä¸åŒå­—æ®µå®šä¹‰è‡ªå®šä¹‰åˆå¹¶ç­–ç•¥
rag.merge_entities(
    source_entities=["John Smith", "Dr. Smith", "J. Smith"],
    target_entity="John Smith",
    merge_strategy={
        "description": "concatenate",  # åˆå¹¶æ‰€æœ‰æè¿°
        "entity_type": "keep_first",   # ä¿ç•™ç¬¬ä¸€ä¸ªå®ä½“çš„ç±»å‹
        "source_id": "join_unique"     # åˆå¹¶æ‰€æœ‰å”¯ä¸€çš„æº ID
    }
)
```

ä½¿ç”¨è‡ªå®šä¹‰ç›®æ ‡å®ä½“æ•°æ®ï¼š

```python
# ä¸ºåˆå¹¶åçš„å®ä½“æŒ‡å®šç²¾ç¡®å€¼
rag.merge_entities(
    source_entities=["New York", "NYC", "Big Apple"],
    target_entity="New York City",
    target_entity_data={
        "entity_type": "LOCATION",
        "description": "New York City is the most populous city in the United States.",
    }
)
```

ç»“åˆä¸Šè¿°ä¸¤ç§æ–¹å¼çš„é«˜çº§ç”¨æ³•ï¼š

```python
# åˆå¹¶å…¬å¸å®ä½“ï¼ŒåŒæ—¶ä½¿ç”¨ç­–ç•¥å’Œè‡ªå®šä¹‰æ•°æ®
rag.merge_entities(
    source_entities=["Microsoft Corp", "Microsoft Corporation", "MSFT"],
    target_entity="Microsoft",
    merge_strategy={
        "description": "concatenate",  # åˆå¹¶æ‰€æœ‰æè¿°
        "source_id": "join_unique"     # åˆå¹¶æº ID
    },
    target_entity_data={
        "entity_type": "ORGANIZATION",
    }
)
```

åˆå¹¶å®ä½“æ—¶ï¼š

* æ‰€æœ‰æ¥è‡ªæºå®ä½“çš„å…³ç³»éƒ½ä¼šé‡å®šå‘åˆ°ç›®æ ‡å®ä½“
* é‡å¤çš„å…³ç³»ä¼šè¢«æ™ºèƒ½åˆå¹¶
* é˜²æ­¢å‡ºç°è‡ªæˆ‘æŒ‡å‘çš„å…³ç³»ï¼ˆè‡ªç¯ï¼‰
* åˆå¹¶å®Œæˆåæºå®ä½“ä¼šè¢«ç§»é™¤
* å…³ç³»æƒé‡å’Œå±æ€§ä¼šè¢«ä¿ç•™

</details>

## å¤šæ¨¡æ€æ–‡æ¡£å¤„ç†ï¼ˆRAG-Anything é›†æˆï¼‰

LightRAG ç°å·²ä¸ [RAG-Anything](https://github.com/HKUDS/RAG-Anything) æ— ç¼é›†æˆï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸º LightRAG æ„å»ºçš„**å…¨èƒ½å¤šæ¨¡æ€æ–‡æ¡£å¤„ç† RAG ç³»ç»Ÿ**ã€‚RAG-Anything èƒ½å¤Ÿå®ç°å…ˆè¿›çš„è§£æå’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½åŠ›ï¼Œå…è®¸æ‚¨æ— ç¼å¤„ç†å¤šæ¨¡æ€æ–‡æ¡£ï¼Œå¹¶ä»å„ç§æ–‡æ¡£æ ¼å¼ä¸­æå–ç»“æ„åŒ–å†…å®¹â€”â€”åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€è¡¨æ ¼å’Œå…¬å¼â€”â€”ä»¥é›†æˆåˆ°æ‚¨çš„ RAG æµç¨‹ä¸­ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- **ç«¯åˆ°ç«¯å¤šæ¨¡æ€æµç¨‹**ï¼šä»æ–‡æ¡£æ‘„å–è§£æåˆ°æ™ºèƒ½å¤šæ¨¡æ€é—®ç­”çš„å®Œæ•´å·¥ä½œæµç¨‹
- **é€šç”¨æ–‡æ¡£æ”¯æŒ**ï¼šæ— ç¼å¤„ç† PDFã€Office æ–‡æ¡£ï¼ˆDOC/DOCX/PPT/PPTX/XLS/XLSXï¼‰ã€å›¾åƒåŠå¤šç§æ–‡ä»¶æ ¼å¼
- **ä¸“ä¸šå†…å®¹åˆ†æ**ï¼šé’ˆå¯¹å›¾åƒã€è¡¨æ ¼ã€æ•°å­¦å…¬å¼åŠå¼‚æ„å†…å®¹ç±»å‹çš„ä¸“ç”¨å¤„ç†å™¨
- **å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±**ï¼šè‡ªåŠ¨å®ä½“æå–å’Œè·¨æ¨¡æ€å…³ç³»å‘ç°ï¼Œå¢å¼ºç†è§£åŠ›
- **æ··åˆæ™ºèƒ½æ£€ç´¢**ï¼šè·¨è¶Šæ–‡æœ¬å’Œå¤šæ¨¡æ€å†…å®¹çš„é«˜çº§æœç´¢èƒ½åŠ›ï¼Œå…·å¤‡ä¸Šä¸‹æ–‡ç†è§£

**å¿«é€Ÿå¼€å§‹ï¼š**
1. å®‰è£… RAG-Anythingï¼š
   ```bash
   pip install raganything
   ```
2. å¤„ç†å¤šæ¨¡æ€æ–‡æ¡£ï¼š
    <details>
    <summary> <b> RAGAnything ä½¿ç”¨ç¤ºä¾‹ </b></summary>

    ```python
        import asyncio
        from raganything import RAGAnything
        from lightrag import LightRAG
        from lightrag.llm.openai import openai_complete_if_cache, openai_embed
        from lightrag.utils import EmbeddingFunc
        import os

        async def load_existing_lightrag():
            # é¦–å…ˆï¼Œåˆ›å»ºæˆ–åŠ è½½ä¸€ä¸ªç°æœ‰çš„ LightRAG å®ä¾‹
            lightrag_working_dir = "./existing_lightrag_storage"

            # æ£€æŸ¥å…ˆå‰çš„ LightRAG å®ä¾‹æ˜¯å¦å­˜åœ¨
            if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
                print("âœ… Found existing LightRAG instance, loading...")
            else:
                print("âŒ No existing LightRAG instance found, will create new one")

            from functools import partial

            # ä½¿ç”¨æ‚¨çš„é…ç½®åˆ›å»º/åŠ è½½ LightRAG å®ä¾‹
            lightrag_instance = LightRAG(
                working_dir=lightrag_working_dir,
                llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
                    "gpt-4o-mini",
                    prompt,
                    system_prompt=system_prompt,
                    history_messages=history_messages,
                    api_key="your-api-key",
                    **kwargs,
                ),
                embedding_func=EmbeddingFunc(
                    embedding_dim=3072,
                    max_token_size=8192,
                    model="text-embedding-3-large",
                    func=partial(
                        openai_embed.func,  # ä½¿ç”¨ .func è®¿é—®æœªå°è£…çš„åŸå§‹å‡½æ•°
                        model="text-embedding-3-large",
                        api_key=api_key,
                        base_url=base_url,
                    ),
                )
            )

            # åˆå§‹åŒ–å­˜å‚¨ï¼ˆè¿™å°†åŠ è½½ç°æœ‰æ•°æ®ï¼Œå¦‚æœæœ‰çš„è¯ï¼‰
            await lightrag_instance.initialize_storages()

            # ç°åœ¨ä½¿ç”¨ç°æœ‰çš„ LightRAG å®ä¾‹åˆå§‹åŒ– RAGAnything
            rag = RAGAnything(
                lightrag=lightrag_instance,  # ä¼ å…¥ç°æœ‰çš„ LightRAG å®ä¾‹
                # ä»…åœ¨å¤šæ¨¡æ€å¤„ç†æ—¶éœ€è¦è§†è§‰æ¨¡å‹
                vision_model_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
                    "gpt-4o",
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=[
                        {"role": "system", "content": system_prompt} if system_prompt else None,
                        {"role": "user", "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                        ]} if image_data else {"role": "user", "content": prompt}
                    ],
                    api_key="your-api-key",
                    **kwargs,
                ) if image_data else openai_complete_if_cache(
                    "gpt-4o-mini",
                    prompt,
                    system_prompt=system_prompt,
                    history_messages=history_messages,
                    api_key="your-api-key",
                    **kwargs,
                )
                # æ³¨æ„ï¼šworking_dir, llm_model_func, embedding_func ç­‰éƒ½ç»§æ‰¿è‡ª lightrag_instance
            )

            # æŸ¥è¯¢ç°æœ‰çš„çŸ¥è¯†åº“
            result = await rag.query_with_multimodal(
                "What data has been processed in this LightRAG instance?",
                mode="hybrid"
            )
            print("Query result:", result)

            # å‘ç°æœ‰çš„ LightRAG å®ä¾‹æ·»åŠ æ–°çš„å¤šæ¨¡æ€æ–‡æ¡£
            await rag.process_document_complete(
                file_path="path/to/new/multimodal_document.pdf",
                output_dir="./output"
            )

        if __name__ == "__main__":
            asyncio.run(load_existing_lightrag())
    ```
    </details>

æœ‰å…³è¯¦ç»†æ–‡æ¡£å’Œé«˜çº§ç”¨æ³•ï¼Œè¯·å‚è€ƒ [RAG-Anything ä»“åº“](https://github.com/HKUDS/RAG-Anything)ã€‚

## Token ä½¿ç”¨é‡è·Ÿè¸ª

<details>
<summary> <b>æ¦‚è§ˆä¸ç”¨æ³•</b> </summary>

LightRAG æä¾›äº†ä¸€ä¸ª TokenTracker å·¥å…·ï¼Œç”¨äºç›‘æ§å’Œç®¡ç†å¤§è¯­è¨€æ¨¡å‹çš„ token æ¶ˆè€—æƒ…å†µã€‚æ­¤åŠŸèƒ½å¯¹äºæ§åˆ¶ API æˆæœ¬å’Œä¼˜åŒ–æ€§èƒ½éå¸¸æœ‰ç”¨ã€‚

### ç”¨æ³•

```python
from lightrag.utils import TokenTracker

# åˆ›å»º TokenTracker å®ä¾‹
token_tracker = TokenTracker()

# æ–¹æ³• 1ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ¨èï¼‰
# é€‚ç”¨äºéœ€è¦è‡ªåŠ¨è·Ÿè¸ª token ä½¿ç”¨é‡çš„åœºæ™¯
with token_tracker:
    result1 = await llm_model_func("your question 1")
    result2 = await llm_model_func("your question 2")

# æ–¹æ³• 2ï¼šæ‰‹åŠ¨æ·»åŠ  token ä½¿ç”¨è®°å½•
# é€‚ç”¨äºéœ€è¦æ›´ç²¾ç»†æ§åˆ¶ token ç»Ÿè®¡çš„åœºæ™¯
token_tracker.reset()

rag.insert()

rag.query("your question 1", param=QueryParam(mode="naive"))
rag.query("your question 2", param=QueryParam(mode="mix"))

# æ˜¾ç¤ºæ€» token ä½¿ç”¨é‡ï¼ˆåŒ…æ‹¬æ’å…¥å’ŒæŸ¥è¯¢æ“ä½œï¼‰
print("Token usage:", token_tracker.get_usage())
```

### ä½¿ç”¨æŠ€å·§
- åœ¨é•¿ä¼šè¯æˆ–æ‰¹é‡æ“ä½œä¸­ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨è·Ÿè¸ªæ‰€æœ‰ token æ¶ˆè€—
- å¯¹äºéœ€è¦åˆ†æ®µç»Ÿè®¡çš„åœºæ™¯ï¼Œä½¿ç”¨æ‰‹åŠ¨æ¨¡å¼å¹¶åœ¨é€‚å½“æ—¶å€™è°ƒç”¨ reset()
- å®šæœŸæ£€æŸ¥ token ä½¿ç”¨é‡æœ‰åŠ©äºåŠæ—©å‘ç°å¼‚å¸¸æ¶ˆè€—
- åœ¨å¼€å‘å’Œæµ‹è¯•è¿‡ç¨‹ä¸­ç§¯æä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œä»¥ä¼˜åŒ–ç”Ÿäº§æˆæœ¬

### å®è·µæ¡ˆä¾‹
æ‚¨å¯ä»¥å‚è€ƒä»¥ä¸‹ç¤ºä¾‹æ¥å®æ–½ token è·Ÿè¸ªï¼š
- `examples/lightrag_gemini_track_token_demo.py`ï¼šä½¿ç”¨ Google Gemini æ¨¡å‹çš„ token è·Ÿè¸ªç¤ºä¾‹
- `examples/lightrag_siliconcloud_track_token_demo.py`ï¼šä½¿ç”¨ SiliconCloud æ¨¡å‹çš„ token è·Ÿè¸ªç¤ºä¾‹

è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ä¸åŒæ¨¡å‹å’Œåœºæ™¯ä¸‹æœ‰æ•ˆåœ°ä½¿ç”¨ TokenTracker åŠŸèƒ½ã€‚

</details>

## æ•°æ®å¯¼å‡ºåŠŸèƒ½

### æ¦‚è§ˆ

LightRAG å…è®¸æ‚¨ä»¥å„ç§æ ¼å¼å¯¼å‡ºçŸ¥è¯†å›¾è°±æ•°æ®ï¼Œç”¨äºåˆ†æã€å…±äº«å’Œå¤‡ä»½ã€‚ç³»ç»Ÿæ”¯æŒå¯¼å‡ºå®ä½“ã€å…³ç³»åŠå…³ç³»æ•°æ®ã€‚

### å¯¼å‡ºå‡½æ•°

<details>
  <summary> <b> åŸºç¡€ç”¨æ³• </b></summary>

```python
# åŸºç¡€ CSV å¯¼å‡ºï¼ˆé»˜è®¤æ ¼å¼ï¼‰
rag.export_data("knowledge_graph.csv")

# æŒ‡å®šä»»æ„æ ¼å¼
rag.export_data("output.xlsx", file_format="excel")
```

</details>

<details>
  <summary> <b> æ”¯æŒçš„ä¸åŒæ–‡ä»¶æ ¼å¼ </b></summary>

```python
# ä»¥ CSV æ ¼å¼å¯¼å‡ºæ•°æ®
rag.export_data("graph_data.csv", file_format="csv")

# å¯¼å‡ºåˆ° Excel å·¥ä½œè¡¨
rag.export_data("graph_data.xlsx", file_format="excel")

# ä»¥ markdown æ ¼å¼å¯¼å‡ºæ•°æ®
rag.export_data("graph_data.md", file_format="md")

# å¯¼å‡ºä¸ºçº¯æ–‡æœ¬
rag.export_data("graph_data.txt", file_format="txt")
```
</details>

<details>
  <summary> <b> é™„åŠ é€‰é¡¹ </b></summary>

åœ¨å¯¼å‡ºä¸­åŒ…å«å‘é‡åµŒå…¥ï¼ˆå¯é€‰ï¼‰ï¼š

```python
rag.export_data("complete_data.csv", include_vector_data=True)
```
</details>

### å¯¼å‡ºä¸­åŒ…å«çš„æ•°æ®

æ‰€æœ‰å¯¼å‡ºå‡åŒ…å«ï¼š

* å®ä½“ä¿¡æ¯ï¼ˆåç§°ã€IDã€å…ƒæ•°æ®ï¼‰
* å…³ç³»æ•°æ®ï¼ˆå®ä½“é—´çš„è¿æ¥ï¼‰
* æ¥è‡ªå‘é‡æ•°æ®åº“çš„å…³ç³»ä¿¡æ¯

## ç¼“å­˜

<details>
  <summary> <b>æ¸…é™¤ç¼“å­˜</b> </summary>

æ‚¨å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ¨¡å¼æ¸…é™¤ LLM å“åº”ç¼“å­˜ï¼š

```python
# æ¸…é™¤æ‰€æœ‰ç¼“å­˜
await rag.aclear_cache()

# æ¸…é™¤ local æ¨¡å¼ç¼“å­˜
await rag.aclear_cache(modes=["local"])

# æ¸…é™¤æå–ï¼ˆextractionï¼‰ç¼“å­˜
await rag.aclear_cache(modes=["default"])

# æ¸…é™¤å¤šä¸ªæ¨¡å¼çš„ç¼“å­˜
await rag.aclear_cache(modes=["local", "global", "hybrid"])

# åŒæ­¥ç‰ˆæœ¬
rag.clear_cache(modes=["local"])
```

æœ‰æ•ˆæ¨¡å¼åŒ…æ‹¬ï¼š

- `"default"`ï¼šæå–ç¼“å­˜
- `"naive"`ï¼šæœ´ç´ æœç´¢ç¼“å­˜
- `"local"`ï¼šæœ¬åœ°æœç´¢ç¼“å­˜
- `"global"`ï¼šå…¨å±€æœç´¢ç¼“å­˜
- `"hybrid"`ï¼šæ··åˆæœç´¢ç¼“å­˜
- `"mix"`ï¼šæ··åˆï¼ˆMixï¼‰æœç´¢ç¼“å­˜

</details>

## æ•…éšœæ’é™¤

### å¸¸è§åˆå§‹åŒ–é”™è¯¯

å¦‚æœæ‚¨åœ¨ä½¿ç”¨ LightRAG æ—¶é‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š

1. **`AttributeError: __aenter__`**
   - **åŸå› **ï¼šå­˜å‚¨åç«¯æœªåˆå§‹åŒ–
   - **è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨åˆ›å»º LightRAG å®ä¾‹åè°ƒç”¨ `await rag.initialize_storages()`

2. **`KeyError: 'history_messages'`**
   - **åŸå› **ï¼šæµæ°´çº¿çŠ¶æ€æœªåˆå§‹åŒ–
   - **è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨åˆ›å»º LightRAG å®ä¾‹åè°ƒç”¨ `await rag.initialize_storages()`

3. **ä¸¤ä¸ªé”™è¯¯ç›¸ç»§å‡ºç°**
   - **åŸå› **ï¼šä¸¤ä¸ªåˆå§‹åŒ–æ–¹æ³•éƒ½æœªè¢«è°ƒç”¨
   - **è§£å†³æ–¹æ¡ˆ**ï¼šå§‹ç»ˆéµå¾ªä»¥ä¸‹æ¨¡å¼ï¼š
   ```python
   rag = LightRAG(...)
   await rag.initialize_storages()
   ```

### æ¨¡å‹åˆ‡æ¢é—®é¢˜

åœ¨ä¸åŒçš„åµŒå…¥æ¨¡å‹ï¼ˆembedding modelsï¼‰ä¹‹é—´åˆ‡æ¢æ—¶ï¼Œæ‚¨å¿…é¡»æ¸…ç©ºæ•°æ®ç›®å½•ä»¥é¿å…é”™è¯¯ã€‚å¦‚æœæ‚¨å¸Œæœ›ä¿ç•™ LLM ç¼“å­˜ï¼Œå”¯ä¸€å¯ä»¥ä¿ç•™çš„æ–‡ä»¶æ˜¯ `kv_store_llm_response_cache.json`ã€‚

## LightRAG API

LightRAG æœåŠ¡å™¨æ—¨åœ¨æä¾› Web UI å’Œ API æ”¯æŒã€‚**æœ‰å…³ LightRAG æœåŠ¡å™¨çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ [LightRAG Server](./lightrag/api/README.md)ã€‚**

## å›¾è°±å¯è§†åŒ–

LightRAG æœåŠ¡å™¨æä¾›äº†å…¨é¢çš„çŸ¥è¯†å›¾è°±å¯è§†åŒ–åŠŸèƒ½ã€‚å®ƒæ”¯æŒå„ç§é‡åŠ›å¸ƒå±€ã€èŠ‚ç‚¹æŸ¥è¯¢ã€å­å›¾è¿‡æ»¤ç­‰ã€‚**æœ‰å…³ LightRAG æœåŠ¡å™¨çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ [LightRAG Server](./lightrag/api/README.md)ã€‚**

![iShot_2025-03-23_12.40.08](./README.assets/iShot_2025-03-23_12.40.08.png)

## Langfuse å¯è§‚æµ‹æ€§é›†æˆ

Langfuse æä¾›äº†ä¸€ä¸ªå¯ä»¥ç›´æ¥æ›¿æ¢ OpenAI å®¢æˆ·ç«¯çš„æ–¹æ¡ˆï¼Œè‡ªåŠ¨è·Ÿè¸ªæ‰€æœ‰ LLM äº¤äº’ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿåœ¨ä¸æ›´æ”¹ä»£ç çš„æƒ…å†µä¸‹ç›‘æ§ã€è°ƒè¯•å’Œä¼˜åŒ–å…¶ RAG ç³»ç»Ÿã€‚

### å®‰è£…å¯è§‚æµ‹æ€§é€‰é¡¹

```bash
pip install lightrag-hku
pip install lightrag-hku[observability]

# æˆ–ä»æºä»£ç å®‰è£…å¹¶å¯ç”¨è°ƒè¯•æ¨¡å¼
pip install -e .
pip install -e ".[observability]"
```

### é…ç½® Langfuse ç¯å¢ƒå˜é‡

ä¿®æ”¹ .env æ–‡ä»¶ï¼š

```bash
## Langfuse Observability (Optional)
# LLM observability and tracing platform
# Install with: pip install lightrag-hku[observability]
# Sign up at: https://cloud.langfuse.com or self-host
LANGFUSE_SECRET_KEY=""
LANGFUSE_PUBLIC_KEY=""
LANGFUSE_HOST="https://cloud.langfuse.com"  # æˆ–æ‚¨çš„è‡ªæ‰˜ç®¡å®ä¾‹
LANGFUSE_ENABLE_TRACE=true
```

### Langfuse ç”¨æ³•

å®‰è£…å¹¶é…ç½®å®Œæˆåï¼ŒLangfuse ä¼šè‡ªåŠ¨è¿½è¸ªæ‰€æœ‰ OpenAI LLM è°ƒç”¨ã€‚Langfuse ä»ªè¡¨æ¿åŠŸèƒ½åŒ…æ‹¬ï¼š

- **è¿½è¸ªï¼ˆTracingï¼‰**ï¼šæŸ¥çœ‹å®Œæ•´çš„ LLM è°ƒç”¨é“¾
- **åˆ†æï¼ˆAnalyticsï¼‰**ï¼šToken ä½¿ç”¨æƒ…å†µã€å»¶è¿Ÿã€æˆæœ¬æŒ‡æ ‡
- **è°ƒè¯•ï¼ˆDebuggingï¼‰**ï¼šæ£€æŸ¥æç¤ºè¯å’Œå“åº”
- **è¯„ä¼°ï¼ˆEvaluationï¼‰**ï¼šæ¯”è¾ƒæ¨¡å‹è¾“å‡º
- **ç›‘æ§ï¼ˆMonitoringï¼‰**ï¼šå®æ—¶å‘Šè­¦

### é‡è¦é€šçŸ¥

**æ³¨æ„**ï¼šLightRAG ç›®å‰ä»…å°† OpenAI å…¼å®¹çš„ API è°ƒç”¨ä¸ Langfuse é›†æˆã€‚Ollamaã€Azure å’Œ AWS Bedrock ç­‰ API å°šä¸æ”¯æŒ Langfuse å¯è§‚æµ‹æ€§ã€‚

## åŸºäº RAGAS çš„è¯„ä¼°

**RAGAS** (Retrieval Augmented Generation Assessment) æ˜¯ä¸€ä¸ªä½¿ç”¨ LLM å¯¹ RAG ç³»ç»Ÿè¿›è¡Œæ— å‚è€ƒè¯„ä¼°çš„æ¡†æ¶ã€‚é¡¹ç›®ä¸­åŒ…å«ä¸€ä¸ªåŸºäº RAGAS çš„è¯„ä¼°è„šæœ¬ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒ [åŸºäº RAGAS çš„è¯„ä¼°æ¡†æ¶](lightrag/evaluation/README_EVALUASTION_RAGAS.md)ã€‚

## è¯„ä¼°

### æ•°æ®é›†

LightRAG ä¸­ä½¿ç”¨çš„æ•°æ®é›†å¯ä»¥ä» [TommyChien/UltraDomain](https://huggingface.co/datasets/TommyChien/UltraDomain) ä¸‹è½½ã€‚

### ç”ŸæˆæŸ¥è¯¢

LightRAG ä½¿ç”¨ä»¥ä¸‹æç¤ºï¼ˆpromptï¼‰ç”Ÿæˆé«˜å±‚çº§æŸ¥è¯¢ï¼Œç›¸åº”ä»£ç ä½äº `examples/generate_query.py`ã€‚

<details>
<summary> æç¤ºè¯ </summary>

```python
Given the following description of a dataset:

{description}

Please identify 5 potential users who would engage with this dataset. For each user, list 5 tasks they would perform with this dataset. Then, for each (user, task) combination, generate 5 questions that require a high-level understanding of the entire dataset.

Output the results in the following structure:
- User 1: [user description]
    - Task 1: [task description]
        - Question 1:
        - Question 2:
        - Question 3:
        - Question 4:
        - Question 5:
    - Task 2: [task description]
        ...
    - Task 5: [task description]
- User 2: [user description]
    ...
- User 5: [user description]
    ...
```

</details>

### æ‰¹é‡è¯„ä¼°

ä¸ºäº†åœ¨å¤„ç†é«˜å±‚çº§æŸ¥è¯¢æ—¶è¯„ä¼°ä¸¤ä¸ª RAG ç³»ç»Ÿçš„æ€§èƒ½ï¼ŒLightRAG ä½¿ç”¨ä»¥ä¸‹æç¤ºè¯ï¼Œå…·ä½“ä»£ç è§ `reproduce/batch_eval.py`ã€‚

<details>
<summary> æç¤ºè¯ </summary>

```python
---Role---
You are an expert tasked with evaluating two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.
---Goal---
You will evaluate two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.

- **Comprehensiveness**: How much detail does the answer provide to cover all aspects and details of the question?
- **Diversity**: How varied and rich is the answer in providing different perspectives and insights on the question?
- **Empowerment**: How well does the answer help the reader understand and make informed judgments about the topic?

For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these three categories.

Here is the question:
{query}

Here are the two answers:

**Answer 1:**
{answer1}

**Answer 2:**
{answer2}

Evaluate both answers using the three criteria listed above and provide detailed explanations for each criterion.

Output your evaluation in the following JSON format:

{{
    "Comprehensiveness": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide explanation here]"
    }},
    "Empowerment": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide explanation here]"
    }},
    "Overall Winner": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Summarize why this answer is the overall winner based on the three criteria]"
    }}
}}
```

</details>

### æ€»ä½“æ€§èƒ½è¡¨

||**å†œä¸š**||**è®¡ç®—æœºç§‘å­¦**||**æ³•å¾‹**||**æ··åˆ**||
|----------------------|---------------|------------|------|------------|---------|------------|-------|------------|
||NaiveRAG|**LightRAG**|NaiveRAG|**LightRAG**|NaiveRAG|**LightRAG**|NaiveRAG|**LightRAG**|
|**å…¨é¢æ€§**|32.4%|**67.6%**|38.4%|**61.6%**|16.4%|**83.6%**|38.8%|**61.2%**|
|**å¤šæ ·æ€§**|23.6%|**76.4%**|38.0%|**62.0%**|13.6%|**86.4%**|32.4%|**67.6%**|
|**èµ‹èƒ½æ€§**|32.4%|**67.6%**|38.8%|**61.2%**|16.4%|**83.6%**|42.8%|**57.2%**|
|**æ€»ä½“**|32.4%|**67.6%**|38.8%|**61.2%**|15.2%|**84.8%**|40.0%|**60.0%**|
||RQ-RAG|**LightRAG**|RQ-RAG|**LightRAG**|RQ-RAG|**LightRAG**|RQ-RAG|**LightRAG**|
|**å…¨é¢æ€§**|31.6%|**68.4%**|38.8%|**61.2%**|15.2%|**84.8%**|39.2%|**60.8%**|
|**å¤šæ ·æ€§**|29.2%|**70.8%**|39.2%|**60.8%**|11.6%|**88.4%**|30.8%|**69.2%**|
|**èµ‹èƒ½æ€§**|31.6%|**68.4%**|36.4%|**63.6%**|15.2%|**84.8%**|42.4%|**57.6%**|
|**æ€»ä½“**|32.4%|**67.6%**|38.0%|**62.0%**|14.4%|**85.6%**|40.0%|**60.0%**|
||HyDE|**LightRAG**|HyDE|**LightRAG**|HyDE|**LightRAG**|HyDE|**LightRAG**|
|**å…¨é¢æ€§**|26.0%|**74.0%**|41.6%|**58.4%**|26.8%|**73.2%**|40.4%|**59.6%**|
|**å¤šæ ·æ€§**|24.0%|**76.0%**|38.8%|**61.2%**|20.0%|**80.0%**|32.4%|**67.6%**|
|**èµ‹èƒ½æ€§**|25.2%|**74.8%**|40.8%|**59.2%**|26.0%|**74.0%**|46.0%|**54.0%**|
|**æ€»ä½“**|24.8%|**75.2%**|41.6%|**58.4%**|26.4%|**73.6%**|42.4%|**57.6%**|
||GraphRAG|**LightRAG**|GraphRAG|**LightRAG**|GraphRAG|**LightRAG**|GraphRAG|**LightRAG**|
|**å…¨é¢æ€§**|45.6%|**54.4%**|48.4%|**51.6%**|48.4%|**51.6%**|**50.4%**|49.6%|
|**å¤šæ ·æ€§**|22.8%|**77.2%**|40.8%|**59.2%**|26.4%|**73.6%**|36.0%|**64.0%**|
|**èµ‹èƒ½æ€§**|41.2%|**58.8%**|45.2%|**54.8%**|43.6%|**56.4%**|**50.8%**|49.2%|
|**æ€»ä½“**|45.2%|**54.8%**|48.0%|**52.0%**|47.2%|**52.8%**|**50.4%**|49.6%|

## å¤ç°

æ‰€æœ‰ä»£ç å‡å¯åœ¨ `./reproduce` ç›®å½•ä¸­æ‰¾åˆ°ã€‚

### Step-0 æå–å”¯ä¸€ä¸Šä¸‹æ–‡

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æå–æ•°æ®é›†ä¸­çš„å”¯ä¸€ä¸Šä¸‹æ–‡ï¼ˆunique contextsï¼‰ã€‚

<details>
<summary> ä»£ç  </summary>

```python
def extract_unique_contexts(input_directory, output_directory):

    os.makedirs(output_directory, exist_ok=True)

    jsonl_files = glob.glob(os.path.join(input_directory, '*.jsonl'))
    print(f"Found {len(jsonl_files)} JSONL files.")

    for file_path in jsonl_files:
        filename = os.path.basename(file_path)
        name, ext = os.path.splitext(filename)
        output_filename = f"{name}_unique_contexts.json"
        output_path = os.path.join(output_directory, output_filename)

        unique_contexts_dict = {}

        print(f"Processing file: {filename}")

        try:
            with open(file_path, 'r', encoding='utf-8') as infile:
                for line_number, line in enumerate(infile, start=1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        json_obj = json.loads(line)
                        context = json_obj.get('context')
                        if context and context not in unique_contexts_dict:
                            unique_contexts_dict[context] = None
                    except json.JSONDecodeError as e:
                        print(f"JSON decoding error in file {filename} at line {line_number}: {e}")
        except FileNotFoundError:
            print(f"File not found: {filename}")
            continue
        except Exception as e:
            print(f"An error occurred while processing file {filename}: {e}")
            continue

        unique_contexts_list = list(unique_contexts_dict.keys())
        print(f"There are {len(unique_contexts_list)} unique `context` entries in the file {filename}.")

        try:
            with open(output_path, 'w', encoding='utf-8') as outfile:
                json.dump(unique_contexts_list, outfile, ensure_ascii=False, indent=4)
            print(f"Unique `context` entries have been saved to: {output_filename}")
        except Exception as e:
            print(f"An error occurred while saving to the file {output_filename}: {e}")

    print("All files have been processed.")

```

</details>

### Step-1 æ’å…¥ä¸Šä¸‹æ–‡

æˆ‘ä»¬å°†æå–å‡ºçš„ä¸Šä¸‹æ–‡æ’å…¥åˆ° LightRAG ç³»ç»Ÿä¸­ã€‚

<details>
<summary> ä»£ç  </summary>

```python
def insert_text(rag, file_path):
    with open(file_path, mode='r') as f:
        unique_contexts = json.load(f)

    retries = 0
    max_retries = 3
    while retries < max_retries:
        try:
            rag.insert(unique_contexts)
            break
        except Exception as e:
            retries += 1
            print(f"Insertion failed, retrying ({retries}/{max_retries}), error: {e}")
            time.sleep(10)
    if retries == max_retries:
        print("Insertion failed after exceeding the maximum number of retries")
```

</details>

### Step-2 ç”ŸæˆæŸ¥è¯¢

æˆ‘ä»¬ä»æ•°æ®é›†æ¯ä¸ªä¸Šä¸‹æ–‡çš„å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†æå– tokenï¼Œç„¶åå°†å®ƒä»¬ç»„åˆä½œä¸ºæ•°æ®é›†æè¿°æ¥ç”ŸæˆæŸ¥è¯¢ã€‚

<details>
<summary> ä»£ç  </summary>

```python
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

def get_summary(context, tot_tokens=2000):
    tokens = tokenizer.tokenize(context)
    half_tokens = tot_tokens // 2

    start_tokens = tokens[1000:1000 + half_tokens]
    end_tokens = tokens[-(1000 + half_tokens):1000]

    summary_tokens = start_tokens + end_tokens
    summary = tokenizer.convert_tokens_to_string(summary_tokens)

    return summary
```

</details>

### Step-3 æŸ¥è¯¢

å¯¹äº Step-2 ä¸­ç”Ÿæˆçš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬å°†æå–å®ƒä»¬å¹¶å¯¹ LightRAG è¿›è¡ŒæŸ¥è¯¢ã€‚

<details>
<summary> ä»£ç  </summary>

```python
def extract_queries(file_path):
    with open(file_path, 'r') as f:
        data = f.read()

    data = data.replace('**', '')

    queries = re.findall(r'- Question \d+: (.+)', data)

    return queries
```

</details>

## ğŸ”— ç›¸å…³é¡¹ç›®

*ç”Ÿæ€ä¸æ‰©å±•*

<div align="center">
  <table>
    <tr>
      <td align="center">
        <a href="https://github.com/HKUDS/RAG-Anything">
          <div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
            <span style="font-size: 32px;">ğŸ“¸</span>
          </div>
          <b>RAG-Anything</b><br>
          <sub>å¤šæ¨¡æ€ RAG</sub>
        </a>
      </td>
      <td align="center">
        <a href="https://github.com/HKUDS/VideoRAG">
          <div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
            <span style="font-size: 32px;">ğŸ¥</span>
          </div>
          <b>VideoRAG</b><br>
          <sub>æç«¯é•¿ä¸Šä¸‹æ–‡è§†é¢‘ RAG</sub>
        </a>
      </td>
      <td align="center">
        <a href="https://github.com/HKUDS/MiniRAG">
          <div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
            <span style="font-size: 32px;">âœ¨</span>
          </div>
          <b>MiniRAG</b><br>
          <sub>æç®€ RAG</sub>
        </a>
      </td>
    </tr>
  </table>
</div>

---

## â­ Star å†å²

<a href="https://star-history.com/#HKUDS/LightRAG&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/LightRAG&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/LightRAG&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/LightRAG&type=Date" />
 </picture>
</a>

## ğŸ¤ è´¡çŒ®

<div align="center">
  æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…åšå‡ºçš„å®è´µè´¡çŒ®ã€‚
</div>

<div align="center">
  <a href="https://github.com/HKUDS/LightRAG/graphs/contributors">
    <img src="https://contrib.rocks/image?repo=HKUDS/LightRAG" style="border-radius: 15px; box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);" />
  </a>
</div>

---

## ğŸ“– å¼•ç”¨

```python
@article{guo2024lightrag,
title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
year={2024},
eprint={2410.05779},
archivePrefix={arXiv},
primaryClass={cs.IR}
}
```

---

<div align="center" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 30px 0;">
  <div>
    <img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="500">
  </div>
  <div style="margin-top: 20px;">
    <a href="https://github.com/HKUDS/LightRAG" style="text-decoration: none;">
      <img src="https://img.shields.io/badge/â­%20åœ¨%20GitHub%20ä¸Šç‚¹äº®æ˜Ÿæ˜Ÿ-1a1a2e?style=for-the-badge&logo=github&logoColor=white">
    </a>
    <a href="https://github.com/HKUDS/LightRAG/issues" style="text-decoration: none;">
      <img src="https://img.shields.io/badge/ğŸ›%20æŠ¥å‘Šé—®é¢˜-ff6b6b?style=for-the-badge&logo=github&logoColor=white">
    </a>
    <a href="https://github.com/HKUDS/LightRAG/discussions" style="text-decoration: none;">
      <img src="https://img.shields.io/badge/ğŸ’¬%20è®¨è®º-4ecdc4?style=for-the-badge&logo=github&logoColor=white">
    </a>
  </div>
</div>

<div align="center">
  <div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);">
    <div style="display: flex; justify-content: center; align-items: center; gap: 15px;">
      <span style="font-size: 24px;">â­</span>
      <span style="color: #00d9ff; font-size: 18px;">æ„Ÿè°¢æ‚¨è®¿é—® LightRAG!</span>
      <span style="font-size: 24px;">â­</span>
    </div>
  </div>
</div>
