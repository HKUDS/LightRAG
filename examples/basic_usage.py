#!/usr/bin/env python3
"""
Ejemplo b√°sico de uso del m√≥dulo WorldClass RAG.

Demuestra:
1. Inicializaci√≥n del motor RAG
2. Procesamiento de documentos
3. Chunking inteligente
4. B√∫squeda h√≠brida
5. Evaluaci√≥n de rendimiento

Basado en las mejores pr√°cticas del video AI News & Strategy Daily.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from worldclass_rag import (
    RAGEngine,
    SemanticChunker,
    RecursiveChunker,
    SentenceChunker,
    HybridRetriever,
    OpenAIEmbeddings,
    SentenceTransformerEmbeddings,
    RAGEvaluator
)

from worldclass_rag.processors import TextProcessor, PDFProcessor
from worldclass_rag.core.retrieval import SemanticRetriever, KeywordRetriever
from worldclass_rag.core.evaluation import RelevanceEvaluator


def main():
    """Demostraci√≥n completa del sistema WorldClass RAG."""
    
    print("üåü WorldClass RAG - Demostraci√≥n B√°sica")
    print("=" * 50)
    
    # 1. Configurar modelos de embeddings
    print("\n1. Configurando modelos de embeddings...")
    
    try:
        # Intentar usar OpenAI (requiere API key)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        if not embeddings.is_available():
            raise Exception("OpenAI no disponible")
        print("‚úÖ Usando OpenAI embeddings")
    except:
        # Fallback a modelo local
        embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
        print("‚úÖ Usando Sentence Transformers embeddings (local)")
    
    # 2. Crear estrategias de chunking
    print("\n2. Configurando estrategias de chunking...")
    
    # Estrategia sem√°ntica (agrupa por significado)
    semantic_chunker = SemanticChunker(
        chunk_size=800,
        chunk_overlap=150,
        similarity_threshold=0.5
    )
    
    # Estrategia recursiva (respeta estructura)
    recursive_chunker = RecursiveChunker(
        chunk_size=1000,
        chunk_overlap=200,
        preserve_sentence_boundaries=True
    )
    
    # Estrategia por oraciones (nunca rompe oraciones)
    sentence_chunker = SentenceChunker(
        chunk_size=900,
        chunk_overlap=180,
        language="es"
    )
    
    print("‚úÖ Configuradas 3 estrategias de chunking")
    
    # 3. Crear retrievers
    print("\n3. Configurando sistema de retrieval h√≠brido...")
    
    semantic_retriever = SemanticRetriever(
        embedding_model=embeddings,
        similarity_threshold=0.3
    )
    
    keyword_retriever = KeywordRetriever(
        scoring_method="bm25",
        language="es"
    )
    
    # Configuraci√≥n h√≠brida optimizada
    from worldclass_rag.core.retrieval.hybrid_retriever import HybridSearchConfig
    
    hybrid_config = HybridSearchConfig(
        semantic_weight=0.7,    # Priorizar b√∫squeda sem√°ntica
        keyword_weight=0.3,     # Complementar con keywords
        fusion_method="rrf",    # Reciprocal Rank Fusion
        rerank=True,            # Aplicar re-ranking
        rerank_top_k=20,
        final_top_k=5
    )
    
    hybrid_retriever = HybridRetriever(
        semantic_retriever=semantic_retriever,
        keyword_retriever=keyword_retriever,
        config=hybrid_config
    )
    
    print("‚úÖ Sistema h√≠brido configurado (sem√°ntica + keywords + re-ranking)")
    
    # 4. Crear documentos de ejemplo
    print("\n4. Preparando documentos de ejemplo...")
    
    sample_documents = [
        {
            "content": """
            La Recuperaci√≥n Aumentada por Generaci√≥n (RAG) es una t√©cnica revolucionaria en IA 
            que combina la potencia de los Large Language Models con bases de conocimiento externas. 
            RAG permite que los modelos accedan a informaci√≥n actualizada y espec√≠fica del dominio, 
            eliminando las limitaciones de fechas de corte de conocimiento.
            
            El proceso RAG consta de tres fases principales:
            1. Recuperaci√≥n: Buscar informaci√≥n relevante en la base de conocimiento
            2. Aumento: Combinar la consulta con los datos recuperados  
            3. Generaci√≥n: Producir una respuesta fundamentada en evidencia real
            """,
            "metadata": {
                "source": "manual_rag.pdf",
                "section": "Introducci√≥n",
                "author": "WorldClass AI Team",
                "topic": "RAG Fundamentals"
            }
        },
        {
            "content": """
            Los embeddings son representaciones vectoriales de texto en espacios de alta dimensi√≥n,
            t√≠picamente 1,536 dimensiones seg√∫n las mejores pr√°cticas actuales. Estos vectores
            capturan el significado sem√°ntico del texto, permitiendo que significados similares
            se agrupen matem√°ticamente mediante similitud coseno.
            
            La elecci√≥n del modelo de embeddings es cr√≠tica para el √©xito de RAG:
            - OpenAI text-embedding-3-large: 3,072 dimensiones, m√°xima calidad
            - OpenAI text-embedding-3-small: 1,536 dimensiones, balance calidad-costo  
            - Sentence Transformers: Modelos locales, privacidad garantizada
            """,
            "metadata": {
                "source": "manual_embeddings.pdf", 
                "section": "Embeddings",
                "author": "WorldClass AI Team",
                "topic": "Vector Representations"
            }
        },
        {
            "content": """
            El chunking o segmentaci√≥n de documentos es un arte y ciencia cr√≠ticos en RAG.
            Una mala segmentaci√≥n puede arruinar completamente un proyecto RAG.
            
            Estrategias de chunking recomendadas:
            - Chunking sem√°ntico: Agrupa por significado, preserva coherencia tem√°tica
            - Chunking recursivo: Respeta jerarqu√≠as estructurales del documento
            - Chunking por oraciones: Nunca rompe oraciones, preserva gram√°tica
            
            Principio fundamental: SIEMPRE incluir superposici√≥n entre chunks para
            preservar contexto y evitar p√©rdida de informaci√≥n cr√≠tica.
            """,
            "metadata": {
                "source": "manual_chunking.pdf",
                "section": "Chunking Strategies", 
                "author": "WorldClass AI Team",
                "topic": "Document Processing"
            }
        },
        {
            "content": """
            La b√∫squeda h√≠brida representa el nivel 2 de RAG seg√∫n las mejores pr√°cticas.
            Combina b√∫squeda sem√°ntica (por significado) con b√∫squeda por palabras clave,
            ofreciendo mayor precisi√≥n y potencial de velocidad mejorada.
            
            Componentes de b√∫squeda h√≠brida:
            - B√∫squeda vectorial: Encuentra documentos sem√°nticamente relacionados
            - B√∫squeda BM25: Captura coincidencias exactas de t√©rminos importantes  
            - Re-ranking: Mejora significativamente la precisi√≥n para prop√≥sitos comerciales
            - Fusi√≥n de resultados: Combina ambas estrategias de forma inteligente
            """,
            "metadata": {
                "source": "manual_hybrid.pdf",
                "section": "Hybrid Search",
                "author": "WorldClass AI Team", 
                "topic": "Advanced Retrieval"
            }
        },
        {
            "content": """
            Las m√©tricas de evaluaci√≥n de RAG son fundamentales para el √©xito en producci√≥n.
            Seg√∫n AI News & Strategy Daily, existen 4 m√©tricas clave:
            
            1. Relevancia: ¬øSe recuperan los chunks correctos?
            2. Fidelidad: ¬øLa respuesta se basa en fuentes reales?
            3. Calidad: ¬øUn humano la calificar√≠a como correcta?
            4. Latencia: ¬øEs suficientemente r√°pido (menos de 2 segundos)?
            
            Un sistema RAG debe ser evaluado continuamente usando estas m√©tricas
            para identificar problemas y oportunidades de mejora sistem√°tica.
            """,
            "metadata": {
                "source": "manual_evaluation.pdf",
                "section": "RAG Metrics",
                "author": "WorldClass AI Team",
                "topic": "Evaluation Framework"
            }
        }
    ]
    
    print(f"‚úÖ Preparados {len(sample_documents)} documentos de ejemplo")
    
    # 5. Procesar documentos con diferentes estrategias de chunking
    print("\n5. Procesando documentos con chunking inteligente...")
    
    all_chunks = []
    
    for i, doc in enumerate(sample_documents):
        print(f"   Procesando documento {i+1}/5...")
        
        # Usar diferentes estrategias seg√∫n el contenido
        if "RAG" in doc["content"]:
            chunker = semantic_chunker  # Usar chunking sem√°ntico para contenido conceptual
        elif "embeddings" in doc["content"].lower():
            chunker = recursive_chunker  # Usar chunking recursivo para contenido t√©cnico
        else:
            chunker = sentence_chunker  # Usar chunking por oraciones por defecto
        
        # Procesar documento
        chunks = chunker.chunk_document(
            text=doc["content"],
            source=doc["metadata"]["source"],
            section=doc["metadata"]["section"],
            additional_metadata=doc["metadata"]
        )
        
        # A√±adir identificadores √∫nicos
        for j, chunk in enumerate(chunks):
            chunk.chunk_id = f"doc_{i}_chunk_{j}"
            chunk.add_metadata("document_index", i)
        
        all_chunks.extend(chunks)
    
    print(f"‚úÖ Generados {len(all_chunks)} chunks usando estrategias inteligentes")
    
    # 6. Indexar chunks en el sistema h√≠brido
    print("\n6. Indexando chunks en sistema h√≠brido...")
    
    # Preparar chunks para indexaci√≥n
    chunks_for_indexing = []
    for chunk in all_chunks:
        chunk_data = {
            "content": chunk.content,
            "chunk_id": chunk.chunk_id,
            "source_id": chunk.get_metadata("source", "unknown"),
            "metadata": chunk.metadata,
            "source_file": chunk.get_metadata("source"),
            "source_section": chunk.get_metadata("section"),
        }
        chunks_for_indexing.append(chunk_data)
    
    # Indexar en retriever h√≠brido
    hybrid_retriever.add_chunks(chunks_for_indexing)
    
    print("‚úÖ Chunks indexados en ambos sistemas (sem√°ntico + keywords)")
    
    # 7. Realizar consultas de prueba
    print("\n7. Realizando consultas de prueba...")
    
    test_queries = [
        "¬øQu√© es RAG y c√≥mo funciona?",
        "¬øCu√°les son las mejores pr√°cticas para chunking?", 
        "¬øC√≥mo funcionan los embeddings en RAG?",
        "¬øQu√© m√©tricas usar para evaluar RAG?",
        "¬øQu√© ventajas tiene la b√∫squeda h√≠brida?"
    ]
    
    results_summary = []
    
    for i, query in enumerate(test_queries):
        print(f"\n   Consulta {i+1}: {query}")
        
        # Realizar b√∫squeda h√≠brida
        results = hybrid_retriever.retrieve(query, top_k=3)
        
        # Mostrar resultados
        if results:
            print(f"   üìä Encontrados {len(results)} resultados relevantes:")
            for j, result in enumerate(results):
                print(f"      {j+1}. Score: {result.score:.3f} | Fuente: {result.source_id}")
                print(f"         Chunk: {result.content[:100]}...")
                
                # Informaci√≥n sobre el m√©todo de retrieval
                fusion_method = result.get_metadata("fusion_method", "unknown")
                print(f"         M√©todo: {fusion_method}, Reranked: {result.get_metadata('reranked', False)}")
        else:
            print("   ‚ùå No se encontraron resultados")
        
        results_summary.append({
            "query": query,
            "results_count": len(results),
            "top_score": results[0].score if results else 0.0,
            "results": results
        })
    
    # 8. Evaluaci√≥n de relevancia
    print("\n8. Evaluando relevancia de resultados...")
    
    # Crear evaluador de relevancia
    relevance_evaluator = RelevanceEvaluator(
        use_semantic_similarity=True,
        use_keyword_overlap=True,
        relevance_threshold=0.3
    )
    
    evaluation_results = []
    
    for query_result in results_summary:
        query = query_result["query"]
        results = query_result["results"]
        
        if results:
            # Simular respuesta generada (en implementaci√≥n real vendr√≠a del LLM)
            simulated_response = f"Bas√°ndome en la informaci√≥n recuperada: {results[0].content[:200]}..."
            
            # Evaluar relevancia
            eval_result = relevance_evaluator.evaluate(
                query=query,
                response=simulated_response,
                retrieved_chunks=results
            )
            
            evaluation_results.append(eval_result)
            
            print(f"   üìà Relevancia para '{query[:50]}...': {eval_result.metrics.relevance:.3f}")
    
    # 9. An√°lisis de rendimiento
    print("\n9. An√°lisis de rendimiento del sistema...")
    
    # Estad√≠sticas del retriever h√≠brido
    hybrid_stats = hybrid_retriever.get_stats()
    
    print("   üìä Estad√≠sticas del sistema:")
    print(f"      - Chunks indexados: {hybrid_stats['indexed_chunks']}")
    print(f"      - B√∫squedas realizadas: {hybrid_stats['search_performance']['total_searches']}")
    print(f"      - Tiempo promedio total: {hybrid_stats['search_performance']['avg_total_time']:.1f}ms")
    print(f"      - Tiempo sem√°ntico: {hybrid_stats['search_performance']['avg_semantic_time']:.1f}ms")
    print(f"      - Tiempo keywords: {hybrid_stats['search_performance']['avg_keyword_time']:.1f}ms")
    
    # Estad√≠sticas de evaluaci√≥n
    if evaluation_results:
        avg_relevance = sum(r.metrics.relevance for r in evaluation_results) / len(evaluation_results)
        print(f"      - Relevancia promedio: {avg_relevance:.3f}")
        
        passing_evals = sum(1 for r in evaluation_results if r.metrics.relevance >= 0.7)
        pass_rate = passing_evals / len(evaluation_results)
        print(f"      - Tasa de √©xito (>0.7): {pass_rate:.1%}")
    
    # 10. Recomendaciones
    print("\n10. Recomendaciones del sistema...")
    
    recommendations = []
    
    if hybrid_stats['search_performance']['avg_total_time'] > 2000:  # > 2 segundos
        recommendations.append("üêå Latencia alta detectada. Considerar optimizar embeddings o usar cach√©")
    
    if evaluation_results and avg_relevance < 0.7:
        recommendations.append("üìâ Relevancia baja. Considerar ajustar estrategia de chunking o umbral de similitud")
    
    if not recommendations:
        recommendations.append("‚úÖ Sistema funcionando √≥ptimamente seg√∫n mejores pr√°cticas")
    
    for rec in recommendations:
        print(f"   {rec}")
    
    print("\n" + "=" * 50)
    print("üéâ Demostraci√≥n completada exitosamente!")
    print("\nEl sistema WorldClass RAG est√° listo para:")
    print("- ‚úÖ Procesar documentos con chunking inteligente")
    print("- ‚úÖ Realizar b√∫squeda h√≠brida (sem√°ntica + keywords)")
    print("- ‚úÖ Aplicar re-ranking para mayor precisi√≥n")
    print("- ‚úÖ Evaluar rendimiento con m√©tricas clave")
    print("- ‚úÖ Escalar a producci√≥n empresarial")


if __name__ == "__main__":
    main()