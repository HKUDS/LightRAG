#!/usr/bin/env python3
"""
Ejemplo bÃ¡sico de uso del mÃ³dulo WorldClass RAG.

Demuestra:
1. InicializaciÃ³n del motor RAG
2. Procesamiento de documentos
3. Chunking inteligente
4. BÃºsqueda hÃ­brida
5. EvaluaciÃ³n de rendimiento

Basado en las mejores prÃ¡cticas del video AI News & Strategy Daily.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from worldclass_rag import (
    RAGEngine,
    SemanticChunker,
    RecursiveChunker,
    SentenceChunker,
    HybridRetriever,
    OpenAIEmbeddings,
    SentenceTransformerEmbeddings,
    RAGEvaluator
)

from worldclass_rag.processors import TextProcessor, PDFProcessor
from worldclass_rag.core.retrieval import SemanticRetriever, KeywordRetriever
from worldclass_rag.core.evaluation import RelevanceEvaluator


def main():
    """DemostraciÃ³n completa del sistema WorldClass RAG."""
    
    print("ğŸŒŸ WorldClass RAG - DemostraciÃ³n BÃ¡sica")
    print("=" * 50)
    
    # 1. Configurar modelos de embeddings
    print("\n1. Configurando modelos de embeddings...")
    
    try:
        # Intentar usar OpenAI (requiere API key)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        if not embeddings.is_available():
            raise Exception("OpenAI no disponible")
        print("âœ… Usando OpenAI embeddings")
    except:
        # Fallback a modelo local
        embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
        print("âœ… Usando Sentence Transformers embeddings (local)")
    
    # 2. Crear estrategias de chunking
    print("\n2. Configurando estrategias de chunking...")
    
    # Estrategia semÃ¡ntica (agrupa por significado)
    semantic_chunker = SemanticChunker(
        chunk_size=800,
        chunk_overlap=150,
        similarity_threshold=0.5
    )
    
    # Estrategia recursiva (respeta estructura)
    recursive_chunker = RecursiveChunker(
        chunk_size=1000,
        chunk_overlap=200,
        preserve_sentence_boundaries=True
    )
    
    # Estrategia por oraciones (nunca rompe oraciones)
    sentence_chunker = SentenceChunker(
        chunk_size=900,
        chunk_overlap=180,
        language="es"
    )
    
    print("âœ… Configuradas 3 estrategias de chunking")
    
    # 3. Crear retrievers
    print("\n3. Configurando sistema de retrieval hÃ­brido...")
    
    semantic_retriever = SemanticRetriever(
        embedding_model=embeddings,
        similarity_threshold=0.3
    )
    
    keyword_retriever = KeywordRetriever(
        scoring_method="bm25",
        language="es"
    )
    
    # ConfiguraciÃ³n hÃ­brida optimizada
    from worldclass_rag.core.retrieval.hybrid_retriever import HybridSearchConfig
    
    hybrid_config = HybridSearchConfig(
        semantic_weight=0.7,    # Priorizar bÃºsqueda semÃ¡ntica
        keyword_weight=0.3,     # Complementar con keywords
        fusion_method="rrf",    # Reciprocal Rank Fusion
        rerank=True,            # Aplicar re-ranking
        rerank_top_k=20,
        final_top_k=5
    )
    
    hybrid_retriever = HybridRetriever(
        semantic_retriever=semantic_retriever,
        keyword_retriever=keyword_retriever,
        config=hybrid_config
    )
    
    print("âœ… Sistema hÃ­brido configurado (semÃ¡ntica + keywords + re-ranking)")
    
    # 4. Crear documentos de ejemplo
    print("\n4. Preparando documentos de ejemplo...")
    
    sample_documents = [
        {
            "content": """
            La RecuperaciÃ³n Aumentada por GeneraciÃ³n (RAG) es una tÃ©cnica revolucionaria en IA 
            que combina la potencia de los Large Language Models con bases de conocimiento externas. 
            RAG permite que los modelos accedan a informaciÃ³n actualizada y especÃ­fica del dominio, 
            eliminando las limitaciones de fechas de corte de conocimiento.
            
            El proceso RAG consta de tres fases principales:
            1. RecuperaciÃ³n: Buscar informaciÃ³n relevante en la base de conocimiento
            2. Aumento: Combinar la consulta con los datos recuperados  
            3. GeneraciÃ³n: Producir una respuesta fundamentada en evidencia real
            """,
            "metadata": {
                "source": "manual_rag.pdf",
                "section": "IntroducciÃ³n",
                "author": "WorldClass AI Team",
                "topic": "RAG Fundamentals"
            }
        },
        {
            "content": """
            Los embeddings son representaciones vectoriales de texto en espacios de alta dimensiÃ³n,
            tÃ­picamente 1,536 dimensiones segÃºn las mejores prÃ¡cticas actuales. Estos vectores
            capturan el significado semÃ¡ntico del texto, permitiendo que significados similares
            se agrupen matemÃ¡ticamente mediante similitud coseno.
            
            La elecciÃ³n del modelo de embeddings es crÃ­tica para el Ã©xito de RAG:
            - OpenAI text-embedding-3-large: 3,072 dimensiones, mÃ¡xima calidad
            - OpenAI text-embedding-3-small: 1,536 dimensiones, balance calidad-costo  
            - Sentence Transformers: Modelos locales, privacidad garantizada
            """,
            "metadata": {
                "source": "manual_embeddings.pdf", 
                "section": "Embeddings",
                "author": "WorldClass AI Team",
                "topic": "Vector Representations"
            }
        },
        {
            "content": """
            El chunking o segmentaciÃ³n de documentos es un arte y ciencia crÃ­ticos en RAG.
            Una mala segmentaciÃ³n puede arruinar completamente un proyecto RAG.
            
            Estrategias de chunking recomendadas:
            - Chunking semÃ¡ntico: Agrupa por significado, preserva coherencia temÃ¡tica
            - Chunking recursivo: Respeta jerarquÃ­as estructurales del documento
            - Chunking por oraciones: Nunca rompe oraciones, preserva gramÃ¡tica
            
            Principio fundamental: SIEMPRE incluir superposiciÃ³n entre chunks para
            preservar contexto y evitar pÃ©rdida de informaciÃ³n crÃ­tica.
            """,
            "metadata": {
                "source": "manual_chunking.pdf",
                "section": "Chunking Strategies", 
                "author": "WorldClass AI Team",
                "topic": "Document Processing"
            }
        },
        {
            "content": """
            La bÃºsqueda hÃ­brida representa el nivel 2 de RAG segÃºn las mejores prÃ¡cticas.
            Combina bÃºsqueda semÃ¡ntica (por significado) con bÃºsqueda por palabras clave,
            ofreciendo mayor precisiÃ³n y potencial de velocidad mejorada.
            
            Componentes de bÃºsqueda hÃ­brida:
            - BÃºsqueda vectorial: Encuentra documentos semÃ¡nticamente relacionados
            - BÃºsqueda BM25: Captura coincidencias exactas de tÃ©rminos importantes  
            - Re-ranking: Mejora significativamente la precisiÃ³n para propÃ³sitos comerciales
            - FusiÃ³n de resultados: Combina ambas estrategias de forma inteligente
            """,
            "metadata": {
                "source": "manual_hybrid.pdf",
                "section": "Hybrid Search",
                "author": "WorldClass AI Team", 
                "topic": "Advanced Retrieval"
            }
        },
        {
            "content": """
            Las mÃ©tricas de evaluaciÃ³n de RAG son fundamentales para el Ã©xito en producciÃ³n.
            SegÃºn AI News & Strategy Daily, existen 4 mÃ©tricas clave:
            
            1. Relevancia: Â¿Se recuperan los chunks correctos?
            2. Fidelidad: Â¿La respuesta se basa en fuentes reales?
            3. Calidad: Â¿Un humano la calificarÃ­a como correcta?
            4. Latencia: Â¿Es suficientemente rÃ¡pido (menos de 2 segundos)?
            
            Un sistema RAG debe ser evaluado continuamente usando estas mÃ©tricas
            para identificar problemas y oportunidades de mejora sistemÃ¡tica.
            """,
            "metadata": {
                "source": "manual_evaluation.pdf",
                "section": "RAG Metrics",
                "author": "WorldClass AI Team",
                "topic": "Evaluation Framework"
            }
        }
    ]
    
    print(f"âœ… Preparados {len(sample_documents)} documentos de ejemplo")
    
    # 5. Procesar documentos con diferentes estrategias de chunking
    print("\n5. Procesando documentos con chunking inteligente...")
    
    all_chunks = []
    
    for i, doc in enumerate(sample_documents):
        print(f"   Procesando documento {i+1}/5...")
        
        # Usar diferentes estrategias segÃºn el contenido
        if "RAG" in doc["content"]:
            chunker = semantic_chunker  # Usar chunking semÃ¡ntico para contenido conceptual
        elif "embeddings" in doc["content"].lower():
            chunker = recursive_chunker  # Usar chunking recursivo para contenido tÃ©cnico
        else:
            chunker = sentence_chunker  # Usar chunking por oraciones por defecto
        
        # Procesar documento
        chunks = chunker.chunk_document(
            text=doc["content"],
            source=doc["metadata"]["source"],
            section=doc["metadata"]["section"],
            additional_metadata=doc["metadata"]
        )
        
        # AÃ±adir identificadores Ãºnicos
        for j, chunk in enumerate(chunks):
            chunk.chunk_id = f"doc_{i}_chunk_{j}"
            chunk.add_metadata("document_index", i)
        
        all_chunks.extend(chunks)
    
    print(f"âœ… Generados {len(all_chunks)} chunks usando estrategias inteligentes")
    
    # 6. Indexar chunks en el sistema hÃ­brido
    print("\n6. Indexando chunks en sistema hÃ­brido...")
    
    # Preparar chunks para indexaciÃ³n
    chunks_for_indexing = []
    for chunk in all_chunks:
        chunk_data = {
            "content": chunk.content,
            "chunk_id": chunk.chunk_id,
            "source_id": chunk.get_metadata("source", "unknown"),
            "metadata": chunk.metadata,
            "source_file": chunk.get_metadata("source"),
            "source_section": chunk.get_metadata("section"),
        }
        chunks_for_indexing.append(chunk_data)
    
    # Indexar en retriever hÃ­brido
    hybrid_retriever.add_chunks(chunks_for_indexing)
    
    print("âœ… Chunks indexados en ambos sistemas (semÃ¡ntico + keywords)")
    
    # 7. Realizar consultas de prueba
    print("\n7. Realizando consultas de prueba...")
    
    test_queries = [
        "Â¿QuÃ© es RAG y cÃ³mo funciona?",
        "Â¿CuÃ¡les son las mejores prÃ¡cticas para chunking?", 
        "Â¿CÃ³mo funcionan los embeddings en RAG?",
        "Â¿QuÃ© mÃ©tricas usar para evaluar RAG?",
        "Â¿QuÃ© ventajas tiene la bÃºsqueda hÃ­brida?"
    ]
    
    results_summary = []
    
    for i, query in enumerate(test_queries):
        print(f"\n   Consulta {i+1}: {query}")
        
        # Realizar bÃºsqueda hÃ­brida
        results = hybrid_retriever.retrieve(query, top_k=3)
        
        # Mostrar resultados
        if results:
            print(f"   ğŸ“Š Encontrados {len(results)} resultados relevantes:")
            for j, result in enumerate(results):
                print(f"      {j+1}. Score: {result.score:.3f} | Fuente: {result.source_id}")
                print(f"         Chunk: {result.content[:100]}...")
                
                # InformaciÃ³n sobre el mÃ©todo de retrieval
                fusion_method = result.get_metadata("fusion_method", "unknown")
                print(f"         MÃ©todo: {fusion_method}, Reranked: {result.get_metadata('reranked', False)}")
        else:
            print("   âŒ No se encontraron resultados")
        
        results_summary.append({
            "query": query,
            "results_count": len(results),
            "top_score": results[0].score if results else 0.0,
            "results": results
        })
    
    # 8. EvaluaciÃ³n de relevancia
    print("\n8. Evaluando relevancia de resultados...")
    
    # Crear evaluador de relevancia
    relevance_evaluator = RelevanceEvaluator(
        use_semantic_similarity=True,
        use_keyword_overlap=True,
        relevance_threshold=0.3
    )
    
    evaluation_results = []
    
    for query_result in results_summary:
        query = query_result["query"]
        results = query_result["results"]
        
        if results:
            # Simular respuesta generada (en implementaciÃ³n real vendrÃ­a del LLM)
            simulated_response = f"BasÃ¡ndome en la informaciÃ³n recuperada: {results[0].content[:200]}..."
            
            # Evaluar relevancia
            eval_result = relevance_evaluator.evaluate(
                query=query,
                response=simulated_response,
                retrieved_chunks=results
            )
            
            evaluation_results.append(eval_result)
            
            print(f"   ğŸ“ˆ Relevancia para '{query[:50]}...': {eval_result.metrics.relevance:.3f}")
    
    # 9. AnÃ¡lisis de rendimiento
    print("\n9. AnÃ¡lisis de rendimiento del sistema...")
    
    # EstadÃ­sticas del retriever hÃ­brido
    hybrid_stats = hybrid_retriever.get_stats()
    
    print("   ğŸ“Š EstadÃ­sticas del sistema:")
    print(f"      - Chunks indexados: {hybrid_stats['indexed_chunks']}")
    print(f"      - BÃºsquedas realizadas: {hybrid_stats['search_performance']['total_searches']}")
    print(f"      - Tiempo promedio total: {hybrid_stats['search_performance']['avg_total_time']:.1f}ms")
    print(f"      - Tiempo semÃ¡ntico: {hybrid_stats['search_performance']['avg_semantic_time']:.1f}ms")
    print(f"      - Tiempo keywords: {hybrid_stats['search_performance']['avg_keyword_time']:.1f}ms")
    
    # EstadÃ­sticas de evaluaciÃ³n
    if evaluation_results:
        avg_relevance = sum(r.metrics.relevance for r in evaluation_results) / len(evaluation_results)
        print(f"      - Relevancia promedio: {avg_relevance:.3f}")
        
        passing_evals = sum(1 for r in evaluation_results if r.metrics.relevance >= 0.7)
        pass_rate = passing_evals / len(evaluation_results)
        print(f"      - Tasa de Ã©xito (>0.7): {pass_rate:.1%}")
    
    # 10. Recomendaciones
    print("\n10. Recomendaciones del sistema...")
    
    recommendations = []
    
    if hybrid_stats['search_performance']['avg_total_time'] > 2000:  # > 2 segundos
        recommendations.append("ğŸŒ Latencia alta detectada. Considerar optimizar embeddings o usar cachÃ©")
    
    if evaluation_results and avg_relevance < 0.7:
        recommendations.append("ğŸ“‰ Relevancia baja. Considerar ajustar estrategia de chunking o umbral de similitud")
    
    if not recommendations:
        recommendations.append("âœ… Sistema funcionando Ã³ptimamente segÃºn mejores prÃ¡cticas")
    
    for rec in recommendations:
        print(f"   {rec}")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ DemostraciÃ³n completada exitosamente!")
    print("\nEl sistema WorldClass RAG estÃ¡ listo para:")
    print("- âœ… Procesar documentos con chunking inteligente")
    print("- âœ… Realizar bÃºsqueda hÃ­brida (semÃ¡ntica + keywords)")
    print("- âœ… Aplicar re-ranking para mayor precisiÃ³n")
    print("- âœ… Evaluar rendimiento con mÃ©tricas clave")
    print("- âœ… Escalar a producciÃ³n empresarial")


if __name__ == "__main__":
    main()