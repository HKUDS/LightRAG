# å¤šè¯­è¨€å®ä½“æå–å·¥å…·å…¨é¢å¯¹æ¯”ï¼ˆè‹±æ–‡åŠå…¶ä»–è¯­ç§ï¼‰

## å¿«é€Ÿå›ç­”

**è‹±æ–‡å’Œå…¶ä»–è¯­ç§çš„æ¨è**ï¼š

| è¯­è¨€ | æœ€ä½³é€‰æ‹© | æ¬¡é€‰ | å¤‡é€‰ |
|------|---------|------|------|
| **è‹±æ–‡** | spaCyï¼ˆé€Ÿåº¦+è´¨é‡å¹³è¡¡ï¼‰ | StanfordNLPï¼ˆæœ€é«˜è´¨é‡ï¼‰ | GLiNERï¼ˆé›¶æ ·æœ¬çµæ´»ï¼‰ |
| **ä¸­æ–‡** | HanLPï¼ˆä¸“é—¨ä¼˜åŒ–ï¼‰ | - | GLiNERï¼ˆå·®è·å¤§ï¼‰ |
| **æ³•/å¾·/è¥¿** | GLiNERï¼ˆé›¶æ ·æœ¬ï¼‰ | spaCyï¼ˆé¢„è®­ç»ƒæ¨¡å‹ï¼‰ | mBERTï¼ˆå¤šè¯­è¨€ï¼‰ |
| **æ—¥/éŸ©** | spaCyï¼ˆæœ‰æ”¯æŒï¼‰ | HanLPï¼ˆæ—¥è¯­ä¼˜ç§€ï¼‰ | GLiNERï¼ˆé›¶æ ·æœ¬ï¼‰ |
| **å¤šè¯­è¨€æ··åˆ** | **GLiNER** â­â­â­â­â­ | mBERT | å•†ä¸š API |
| **ä»»æ„è¯­è¨€** | **GLiNER** â­â­â­â­â­ | XLM-RoBERTa | LLM |

**æ ¸å¿ƒç»“è®º**ï¼š
- âœ… **è‹±æ–‡**ï¼šspaCy æ˜¯æœ€ä½³å¹³è¡¡é€‰æ‹©ï¼ˆè´¨é‡ 90%+ï¼Œé€Ÿåº¦æå¿«ï¼‰
- âœ… **å¤šè¯­è¨€/æ··åˆåœºæ™¯**ï¼šGLiNER æ˜¯ç‹è€…ï¼ˆé›¶æ ·æœ¬ï¼Œ40+ è¯­è¨€ï¼‰
- âœ… **ä¸­æ–‡**ï¼šHanLP æ— å¯æ›¿ä»£ï¼ˆè´¨é‡å·®è·å¤ªå¤§ï¼‰
- âœ… **éœ€è¦æè‡´è´¨é‡ï¼ˆè‹±æ–‡ï¼‰**ï¼šStanfordNLP
- âœ… **éœ€è¦è‡ªå®šä¹‰å®ä½“**ï¼šGLiNERï¼ˆä»»ä½•è¯­è¨€ï¼‰

---

## è¯¦ç»†å¯¹æ¯”

### 1. è‹±æ–‡ NER å·¥å…·å¯¹æ¯”

#### spaCyï¼ˆæ¨èä½œä¸ºé»˜è®¤é€‰æ‹©ï¼‰

**åŸºæœ¬ä¿¡æ¯**ï¼š
```
GitHub: https://github.com/explosion/spaCy
Stars: 30k+
ç»„ç»‡: Explosion AI
è¯­è¨€æ”¯æŒ: 70+ è¯­è¨€ï¼ˆè‹±æ–‡æœ€æˆç†Ÿï¼‰
è®¸å¯è¯: MIT
```

**æ€§èƒ½**ï¼ˆCoNLL 2003 è‹±æ–‡åŸºå‡†ï¼‰ï¼š
```
æ¨¡å‹: en_core_web_trf (Transformer)
Precision: 90.2%
Recall:    89.8%
F1:        90.0%

é€Ÿåº¦: 1000+ å¥/ç§’ (GPU)
      100-200 å¥/ç§’ (CPU)
æ¨¡å‹å¤§å°: ~440MB (Transformer)
          ~50MB (CNN)
```

**ä¼˜ç‚¹**ï¼š
```
âœ… é€Ÿåº¦æå¿«ï¼ˆå·¥ä¸šç•Œæœ€å¿«ä¹‹ä¸€ï¼‰
âœ… è´¨é‡é«˜ï¼ˆF1 ~90%ï¼‰
âœ… æ–‡æ¡£å®Œå–„ï¼Œç¤¾åŒºæ´»è·ƒ
âœ… æ˜“äºé›†æˆï¼ˆpip install spacyï¼‰
âœ… å¤šç§æ¨¡å‹ï¼ˆå°/ä¸­/å¤§/Transformerï¼‰
âœ… æ”¯æŒ 70+ è¯­è¨€
âœ… å†…ç½® pipelineï¼ˆåˆ†è¯+è¯æ€§+NER+ä¾å­˜ï¼‰
âœ… å¯è§†åŒ–å·¥å…·ï¼ˆdisplaCyï¼‰
```

**ç¼ºç‚¹**ï¼š
```
âŒ å®ä½“ç±»å‹å›ºå®šï¼ˆéœ€è¦é‡æ–°è®­ç»ƒï¼‰
âŒ è‡ªå®šä¹‰å®ä½“éœ€è¦æ ‡æ³¨æ•°æ®
âŒ éè‹±è¯­è¯­è¨€è´¨é‡å‚å·®ä¸é½
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
import spacy

# åŠ è½½è‹±æ–‡æ¨¡å‹
nlp = spacy.load("en_core_web_trf")

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
doc = nlp(text)

# æå–å®ä½“
for ent in doc.ents:
    print(f"{ent.text}: {ent.label_}")

# è¾“å‡º:
# Apple Inc.: ORG
# Steve Jobs: PERSON
# Cupertino: GPE
# California: GPE
```

**æ”¯æŒçš„å®ä½“ç±»å‹**ï¼ˆOntoNotesï¼‰ï¼š
- PERSON, ORG, GPE, LOC, DATE, TIME, MONEY, PERCENT
- PRODUCT, EVENT, FAC, LANGUAGE, LAW, NORP, ORDINAL, QUANTITY, WORK_OF_ART

---

#### StanfordNLP / CoreNLPï¼ˆæœ€é«˜è´¨é‡ï¼‰

**åŸºæœ¬ä¿¡æ¯**ï¼š
```
GitHub: https://github.com/stanfordnlp/CoreNLP
Stars: 10k+
ç»„ç»‡: Stanford NLP Group
è¯­è¨€æ”¯æŒ: 8 ç§ï¼ˆè‹±æ–‡æœ€å¼ºï¼‰
è®¸å¯è¯: GPL v3
```

**æ€§èƒ½**ï¼ˆCoNLL 2003ï¼‰ï¼š
```
F1: 92.3%  â† æ¯” spaCy é«˜ 2-3%

é€Ÿåº¦: 50-100 å¥/ç§’ (CPU)
     â† æ¯” spaCy æ…¢ 2-5 å€
æ¨¡å‹å¤§å°: ~500MB
ä¾èµ–: Java Runtime
```

**ä¼˜ç‚¹**ï¼š
```
âœ… è´¨é‡æœ€é«˜ï¼ˆè‹±æ–‡ F1 92%+ï¼‰
âœ… å­¦æœ¯ç•Œæ ‡å‡†å·¥å…·
âœ… ä¾å­˜å¥æ³•åˆ†æä¼˜ç§€
âœ… å…±æŒ‡æ¶ˆè§£èƒ½åŠ›å¼º
âœ… æ”¯æŒå…³ç³»æå–
```

**ç¼ºç‚¹**ï¼š
```
âŒ é€Ÿåº¦æ…¢ï¼ˆæ¯” spaCy æ…¢ 2-5xï¼‰
âŒ éœ€è¦ Javaï¼ˆéƒ¨ç½²å¤æ‚ï¼‰
âŒ å†…å­˜å ç”¨å¤§
âŒ API ä¸å¤Ÿç°ä»£ï¼ˆç›¸æ¯” spaCyï¼‰
âŒ è¯­è¨€æ”¯æŒæœ‰é™ï¼ˆ8 ç§ï¼‰
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- å­¦æœ¯ç ”ç©¶ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰
- æ³•å¾‹/åŒ»ç–—æ–‡æœ¬ï¼ˆéœ€è¦æé«˜å‡†ç¡®ç‡ï¼‰
- å°è§„æ¨¡æ•°æ®å¤„ç†ï¼ˆé€Ÿåº¦ä¸æ•æ„Ÿï¼‰

---

#### GLiNERï¼ˆé›¶æ ·æœ¬ï¼Œçµæ´»æ€§æœ€å¼ºï¼‰

**åŸºæœ¬ä¿¡æ¯**ï¼š
```
GitHub: https://github.com/urchade/GLiNER
Stars: 3k+
å‘å¸ƒ: NAACL 2024
è¯­è¨€æ”¯æŒ: 40+ è¯­è¨€ï¼ˆé›¶æ ·æœ¬ï¼‰
è®¸å¯è¯: Apache 2.0
```

**æ€§èƒ½**ï¼ˆè‹±æ–‡ - CoNLL++ ç­‰æ•°æ®é›†ï¼‰ï¼š
```
Zero-shot F1: 60.5  â† é›¶æ ·æœ¬åœºæ™¯
Fine-tuned F1: 92.0  â† å¾®è°ƒåæ¥è¿‘ StanfordNLP

é€Ÿåº¦: 500-2000 å¥/ç§’ (GPU)
      300-500 å¥/ç§’ (CPU)
      â† æ¯” spaCy ç¨å¿«

æ¨¡å‹å¤§å°: 280MB
```

**ä¼˜ç‚¹**ï¼š
```
âœ… é›¶æ ·æœ¬å­¦ä¹ ï¼ˆä»»æ„å®ä½“ç±»å‹ï¼‰
âœ… æ— éœ€è®­ç»ƒå³å¯ä½¿ç”¨
âœ… çµæ´»æ€§æé«˜
âœ… é€Ÿåº¦å¿«
âœ… æ”¯æŒ 40+ è¯­è¨€
âœ… æ¨¡å‹è½»é‡ï¼ˆ280MBï¼‰
âœ… åœ¨è‹±æ–‡ä¸Šè¡¨ç°ä¼˜ç§€
âœ… è¶…è¶Š ChatGPTï¼ˆé›¶æ ·æœ¬åœºæ™¯ï¼‰
```

**ç¼ºç‚¹**ï¼š
```
âŒ é›¶æ ·æœ¬æ€§èƒ½ä¸å¦‚ç›‘ç£å­¦ä¹ 
âŒ ä¾èµ–å®ä½“ç±»å‹æè¿°è´¨é‡
âŒ éè‹±è¯­è¯­è¨€æ€§èƒ½ä¸‹é™
âŒ éœ€è¦ä»”ç»†è°ƒæ•´å®ä½“ç±»å‹å®šä¹‰
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from gliner import GLiNER

# åŠ è½½æ¨¡å‹
model = GLiNER.from_pretrained("urchade/gliner_large-v2.1")

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."

# åŠ¨æ€æŒ‡å®šä»»æ„å®ä½“ç±»å‹
labels = ["company", "person", "city", "state", "founder", "tech company"]

entities = model.predict_entities(text, labels)

# è¾“å‡º:
# [
#   {'text': 'Apple Inc.', 'label': 'tech company', 'score': 0.95},
#   {'text': 'Steve Jobs', 'label': 'founder', 'score': 0.92},
#   {'text': 'Cupertino', 'label': 'city', 'score': 0.88},
#   {'text': 'California', 'label': 'state', 'score': 0.90}
# ]
```

---

#### è‹±æ–‡å·¥å…·ç»¼åˆå¯¹æ¯”

| å·¥å…· | F1ï¼ˆç›‘ç£ï¼‰ | F1ï¼ˆé›¶æ ·æœ¬ï¼‰ | é€Ÿåº¦ | çµæ´»æ€§ | æ˜“ç”¨æ€§ | æ¨èåœºæ™¯ |
|------|-----------|-------------|------|-------|--------|---------|
| **spaCy** | 90% | N/A | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | **é€šç”¨é¦–é€‰** |
| **StanfordNLP** | **92%** | N/A | â­â­â­ | â­â­ | â­â­â­ | å­¦æœ¯/é«˜è´¨é‡ |
| **GLiNER** | 92% | **60%** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | é›¶æ ·æœ¬/è‡ªå®šä¹‰ |
| **Flair** | 93% | N/A | â­â­ | â­â­â­â­ | â­â­â­â­ | ç ”ç©¶/fine-tuning |
| **LLM (GPT-4)** | N/A | 55-65% | â­â­ | â­â­â­â­â­ | â­â­â­â­ | åŸå‹/å¤æ‚å®ä½“ |

**æ¨èå†³ç­–æ ‘**ï¼š

```
è‹±æ–‡ NER é€‰æ‹©:

éœ€è¦è‡ªå®šä¹‰å®ä½“ç±»å‹ï¼Ÿ
â”œâ”€ æ˜¯ â†’ GLiNERï¼ˆé›¶æ ·æœ¬ï¼‰
â””â”€ å¦ â†’ ç»§ç»­

è¿½æ±‚æè‡´è´¨é‡ï¼ˆF1 > 91%ï¼‰ï¼Ÿ
â”œâ”€ æ˜¯ â†’ StanfordNLP
â””â”€ å¦ â†’ ç»§ç»­

éœ€è¦é«˜ååé‡ï¼ˆ>100 å¥/ç§’ï¼‰ï¼Ÿ
â”œâ”€ æ˜¯ â†’ spaCyï¼ˆTransformer æˆ– CNNï¼‰
â””â”€ å¦ â†’ spaCyï¼ˆé»˜è®¤æ¨èï¼‰

ç»“è®º: 90% åœºæ™¯ç”¨ spaCy å°±å¤Ÿäº†
```

---

### 2. å¤šè¯­è¨€åœºæ™¯å¯¹æ¯”

#### GLiNER-Multiï¼ˆå¤šè¯­è¨€ä¹‹ç‹ï¼‰

**æ”¯æŒè¯­è¨€**ï¼ˆå®˜æ–¹æµ‹è¯•ï¼‰ï¼š
```
æ‹‰ä¸æ–‡å­—ï¼ˆæ€§èƒ½ä¼˜ç§€ï¼‰:
âœ… English, Spanish, German, French, Italian, Portuguese
âœ… Dutch, Swedish, Norwegian, Danish
âœ… Polish, Czech, Romanian

éæ‹‰ä¸æ–‡å­—ï¼ˆæ€§èƒ½ä¸­ç­‰ï¼‰:
â­ Chinese, Japanese, Korean
â­ Arabic, Hebrew, Russian, Greek

å…¶ä»–ï¼ˆé›¶æ ·æœ¬æ”¯æŒï¼‰:
ğŸŒ 40+ è¯­è¨€ï¼ˆåŒ…æ‹¬ä½èµ„æºè¯­è¨€ï¼‰
```

**æ€§èƒ½**ï¼ˆMultiCoNER æ•°æ®é›†ï¼‰ï¼š

| è¯­è¨€ | GLiNER-Multi F1 | ChatGPT F1 | å¯¹æ¯” |
|------|----------------|-----------|------|
| **English** | 60.5 | 55.2 | âœ… GLiNER èƒœ |
| **Spanish** | 50.2 | 45.8 | âœ… GLiNER èƒœ |
| **German** | 48.9 | 44.3 | âœ… GLiNER èƒœ |
| **French** | 47.3 | 43.1 | âœ… GLiNER èƒœ |
| **Dutch** | 52.1 | 48.7 | âœ… GLiNER èƒœ |
| **Russian** | 38.4 | 36.2 | âœ… GLiNER èƒœ |
| **Chinese** | **24.3** | 28.1 | âŒ ChatGPT èƒœ |
| **Japanese** | 31.2 | 29.8 | âœ… GLiNER èƒœ |
| **Korean** | 28.7 | 27.4 | âœ… GLiNER èƒœ |

**å…³é”®å‘ç°**ï¼š
- âœ… æ¬§æ´²è¯­è¨€ï¼ˆæ‹‰ä¸æ–‡å­—ï¼‰ï¼šGLiNER ä¼˜ç§€ï¼ˆF1 45-60%ï¼‰
- âš ï¸ ä¸œäºšè¯­è¨€ï¼ˆä¸­æ—¥éŸ©ï¼‰ï¼šGLiNER ä¸­ç­‰ï¼ˆF1 25-35%ï¼‰
- âœ… æ‰€æœ‰è¯­è¨€éƒ½è¶…è¿‡ ChatGPTï¼ˆé™¤ä¸­æ–‡å¤–ï¼‰
- âš ï¸ é›¶æ ·æœ¬æ€§èƒ½ä¸å¦‚ç›‘ç£å­¦ä¹ ï¼ˆä½†éå¸¸çµæ´»ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from gliner import GLiNER

# åŠ è½½å¤šè¯­è¨€æ¨¡å‹
model = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")

# æ³•è¯­æ–‡æœ¬
text_fr = "Emmanuel Macron est le prÃ©sident de la France depuis 2017."
labels_fr = ["personne", "pays", "date", "poste"]
entities_fr = model.predict_entities(text_fr, labels_fr)

# å¾·è¯­æ–‡æœ¬
text_de = "Angela Merkel war Bundeskanzlerin von Deutschland."
labels_de = ["Person", "Land", "Position"]
entities_de = model.predict_entities(text_de, labels_de)

# è¥¿ç­ç‰™è¯­æ–‡æœ¬
text_es = "Madrid es la capital de EspaÃ±a desde 1561."
labels_es = ["ciudad", "paÃ­s", "fecha"]
entities_es = model.predict_entities(text_es, labels_es)

# æ—¥è¯­æ–‡æœ¬
text_ja = "æ±äº¬ã¯æ—¥æœ¬ã®é¦–éƒ½ã§ã™ã€‚"
labels_ja = ["éƒ½å¸‚", "å›½"]
entities_ja = model.predict_entities(text_ja, labels_ja)
```

---

#### spaCy å¤šè¯­è¨€æ”¯æŒ

**æ”¯æŒè¯­è¨€**ï¼š70+ è¯­è¨€ï¼Œä½†è´¨é‡å‚å·®ä¸é½

**é«˜è´¨é‡æ”¯æŒ**ï¼ˆF1 > 85%ï¼‰ï¼š
```
âœ… English (90%)
âœ… German (88%)
âœ… Spanish (87%)
âœ… French (86%)
âœ… Italian (85%)
âœ… Portuguese (85%)
âœ… Dutch (87%)
```

**ä¸­ç­‰è´¨é‡**ï¼ˆF1 60-85%ï¼‰ï¼š
```
â­ Chinese (60-70%)  â† ä»åœ¨æ”¹è¿›ä¸­
â­ Japanese (65-75%)
â­ Korean (60-70%)
â­ Russian (75-80%)
â­ Polish (78-82%)
```

**å¯ç”¨ä½†è´¨é‡è¾ƒä½**ï¼ˆF1 < 60%ï¼‰ï¼š
```
âš ï¸ Arabic, Hebrew, Hindi, Thai, Vietnamese...
```

**æ¨¡å‹ç¤ºä¾‹**ï¼š
```python
import spacy

# å¾·è¯­
nlp_de = spacy.load("de_core_news_lg")
# æ³•è¯­
nlp_fr = spacy.load("fr_core_news_lg")
# è¥¿ç­ç‰™è¯­
nlp_es = spacy.load("es_core_news_lg")
# æ—¥è¯­
nlp_ja = spacy.load("ja_core_news_lg")

# ä½¿ç”¨
doc = nlp_de("Angela Merkel ist die ehemalige Bundeskanzlerin.")
for ent in doc.ents:
    print(ent.text, ent.label_)
```

---

#### mBERT / XLM-RoBERTaï¼ˆå¤šè¯­è¨€ BERTï¼‰

**åŸºæœ¬ä¿¡æ¯**ï¼š
```
æ¨¡å‹: bert-base-multilingual-cased-ner-hrl
æ”¯æŒè¯­è¨€: 10 ç§é«˜èµ„æºè¯­è¨€
  - Arabic, German, English, Spanish, French
  - Italian, Latvian, Dutch, Portuguese, Chinese

æ¨¡å‹: XLM-RoBERTa
æ”¯æŒè¯­è¨€: 100+ è¯­è¨€
è®­ç»ƒ: 2.5TB å¤šè¯­è¨€æ–‡æœ¬
```

**æ€§èƒ½**ï¼ˆå¹³å‡ï¼‰ï¼š
```
é«˜èµ„æºè¯­è¨€ï¼ˆè‹±/å¾·/æ³•/è¥¿ï¼‰: F1 75-85%
ä¸­èµ„æºè¯­è¨€ï¼ˆæ„/è‘¡/è·/ä¸­ï¼‰: F1 65-75%
ä½èµ„æºè¯­è¨€: F1 50-65%

é€Ÿåº¦: 50-100 å¥/ç§’ (CPU)
      300-500 å¥/ç§’ (GPU)
```

**ä¼˜ç‚¹**ï¼š
```
âœ… æ”¯æŒ 100+ è¯­è¨€
âœ… è·¨è¯­è¨€è¿ç§»èƒ½åŠ›å¼º
âœ… å¯ä»¥ fine-tune åˆ°ç‰¹å®šé¢†åŸŸ
âœ… Hugging Face é›†æˆå¥½
```

**ç¼ºç‚¹**ï¼š
```
âŒ éœ€è¦ fine-tuningï¼ˆä¸æ˜¯å¼€ç®±å³ç”¨ï¼‰
âŒ è´¨é‡ä¸å¦‚å•è¯­è¨€æ¨¡å‹
âŒ æ¯” GLiNER çµæ´»æ€§å·®
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from transformers import pipeline

# åŠ è½½å¤šè¯­è¨€ NER æ¨¡å‹
ner = pipeline("ner", model="Davlan/bert-base-multilingual-cased-ner-hrl")

# æ”¯æŒå¤šç§è¯­è¨€
text_en = "Apple Inc. is located in Cupertino."
text_de = "Apple Inc. befindet sich in Cupertino."
text_es = "Apple Inc. estÃ¡ ubicada en Cupertino."

entities_en = ner(text_en)
entities_de = ner(text_de)
entities_es = ner(text_es)
```

---

### 3. æŒ‰è¯­è¨€æ—æ¨è

#### æ‹‰ä¸æ–‡å­—è¯­è¨€ï¼ˆæ³•/å¾·/è¥¿/æ„/è‘¡ç­‰ï¼‰

**æ¨èä¼˜å…ˆçº§**ï¼š

1. **GLiNER**ï¼ˆé›¶æ ·æœ¬ï¼Œçµæ´»æ€§å¼ºï¼‰â­â­â­â­â­
   ```
   ä¼˜åŠ¿:
   - é›¶æ ·æœ¬ F1 45-60%ï¼ˆå·²ç»å¾ˆä¸é”™ï¼‰
   - ä»»æ„è‡ªå®šä¹‰å®ä½“ç±»å‹
   - æ— éœ€è®­ç»ƒæ•°æ®
   - è·¨è¯­è¨€è¿ç§»èƒ½åŠ›å¼º

   é€‚åˆ:
   - å¤šè¯­è¨€æ··åˆæ–‡æœ¬
   - è‡ªå®šä¹‰å®ä½“ç±»å‹
   - å¿«é€ŸåŸå‹
   ```

2. **spaCy**ï¼ˆé¢„è®­ç»ƒæ¨¡å‹ï¼‰â­â­â­â­
   ```
   ä¼˜åŠ¿:
   - F1 85-90%ï¼ˆç›‘ç£å­¦ä¹ ï¼‰
   - é€Ÿåº¦å¿«
   - æ˜“äºé›†æˆ

   ç¼ºç‚¹:
   - å®ä½“ç±»å‹å›ºå®š
   - éœ€è¦ä¸ºæ¯ç§è¯­è¨€åŠ è½½æ¨¡å‹
   ```

3. **mBERT / XLM-RoBERTa**ï¼ˆéœ€è¦ fine-tuneï¼‰â­â­â­
   ```
   ä¼˜åŠ¿:
   - è·¨è¯­è¨€èƒ½åŠ›å¼º
   - å¯ä»¥åœ¨å°‘é‡æ•°æ®ä¸Š fine-tune

   ç¼ºç‚¹:
   - éœ€è¦æ ‡æ³¨æ•°æ®
   - éƒ¨ç½²å¤æ‚
   ```

**ç¤ºä¾‹ä»£ç **ï¼ˆå¤šè¯­è¨€ RAG ç³»ç»Ÿï¼‰ï¼š
```python
from gliner import GLiNER

class MultilingualEntityExtractor:
    """å¤šè¯­è¨€å®ä½“æå–å™¨"""

    def __init__(self):
        # ä½¿ç”¨ GLiNER å¤šè¯­è¨€æ¨¡å‹
        self.model = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")

        # å®šä¹‰å¤šè¯­è¨€å®ä½“ç±»å‹
        self.entity_types = {
            'en': ["person", "organization", "location", "date", "product"],
            'es': ["persona", "organizaciÃ³n", "ubicaciÃ³n", "fecha", "producto"],
            'de': ["Person", "Organisation", "Ort", "Datum", "Produkt"],
            'fr': ["personne", "organisation", "lieu", "date", "produit"],
            'it': ["persona", "organizzazione", "luogo", "data", "prodotto"],
        }

    def extract(self, text: str, language: str = 'en'):
        """æå–å®ä½“"""
        labels = self.entity_types.get(language, self.entity_types['en'])
        entities = self.model.predict_entities(text, labels)

        return [
            {'text': e['text'], 'type': e['label'], 'score': e['score']}
            for e in entities
            if e['score'] > 0.5
        ]

# ä½¿ç”¨
extractor = MultilingualEntityExtractor()

# æ³•è¯­æ–‡æœ¬
entities_fr = extractor.extract(
    "Apple a Ã©tÃ© fondÃ©e par Steve Jobs Ã  Cupertino.",
    language='fr'
)

# å¾·è¯­æ–‡æœ¬
entities_de = extractor.extract(
    "Apple wurde von Steve Jobs in Cupertino gegrÃ¼ndet.",
    language='de'
)
```

---

#### ä¸œäºšè¯­è¨€ï¼ˆä¸­/æ—¥/éŸ©ï¼‰

**æ¨èä¼˜å…ˆçº§**ï¼š

1. **ä¸“é—¨æ¨¡å‹**ï¼ˆæœ€é«˜è´¨é‡ï¼‰â­â­â­â­â­
   ```
   ä¸­æ–‡: HanLP (F1 95%)
   æ—¥æ–‡: HanLP (F1 90%+)
   éŸ©æ–‡: KoNLPy + mecab (F1 85-90%)
   ```

2. **spaCy**ï¼ˆé€šç”¨é€‰æ‹©ï¼‰â­â­â­â­
   ```
   ä¸­æ–‡: F1 60-70% (ä»åœ¨æ”¹è¿›)
   æ—¥æ–‡: F1 65-75%
   éŸ©æ–‡: F1 60-70%

   ä¼˜åŠ¿: é€Ÿåº¦å¿«ï¼Œæ˜“é›†æˆ
   ç¼ºç‚¹: è´¨é‡ä¸å¦‚ä¸“é—¨æ¨¡å‹
   ```

3. **GLiNER**ï¼ˆé›¶æ ·æœ¬ï¼Œçµæ´»ï¼‰â­â­â­
   ```
   ä¸­æ–‡: F1 ~24% (ä¸æ¨è)
   æ—¥æ–‡: F1 ~31%
   éŸ©æ–‡: F1 ~29%

   ä»…åœ¨éœ€è¦é›¶æ ·æœ¬è‡ªå®šä¹‰æ—¶è€ƒè™‘
   ```

**æ¨èç­–ç•¥**ï¼š
```python
def get_asian_extractor(language: str):
    """æ ¹æ®ä¸œäºšè¯­è¨€é€‰æ‹©æå–å™¨"""

    if language == 'zh':
        # ä¸­æ–‡: å¼ºçƒˆæ¨è HanLP
        import hanlp
        return hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)

    elif language == 'ja':
        # æ—¥æ–‡: HanLP æˆ– spaCy
        # HanLP æ—¥æ–‡ä¹Ÿå¾ˆå¼º
        import hanlp
        return hanlp.load('CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_JA')

    elif language == 'ko':
        # éŸ©æ–‡: spaCy æˆ– KoNLPy
        import spacy
        return spacy.load("ko_core_news_lg")

    else:
        # å…¶ä»–: ä½¿ç”¨ GLiNER
        from gliner import GLiNER
        return GLiNER.from_pretrained("urchade/gliner_multi-v2.1")
```

---

#### å…¶ä»–è¯­è¨€ï¼ˆé˜¿æ‹‰ä¼¯/ä¿„è¯­/å°åœ°è¯­ç­‰ï¼‰

**æ¨è**ï¼šGLiNER æˆ–å•†ä¸š API

**åŸå› **ï¼š
- å¼€æºæ¨¡å‹å¯¹è¿™äº›è¯­è¨€æ”¯æŒæœ‰é™
- GLiNER é›¶æ ·æœ¬èƒ½åŠ›åœ¨è¿™äº›è¯­è¨€ä¸Šä»ç„¶å¯ç”¨
- å•†ä¸š APIï¼ˆå¦‚ Google Cloud NLP, Azureï¼‰å¯¹è¿™äº›è¯­è¨€æ”¯æŒæ›´å¥½

**å•†ä¸š API å¯¹æ¯”**ï¼š

| æœåŠ¡ | è¯­è¨€æ”¯æŒ | ä»·æ ¼ | F1ï¼ˆä¼°ç®—ï¼‰ |
|------|---------|------|-----------|
| **Google Cloud NLP** | 100+ | $1-5/1k æ–‡æ¡£ | 75-85% |
| **Azure Text Analytics** | 50+ | $2-10/1k æ–‡æ¡£ | 70-80% |
| **AWS Comprehend** | 20+ | $1-3/1k æ–‡æ¡£ | 70-80% |
| **IBM Watson** | 30+ | $3-8/1k æ–‡æ¡£ | 75-85% |

---

### 4. æ··åˆè¯­è¨€æ–‡æœ¬å¤„ç†

#### åœºæ™¯ï¼šè‹±æ–‡ + ä¸­æ–‡æ··åˆæ–‡æ¡£

**é—®é¢˜**ï¼š
```
æ–‡æœ¬ç¤ºä¾‹:
"Apple Inc. åœ¨åº“æ¯”è’‚è¯º(Cupertino)æ€»éƒ¨å‘å¸ƒäº†æ–°æ¬¾ iPhoneï¼Œ
ç”± CEO Tim Cook ä¸»æŒå‘å¸ƒä¼šã€‚"

æŒ‘æˆ˜:
- è‹±æ–‡å®ä½“: Apple Inc., Cupertino, iPhone, Tim Cook
- ä¸­æ–‡å®ä½“: åº“æ¯”è’‚è¯º, CEO
- éœ€è¦åŒæ—¶å¤„ç†ä¸¤ç§è¯­è¨€
```

**æ–¹æ¡ˆ 1: è¯­è¨€æ£€æµ‹ + åˆ†åˆ«å¤„ç†**ï¼ˆæ¨èï¼‰

```python
import langdetect
from gliner import GLiNER
import hanlp
import spacy

class HybridLanguageExtractor:
    """æ··åˆè¯­è¨€å®ä½“æå–å™¨"""

    def __init__(self):
        self.hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)
        self.spacy_en = spacy.load("en_core_web_trf")
        self.gliner = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")

    def segment_by_language(self, text: str):
        """æŒ‰è¯­è¨€åˆ†å‰²æ–‡æœ¬"""
        # ç®€åŒ–ç‰ˆï¼šåŸºäºå­—ç¬¦åˆ†å‰²
        segments = []
        current_segment = ""
        current_lang = None

        for char in text:
            if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦
                lang = 'zh'
            elif 'a' <= char.lower() <= 'z':  # è‹±æ–‡å­—ç¬¦
                lang = 'en'
            else:
                lang = current_lang  # æ ‡ç‚¹ç­‰ä¿æŒå½“å‰è¯­è¨€

            if lang != current_lang and current_segment:
                segments.append((current_segment.strip(), current_lang))
                current_segment = char
                current_lang = lang
            else:
                current_segment += char
                current_lang = lang

        if current_segment:
            segments.append((current_segment.strip(), current_lang))

        return segments

    def extract(self, text: str):
        """æå–æ··åˆè¯­è¨€æ–‡æœ¬ä¸­çš„å®ä½“"""
        segments = self.segment_by_language(text)
        all_entities = []

        for segment, lang in segments:
            if not segment:
                continue

            if lang == 'zh':
                # ä½¿ç”¨ HanLP å¤„ç†ä¸­æ–‡
                result = self.hanlp(segment, tasks='ner')
                # è§£æç»“æœ...
                entities = self._parse_hanlp(result)

            elif lang == 'en':
                # ä½¿ç”¨ spaCy å¤„ç†è‹±æ–‡
                doc = self.spacy_en(segment)
                entities = [
                    {'text': ent.text, 'type': ent.label_}
                    for ent in doc.ents
                ]

            else:
                # å…¶ä»–è¯­è¨€ä½¿ç”¨ GLiNER
                entities = self.gliner.predict_entities(
                    segment,
                    ["person", "organization", "location", "product"]
                )

            all_entities.extend(entities)

        return all_entities
```

**æ–¹æ¡ˆ 2: ç›´æ¥ä½¿ç”¨ GLiNER**ï¼ˆç®€å•ä½†è´¨é‡è¾ƒä½ï¼‰

```python
from gliner import GLiNER

model = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")

text = "Apple Inc. åœ¨åº“æ¯”è’‚è¯º(Cupertino)æ€»éƒ¨å‘å¸ƒäº†æ–°æ¬¾ iPhone"

# GLiNER å¯ä»¥å¤„ç†æ··åˆè¯­è¨€ï¼ˆä½†è´¨é‡ä¸å¦‚æ–¹æ¡ˆ1ï¼‰
labels = ["company", "person", "location", "product", "å…¬å¸", "åœ°ç‚¹", "äº§å“"]
entities = model.predict_entities(text, labels)

# ä¼˜åŠ¿: ç®€å•ï¼Œä¸€æ¬¡è°ƒç”¨
# ç¼ºç‚¹: ä¸­æ–‡éƒ¨åˆ†è´¨é‡è¾ƒä½
```

---

### 5. å®Œæ•´å·¥å…·å¯¹æ¯”çŸ©é˜µ

#### æŒ‰è¯­è¨€å’Œè´¨é‡æ’åº

| å·¥å…· | è‹±æ–‡ | ä¸­æ–‡ | æ³•/å¾·/è¥¿ | æ—¥/éŸ© | å…¶ä»– | é›¶æ ·æœ¬ | é€Ÿåº¦ | æ˜“ç”¨æ€§ |
|------|------|------|---------|------|------|--------|------|--------|
| **HanLP** | 90% | **95%** | - | **90%** | - | âŒ | â­â­â­â­ | â­â­â­â­ |
| **spaCy** | **90%** | 65% | **88%** | 70% | 60% | âŒ | â­â­â­â­â­ | â­â­â­â­â­ |
| **StanfordNLP** | **92%** | 80% | 85% | - | - | âŒ | â­â­â­ | â­â­â­ |
| **GLiNER** | 92% | 24% | **50%** | 31% | **45%** | âœ… | â­â­â­â­â­ | â­â­â­â­ |
| **mBERT** | 80% | 70% | 75% | 65% | 60% | âŒ | â­â­â­â­ | â­â­â­ |
| **LLM (GPT-4)** | 65% | 60% | 55% | 50% | 50% | âœ… | â­â­ | â­â­â­â­ |

#### æŒ‰åœºæ™¯æ¨è

| åœºæ™¯ | é¦–é€‰ | æ¬¡é€‰ | å¤‡é€‰ |
|------|------|------|------|
| **çº¯è‹±æ–‡ï¼Œæ ‡å‡†å®ä½“** | spaCy | StanfordNLP | GLiNER |
| **çº¯è‹±æ–‡ï¼Œè‡ªå®šä¹‰å®ä½“** | GLiNER | LLM | Flair |
| **çº¯ä¸­æ–‡** | HanLP | - | spaCy |
| **çº¯æ³•/å¾·/è¥¿** | GLiNER | spaCy | mBERT |
| **çº¯æ—¥/éŸ©** | HanLP (æ—¥) / spaCy | GLiNER | - |
| **å¤šè¯­è¨€æ··åˆ** | **GLiNER** | åˆ†åˆ«å¤„ç† | LLM |
| **ä»»æ„è¯­è¨€** | **GLiNER** | LLM | å•†ä¸š API |
| **è¿½æ±‚æè‡´è´¨é‡ï¼ˆè‹±ï¼‰** | StanfordNLP | spaCy | GLiNERï¼ˆå¾®è°ƒï¼‰|
| **è¿½æ±‚æè‡´é€Ÿåº¦** | spaCy (CNN) | GLiNER | - |
| **éœ€è¦é›¶æ ·æœ¬** | **GLiNER** | LLM | - |

---

### 6. LightRAG é›†æˆå»ºè®®

#### æ¨èç­–ç•¥ï¼šæŒ‰ä¸»è¦è¯­è¨€é€‰æ‹©

```python
# lightrag/llm/multilingual_entity_extractor.py

from typing import List, Dict
import spacy
from gliner import GLiNER

class MultilingualEntityExtractor:
    """LightRAG å¤šè¯­è¨€å®ä½“æå–å™¨"""

    def __init__(self, primary_language: str = 'en'):
        """
        Args:
            primary_language: ä¸»è¦è¯­è¨€
                - 'en': è‹±æ–‡
                - 'zh': ä¸­æ–‡
                - 'multi': å¤šè¯­è¨€æ··åˆ
        """
        self.primary_language = primary_language

        if primary_language == 'zh':
            # ä¸­æ–‡ä¼˜å…ˆï¼šä½¿ç”¨ HanLP
            import hanlp
            self.extractor = hanlp.load(
                hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH
            )
            self.extract_method = self._extract_hanlp

        elif primary_language == 'en':
            # è‹±æ–‡ä¼˜å…ˆï¼šä½¿ç”¨ spaCy
            self.extractor = spacy.load("en_core_web_trf")
            self.extract_method = self._extract_spacy

        else:  # 'multi' æˆ–å…¶ä»–
            # å¤šè¯­è¨€ï¼šä½¿ç”¨ GLiNER
            self.extractor = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")
            self.extract_method = self._extract_gliner

    def extract(self, text: str, custom_labels: List[str] = None) -> List[Dict]:
        """æå–å®ä½“

        Args:
            text: è¾“å…¥æ–‡æœ¬
            custom_labels: è‡ªå®šä¹‰å®ä½“ç±»å‹ï¼ˆä»… GLiNER æ”¯æŒï¼‰

        Returns:
            [{'entity': '...', 'type': '...', 'score': 0.9}, ...]
        """
        return self.extract_method(text, custom_labels)

    def _extract_spacy(self, text, custom_labels=None):
        """ä½¿ç”¨ spaCy æå–"""
        doc = self.extractor(text)
        return [
            {'entity': ent.text, 'type': ent.label_, 'score': 1.0}
            for ent in doc.ents
        ]

    def _extract_hanlp(self, text, custom_labels=None):
        """ä½¿ç”¨ HanLP æå–"""
        result = self.extractor(text, tasks='ner')
        entities = []
        # è§£æ HanLP ç»“æœ...
        # (è¯¦ç»†å®ç°è§ HanLP vs GLiNER æ–‡æ¡£)
        return entities

    def _extract_gliner(self, text, custom_labels=None):
        """ä½¿ç”¨ GLiNER æå–"""
        if custom_labels is None:
            # é»˜è®¤å®ä½“ç±»å‹
            custom_labels = [
                "person", "organization", "location",
                "date", "product", "event"
            ]

        entities = self.extractor.predict_entities(text, custom_labels)
        return [
            {'entity': e['text'], 'type': e['label'], 'score': e['score']}
            for e in entities
            if e['score'] > 0.5
        ]

# ä½¿ç”¨ç¤ºä¾‹
# è‹±æ–‡æ–‡æ¡£
extractor_en = MultilingualEntityExtractor(primary_language='en')
entities = extractor_en.extract("Apple Inc. was founded by Steve Jobs.")

# ä¸­æ–‡æ–‡æ¡£
extractor_zh = MultilingualEntityExtractor(primary_language='zh')
entities = extractor_zh.extract("è‹¹æœå…¬å¸ç”±å²è’‚å¤«Â·ä¹”å¸ƒæ–¯åˆ›ç«‹ã€‚")

# å¤šè¯­è¨€æ–‡æ¡£
extractor_multi = MultilingualEntityExtractor(primary_language='multi')
entities = extractor_multi.extract(
    "Apple Inc. åœ¨åº“æ¯”è’‚è¯ºå‘å¸ƒæ–°äº§å“ã€‚",
    custom_labels=["company", "location", "product", "å…¬å¸", "åœ°ç‚¹", "äº§å“"]
)
```

---

### 7. æ€§èƒ½å’Œæˆæœ¬å¯¹æ¯”

#### ç´¢å¼• 10,000 chunksï¼ˆå¤šè¯­è¨€æ··åˆï¼‰

| æ–¹æ¡ˆ | æ—¶é—´ | GPU æˆæœ¬ | è´¨é‡ï¼ˆä¼°ç®—ï¼‰|
|------|------|---------|------------|
| **LLM (Qwen-7B)** | 500s | $0.25 | F1 85% |
| **spaCy (è‹±)** | 50s | $0.025 | F1 90% |
| **HanLP (ä¸­)** | 100s | $0.05 | F1 95% |
| **GLiNER (å¤šè¯­è¨€)** | 30s | $0.015 | F1 45-60% |
| **æ··åˆç­–ç•¥*** | 80s | $0.04 | F1 85-90% |

*æ··åˆç­–ç•¥ï¼šä¸­æ–‡ç”¨ HanLPï¼Œè‹±æ–‡ç”¨ spaCyï¼Œå…¶ä»–ç”¨ GLiNER

**ç»“è®º**ï¼š
- âœ… æ··åˆç­–ç•¥è´¨é‡æœ€é«˜ä¸”æˆæœ¬å¯æ§
- âœ… GLiNER é€Ÿåº¦æœ€å¿«ä½†è´¨é‡è¾ƒä½
- âœ… æ ¹æ®è¯­è¨€åˆ†åˆ«å¤„ç†æ˜¯æœ€ä½³å®è·µ

---

### 8. æœ€ç»ˆæ¨è

#### å¯¹äº LightRAG ç”¨æˆ·

**åœºæ™¯ 1: çº¯è‹±æ–‡æ–‡æ¡£**
```python
æ¨è: spaCy
ç†ç”±:
- F1 90%ï¼ˆè´¨é‡é«˜ï¼‰
- é€Ÿåº¦æå¿«
- æ˜“äºé›†æˆ
- æˆæœ¬ä½

å®æ–½:
pip install spacy
python -m spacy download en_core_web_trf
```

**åœºæ™¯ 2: çº¯ä¸­æ–‡æ–‡æ¡£**
```python
æ¨è: HanLP
ç†ç”±:
- F1 95%ï¼ˆæœ€é«˜è´¨é‡ï¼‰
- ä¸“é—¨ä¸ºä¸­æ–‡ä¼˜åŒ–
- é›†æˆåˆ†è¯

å®æ–½:
pip install hanlp
# ä½¿ç”¨ ELECTRA æ¨¡å‹
```

**åœºæ™¯ 3: å¤šè¯­è¨€æ–‡æ¡£ï¼ˆè‹±+ä¸­+å…¶ä»–ï¼‰**
```python
æ¨è: GLiNER + æ··åˆç­–ç•¥
ç†ç”±:
- GLiNER æ”¯æŒ 40+ è¯­è¨€
- é›¶æ ·æœ¬çµæ´»æ€§
- å¯é’ˆå¯¹ä¸»è¦è¯­è¨€ä¼˜åŒ–

å®æ–½:
# ä¸»è¦è¯­è¨€ç”¨ä¸“é—¨æ¨¡å‹
# æ¬¡è¦è¯­è¨€ç”¨ GLiNER
# è¯¦è§ä¸Šé¢çš„ MultilingualEntityExtractor
```

**åœºæ™¯ 4: éœ€è¦è‡ªå®šä¹‰å®ä½“ç±»å‹**
```python
æ¨è: GLiNERï¼ˆä»»ä½•è¯­è¨€ï¼‰
ç†ç”±:
- é›¶æ ·æœ¬å­¦ä¹ 
- æ— éœ€è®­ç»ƒæ•°æ®
- ä»»æ„å®ä½“ç±»å‹

å®æ–½:
from gliner import GLiNER
model = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")
```

---

### 9. å®æˆ˜å»ºè®®

#### é˜¶æ®µ 1: ç¡®å®šä¸»è¦è¯­è¨€

```bash
# åˆ†æä½ çš„æ–‡æ¡£è¯­æ–™åº“
python scripts/analyze_language_distribution.py --input docs/

# è¾“å‡ºç¤ºä¾‹:
# English: 65%
# Chinese: 25%
# French: 5%
# German: 3%
# Other: 2%
```

#### é˜¶æ®µ 2: é€‰æ‹©å·¥å…·

```python
ä¸»è¦è¯­è¨€ > 80%:
  â””â”€ ä½¿ç”¨è¯¥è¯­è¨€çš„æœ€ä½³å·¥å…·
     - è‹±æ–‡ â†’ spaCy
     - ä¸­æ–‡ â†’ HanLP
     - æ³•/å¾·/è¥¿ â†’ GLiNER æˆ– spaCy

ä¸»è¦è¯­è¨€ 50-80%:
  â””â”€ æ··åˆç­–ç•¥
     - ä¸»è¦è¯­è¨€ç”¨ä¸“é—¨æ¨¡å‹
     - å…¶ä»–è¯­è¨€ç”¨ GLiNER

å¤šè¯­è¨€æ··åˆï¼ˆæ— æ˜æ˜¾ä¸»è¦è¯­è¨€ï¼‰:
  â””â”€ GLiNERï¼ˆé›¶æ ·æœ¬ï¼‰
```

#### é˜¶æ®µ 3: å®æ–½å’Œè¯„ä¼°

```bash
# å®æ–½
python scripts/integrate_ner_model.py --model spacy --language en

# è¯„ä¼°è´¨é‡
python scripts/evaluate_entity_extraction.py \
    --method spacy \
    --baseline llm \
    --num_samples 100

# è¯„ä¼°ç«¯åˆ°ç«¯ RAG æ•ˆæœ
python lightrag/evaluation/eval_rag_quality.py
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **è‹±æ–‡ï¼šspaCy æ˜¯æœ€ä½³å¹³è¡¡**
   - F1 90%ï¼Œé€Ÿåº¦å¿«ï¼Œæ˜“ç”¨
   - StanfordNLP è´¨é‡æ›´é«˜ï¼ˆF1 92%ï¼‰ä½†é€Ÿåº¦æ…¢
   - GLiNER é€‚åˆéœ€è¦è‡ªå®šä¹‰å®ä½“çš„åœºæ™¯

2. **ä¸­æ–‡ï¼šHanLP æ— å¯æ›¿ä»£**
   - F1 95% vs GLiNER 24%
   - è´¨é‡å·®è·å¤ªå¤§ï¼Œä¸è€ƒè™‘ GLiNER

3. **å¤šè¯­è¨€/å…¶ä»–è¯­ç§ï¼šGLiNER æ˜¯ç‹è€…**
   - æ”¯æŒ 40+ è¯­è¨€
   - é›¶æ ·æœ¬çµæ´»æ€§
   - æ¬§æ´²è¯­è¨€è¡¨ç°ä¼˜ç§€ï¼ˆF1 45-60%ï¼‰

4. **æ··åˆç­–ç•¥æœ€ä¼˜**
   - ä¸»è¦è¯­è¨€ç”¨ä¸“é—¨æ¨¡å‹
   - æ¬¡è¦è¯­è¨€ç”¨ GLiNER
   - å…¼é¡¾è´¨é‡å’Œæˆæœ¬

5. **è‡ªå®šä¹‰å®ä½“ï¼šGLiNER ç‹¬é¢†é£éªš**
   - ä»»ä½•è¯­è¨€éƒ½å¯é›¶æ ·æœ¬è¯†åˆ«è‡ªå®šä¹‰å®ä½“
   - æ— éœ€è®­ç»ƒæ•°æ®
   - çµæ´»æ€§æ— å¯æ¯”æ‹Ÿ

### å†³ç­–æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¡®å®šä¸»è¦è¯­è¨€                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ è‹±æ–‡ > 80%?  â”‚
      â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
         â”‚ æ˜¯     â”‚ å¦
         â–¼        â–¼
     spaCy    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ ä¸­æ–‡ > 80%?  â”‚
              â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
                 â”‚ æ˜¯     â”‚ å¦
                 â–¼        â–¼
              HanLP   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ éœ€è¦è‡ªå®šä¹‰å®ä½“? â”‚
                      â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                         â”‚ æ˜¯       â”‚ å¦
                         â–¼          â–¼
                      GLiNER    æ··åˆç­–ç•¥
                                (ä¸»è¦+GLiNER)
```

---

## å‚è€ƒèµ„æº

### å¼€æºå·¥å…·
- spaCy: https://spacy.io/
- GLiNER: https://github.com/urchade/GLiNER
- HanLP: https://github.com/hankcs/HanLP
- StanfordNLP: https://stanfordnlp.github.io/CoreNLP/
- Flair: https://github.com/flairNLP/flair

### åŸºå‡†æ•°æ®é›†
- CoNLL 2003 (è‹±æ–‡)
- MSRA (ä¸­æ–‡)
- MultiCoNER (11 è¯­è¨€)
- OntoNotes (è‹±ä¸­)
- Universal NER (å¤šè¯­è¨€)

### è®ºæ–‡
- GLiNER: "Generalist Model for NER" (NAACL 2024)
- Universal NER: "Gold-Standard Multilingual NER" (2024)
- spaCy: "Industrial-strength NLP" (2020)

---

éœ€è¦æˆ‘å¸®ä½ ï¼š
- å®ç°å…·ä½“è¯­è¨€çš„é›†æˆï¼Ÿ
- åˆ›å»ºæ··åˆç­–ç•¥ä»£ç ï¼Ÿ
- è¿è¡Œå¤šè¯­è¨€æ€§èƒ½æµ‹è¯•ï¼Ÿ
