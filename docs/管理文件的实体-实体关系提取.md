# 范围判定


## 1. 知识图谱构建

**system**: 

```
You are a helpful assistant. You can help me by answering my questions. You can also ask me questions.
```

**user**: 

```
这是我刚才跟你谈论记录，请你先了解我们的任务
```

**assistant**: 

```
好的，我已经仔细阅读了您提供的这份名为“改进建议.md”的文档。

根据文档内容，我理解我们当前的任务是围绕**构建一个关于特定管理规定（以《广铁集团动车组列车票价管理实施细则（修订）》为例）的知识图谱**展开的。

具体来说，这个任务包含以下几个核心方面：

1.  **目标文档分析与知识图谱设计**：
    *   分析类似“广铁集团动车组列车票价管理实施细则”这样的规章制度文档。
    *   设计并优化知识图谱的模式（Schema），包括定义更精确、更全面的**实体类型**（如文档、章节、主题、关键词、人员、角色、组织、时间、事件、法规）和**关系类型**（如隶属、版本、引用、依据、责任、审批、时间、生效、关联）。文档中已经包含了对初步设计的详细改进建议。

2.  **信息抽取方法**：
    *   利用强大的**大型语言模型（LLM）**，结合文档已有的良好结构（章、节、条），从文本中自动抽取预定义的实体和关系。
    *   探讨并确定具体的技术实施细节，包括：
        *   **文本分块策略**：如何将长文档切分成适合LLM处理的小块（建议按“节”或“条”）。
        *   **提示词（Prompt）设计**：如何编写清晰、有效的指令来引导LLM准确地抽取出实体和关系，并按指定格式（如JSON）输出。
        *   **结果审核机制**：如何设计自动（如一致性、格式、逻辑校验）和半自动（如抽样人工审核、异常检测）的方法来验证和修正LLM抽取结果的准确性。

3.  **知识图谱的应用目标**：
    *   最终构建的知识图谱旨在提高对管理规定信息的**检索效率**，增强**信息完整性**，支持**智能问答**、**合规性检查**、**管理决策**等应用。
    *   关注组织层级、文档制定与执行单位、文档结构与内容细节（主题、事件、人物、单位）的提取与关联。

总而言之，我们的任务是**设计一个优化的知识图谱模型来表示管理规定文档中的复杂信息，并探讨如何利用LLM技术高效、准确地从这些文档中抽取实体和关系，同时建立审核机制以确保数据质量，最终服务于实际应用需求。**

我的理解是否准确？接下来您希望我们聚焦于哪个具体环节进行讨论或操作呢？
```

**user**: 

```
现在我给你一份对markdown格式的管理文件的解析结果（json）格式，请你先理解这个json文档的内容

```

**assistant**: 

```
好的，我已经加载并查看了您提供的 `demo3.json` 文件。

这是一个 JSON 数组，其中每个元素（对象）代表从一个 Markdown 文件 (`tests/demo3.md`) 解析出来的一个文本块（chunk）。

根据其结构和字段，我理解这个 JSON 文件主要包含以下信息：

1.  **分块（Chunking）**：
    *   原始的 Markdown 文档被分割成了多个块（chunks）。
    *   每个块由一个 JSON 对象表示。

2.  **块内容与元数据**：
    *   `content`: 每个块包含其对应的文本内容。值得注意的是，内容似乎包含了从根节点到当前节点的标题路径以及该节点下的具体文本。例如，子节点的 `content` 会重复其父节点的标题。
    *   `heading`: 明确提取了该块对应的 Markdown 标题。
    *   `tokens`: 该块内容的 token 数量（可能是用于衡量文本长度或 LLM 处理成本）。
    *   `file_path`: 指向原始 Markdown 文件的路径。
    *   `full_doc_id`: 整个文档的唯一标识符。
    *   `page_numbers`: 包含一个数组，指示该块内容在原始文档（可能是 PDF 转换而来）中对应的页码。

3.  **层级结构（Hierarchy）**：
    *   `chunk_id`: 每个块的唯一标识符。
    *   `parent_id`: 指向其父块的 `chunk_id`。根块的 `parent_id` 为 `null`。
    *   `child_ids`: 包含一个数组，列出了直接属于该块的子块的 `chunk_id`。叶子节点（如具体条款内容）的 `child_ids` 为空数组 `[]`。
    *   `chunk_order_index`: 块在其父块下的顺序索引。
    *   这种父子关系 (`parent_id`, `child_ids`) 清晰地表达了原始 Markdown 文档的层级结构（例如：文档标题 -> 章 -> 节 -> 条）。

4.  **文档内容概要**：
    *   从 `heading` 和 `content` 字段的内容来看，这份 JSON 解析的是一份名为《广州局集团公司客票发售和预订系统（含互联网售票部分）应急预案》的管理文件。
    *   文件包含了总则、组织管理、监测预警、应急处置、恢复重建、调查评估、应急保障、附则等章节，以及多个附件（联系方式、报告表、使用办法、车站列表）。
    *   内容详细规定了票务系统突发事件的分类、预警级别、组织职责、响应流程、处置措施等。

**总结**：这份 JSON 文件是对一份结构化的应急预案 Markdown 文档进行层级化分块后的结果。它不仅保留了每个部分的文本内容和标题，还通过父子 ID 显式地记录了文档的树状结构，并附加了文件来源、页码等元数据。这种格式非常适合后续进行基于文档结构的信息抽取或构建知识图谱。

我的理解是否准确？您可以基于这份解析结果提出下一步的要求了。
```

**user**: 

```
现在你用python写一个从这个json文件中提取知识图谱实体和实体关系的程序：
1、提取实体和实体关系建模和方法已经在之前我给你的改进建议.md里;
2、用LLM进行实体和实体关系提取；
请你先理解我的要求，提出你的思路，我们讨论后再开始具体写出程序
```

**assistant**: 

```
好的，我理解您的要求。

您希望我基于以下条件，为您编写一个 Python 程序，用于从 `demo3.json` 文件中提取知识图谱的实体和关系：

1.  **输入数据**：`demo3.json` 文件，它包含了从 Markdown 管理文件解析出的、带有层级结构的文本块（chunks）。
2.  **知识图谱模式（Schema）**：实体和关系的类型已经在您之前提供的 `改进建议.md` 文件中定义好了（包括扩展和新增的类型）。
3.  **提取方法**：利用大型语言模型（LLM）进行实体和关系的抽取。
4.  **输出**：提取出的实体和关系列表。
5.  **当前步骤**：在编写具体代码之前，先提出我的思路，供您审阅和讨论。

---

**我的思路如下：**

1.  **环境准备与配置**：
    *   需要安装必要的 Python 库：`json` (内置库，用于加载 JSON 文件)，以及一个用于调用 LLM API 的库（例如 `openai`, `google-generativeai`, `anthropic` 等，具体取决于您选用哪个 LLM 服务）。
    *   配置 LLM API 密钥：通常通过环境变量或配置文件安全地管理 API 密钥。
    *   定义知识图谱模式：在代码中明确定义 `改进建议.md` 中确定的实体类型列表和关系类型列表，以便在提示词中使用。

2.  **数据加载**：
    *   编写函数读取 `demo3.json` 文件，将其内容加载为一个 Python 列表（其中每个元素是一个代表 chunk 的字典）。

3.  **选择处理单元（Chunk Selection）**：
    *   遍历 JSON 文件中的每一个 chunk 对象。
    *   **策略**：初步考虑处理**每一个** chunk。因为 `demo3.json` 中的 `content` 字段似乎已经包含了从根到当前节点的标题路径和具体内容，这为 LLM 提供了必要的上下文。即使是层级较高的 chunk（如章节标题），其 `content` 也可能包含文档元数据或顶层信息，适合提取如“文档”、“章节”、“组织”等实体。层级较低的 chunk（如具体条款）则包含更详细的信息，适合提取“事件”、“角色”、“关键词”等。

4.  **构建 LLM 提示词（Prompt Engineering）**：
    *   根据 `改进建议.md` 中的指导，设计两个核心的提示词模板：
        *   **实体提取提示词**：
            *   输入：单个 chunk 的 `content` 文本。
            *   指令：要求 LLM 从文本中识别并抽取出所有预定义的实体类型（文档、章节、主题、关键词、人员、角色、组织、时间、事件、法规）。
            *   附加信息：在提示词中包含每种实体类型的简要定义或示例，以提高准确性。
            *   输出格式：明确要求 LLM 以 JSON 格式输出，例如 `{"entities": [{"name": "实体名称", "type": "实体类型"}, ...]}`。
        *   **关系提取提示词**：
            *   输入：单个 chunk 的 `content` 文本。（*或者，为了更好地捕捉关系，可以考虑将实体提取的结果也作为输入，但这会增加复杂度，初步先只用文本*）。
            *   指令：要求 LLM 从文本中识别并抽取出所有预定义的关系类型（隶属、版本、引用、依据、责任、审批、时间、生效、关联），并指出关系连接的源实体和目标实体。
            *   附加信息：在提示词中包含每种关系类型的简要定义或示例。
            *   输出格式：明确要求 LLM 以 JSON 格式输出，例如 `{"relations": [{"source": "源实体名称", "target": "目标实体名称", "type": "关系类型"}, ...]}`。
    *   将 chunk 的 `content` 填入提示词模板，生成最终发送给 LLM 的具体指令。

5.  **LLM API 调用与结果解析**：
    *   编写一个函数，负责调用配置好的 LLM API。该函数接收生成的提示词作为输入。
    *   发送请求并获取 LLM 的响应。
    *   处理 API 可能返回的错误（如超时、速率限制、认证失败等）。
    *   解析 LLM 返回的 JSON 字符串。需要健壮地处理可能出现的 JSON 解析错误（例如 LLM 未完全按要求生成有效的 JSON）。
    *   从解析后的 Python 对象中提取出实体列表和关系列表。

6.  **结果聚合与去重**：
    *   初始化两个集合（Set）：一个用于存储所有提取到的实体（元组形式，例如 `(name, type)`），另一个用于存储所有提取到的关系（元组形式，例如 `(source_name, target_name, type)`）。使用集合可以自动处理完全相同的重复项。
    *   在处理完每个 chunk 后，将从 LLM 获取并解析出的实体和关系添加到相应的集合中。

7.  **输出**：
    *   处理完所有 chunk 后，将集合中的实体和关系转换回列表或字典的格式，方便查看或进一步处理。
    *   打印或保存最终提取到的、去重后的实体和关系列表。

8.  **代码结构**：
    *   主函数/脚本：协调整个流程（加载数据 -> 遍历 chunk -> 调用提取 -> 聚合结果 -> 输出）。
    *   LLM 调用模块/函数：封装与 LLM API 的交互。
    *   提示词生成模块/函数：根据 chunk 内容和模式生成具体的提示词。
    *   配置文件（可选）：存储 API 密钥、模型名称等。

**待讨论/确认的点：**

*   **LLM 选择**：您计划使用哪个具体的 LLM 服务或模型？不同的模型可能在遵循指令、输出格式和抽取能力上有所差异。
*   **API 密钥管理**：确认 API 密钥的安全存储方式。
*   **性能与成本**：`demo3.json` 文件包含 61 个 chunk。对每个 chunk 分别调用 LLM 进行实体和关系提取（即 122 次 API 调用）可能会有时间和成本上的考虑。是否需要优化（例如，尝试在一个提示词中同时提取实体和关系，或者过滤掉内容过少的 chunk）？
*   **错误处理**：对于 LLM 返回无效 JSON 或提取结果不符合预期的情况，需要定义处理策略（例如，跳过、记录错误、重试）。
*   **关系提取的上下文**：仅使用单个 chunk 的 `content` 提取关系可能无法捕捉跨 chunk 的关系。当前 `demo3.json` 的 `content` 包含了层级路径，这有一定帮助。我们是否满足于 chunk 内的关系提取，还是需要更复杂的策略？（根据 `改进建议.md`，目前的重点似乎是 chunk 内提取）。
*   **实体/关系规范化**：LLM 提取的实体名称可能存在细微差异（如“集团公司客运部” vs “客运部”）。本程序初步只做简单去重，后续可能需要加入实体链接或规范化步骤。

请您审阅以上思路，特别是待讨论的点。如果您觉得方向正确，或者有任何修改意见，请告诉我，然后我们可以开始着手编写具体的 Python 代码。
```

**user**: 

```
你提出下面的两点非常有意义，请你考虑进去
1、关系提取的上下文：仅使用单个 chunk 的 content 提取关系可能无法捕捉跨 chunk 的关系。当前 demo3.json 的 content 包含了层级路径，这有一定帮助。我们是否满足于 chunk 内的关系提取，还是需要更复杂的策略？（根据 改进建议.md，目前的重点似乎是 chunk 内提取）。
2、实体/关系规范化：LLM 提取的实体名称可能存在细微差异（如“集团公司客运部” vs “客运部”）。本程序初步只做简单去重，后续可能需要加入实体链接或规范化步骤。
```

**assistant**: 

```
好的，这确实是两个非常关键的点，直接关系到知识图谱的完整性和准确性。我们来把这两点考虑进去，并细化思路：

---

**Refined 思路：**

1.  **环境准备与配置**：(同前)
    *   安装所需库 (`json`, LLM 库)。
    *   配置 LLM API 密钥。
    *   定义知识图谱模式（实体类型、关系类型）。
    *   **新增**：定义一个简单的**实体名称规范化映射表 (Canonical Map)**，例如：
        
        CANONICAL_MAP = {
            "客运部": "集团公司客运部",
            "信息技术所": "集团公司信息技术所", # 假设需要统一
            "广铁集团": "中国铁路广州局集团有限公司",
            "集团公司": "中国铁路广州局集团有限公司", # 根据上下文判断是否合适
            "本预案": "《广州局集团公司客票发售和预订系统（含互联网售票部分）应急预案》", # 预定义文档主标题
            # ... 更多可能的别名、简称 -> 规范名
        }
        

2.  **数据加载**：(同前)
    *   读取 `demo3.json`。

3.  **处理单元选择与上下文增强**：
    *   遍历 JSON 文件中的每一个 chunk 对象。
    *   **策略**：仍然以单个 chunk 作为 LLM 处理的基本单元。
    *   **上下文增强**：虽然 `content` 字段包含了标题路径，这有助于理解当前 chunk 在文档中的位置，但对于跨 chunk 关系，仅靠 LLM 读取单个 chunk 的文本仍然困难。我们将采用**两阶段**方法处理关系：
        *   **阶段一 (LLM 抽取)**：LLM 专注于从**当前 chunk 的 `content` 文本内**显式描述的关系（如 `ResponsibleFor`, `BasedOn`, `References`, `OccursAt`, `EffectiveFrom`, `ApprovedBy` 等）。
        *   **阶段二 (结构化抽取)**：在 LLM 处理完所有 chunk 后，利用 `demo3.json` 本身提供的 `parent_id` 和 `child_ids` 结构信息，**程序化地**添加层级关系，主要是 `BelongsTo` 关系。

4.  **构建 LLM 提示词**：(基本同前)
    *   **实体提取提示词**：保持不变，要求提取预定义实体并以 JSON 输出。包含类型定义。
    *   **关系提取提示词**：明确指示 LLM **仅提取在所提供的文本段落内能直接找到依据的关系**。告知 LLM 输出源实体、目标实体和关系类型（JSON 格式）。包含类型定义。

5.  **LLM API 调用与结果解析**：(同前)
    *   调用 LLM API，处理错误，解析返回的 JSON。

6.  **结果聚合、规范化与去重**：(**重点改进**)
    *   初始化三个集合：`entities_set` (存储规范化后的 `(name, type)` 元组)，`relations_set` (存储规范化后的 `(source_name, target_name, type)` 元组)，以及一个临时字典 `chunk_entities` (存储每个 chunk ID 对应的规范化实体列表，用于后续添加结构化关系)。
    *   **定义规范化函数**：
        
        def normalize_entity_name(raw_name):
            cleaned_name = raw_name.strip() # 去除首尾空格
            # 查找映射表，如果找到则返回规范名，否则返回清理后的原始名
            return CANONICAL_MAP.get(cleaned_name, cleaned_name)
        
    *   **处理 LLM 结果**：
        *   对于从 LLM 提取的每个实体 `(raw_name, type)`：
            *   `normalized_name = normalize_entity_name(raw_name)`
            *   将 `(normalized_name, type)` 添加到 `entities_set`。
            *   将规范化后的实体信息存入 `chunk_entities[current_chunk_id]`。
        *   对于从 LLM 提取的每个关系 `(raw_source, raw_target, type)`：
            *   `normalized_source = normalize_entity_name(raw_source)`
            *   `normalized_target = normalize_entity_name(raw_target)`
            *   将 `(normalized_source, normalized_target, type)` 添加到 `relations_set`。

7.  **添加结构化关系（BelongsTo）**：(**新增步骤**)
    *   在所有 chunk 都经过 LLM 处理后，再次遍历 `demo3.json` 数据。
    *   对于每一个 chunk `C`：
        *   获取其 `chunk_id` 和 `parent_id`。
        *   如果 `parent_id` 不为 `null`：
            *   获取当前 chunk `C` 的规范化实体名称（可能需要从 `C['heading']` 规范化得到，或者最好是从 `chunk_entities[C['chunk_id']]` 中查找代表该 chunk 的实体，例如类型为 '章节' 或 '文档' 的实体）。
            *   获取父 chunk `P` 的规范化实体名称（同理，从 `P['heading']` 规范化或 `chunk_entities[P['chunk_id']]` 查找）。
            *   **如果能成功找到代表这两个 chunk 的实体名称**（例如，`child_entity_name` 和 `parent_entity_name`），则添加关系 `(child_entity_name, parent_entity_name, "BelongsTo")` 到 `relations_set`。
            *   *注意：* 此处需要仔细处理如何准确识别代表 chunk 本身的实体。一个简单的方法是，如果 LLM 没有提取出代表当前 chunk 标题的实体，我们可以基于 `heading` 字段创建它（并推断类型，如'章节'），然后进行规范化。

8.  **输出**：(同前)
    *   将 `entities_set` 和 `relations_set` 中的内容转换为列表或字典格式进行输出或保存。

**改进后的优势：**

*   **关系上下文**：通过结合 LLM 抽取（处理文本内关系）和结构化抽取（处理层级关系），提高了关系抽取的覆盖面，特别是准确捕捉了文档的 `BelongsTo` 结构。
*   **实体规范化**：通过引入简单的规范化步骤，减少了因名称不一致导致的数据冗余和后续处理困难，提高了知识图谱的一致性。

**待确认/进一步细化：**

*   **规范化映射表 `CANONICAL_MAP` 的构建**：这个表需要根据文档内容和常见缩写预先定义，可能需要一些人工分析。
*   **结构化关系中实体的识别**：在步骤 7 中，如何精确地将 chunk ID 映射到其代表的实体（特别是当 LLM 未提取时），需要一个明确的逻辑。是直接使用规范化后的 `heading` 作为实体名，还是有其他策略？（建议初步使用规范化后的 `heading`，并假设其类型为 '章节' 或根据层级判断）。
*   **LLM 能力**：即使有规范化，LLM 提取的实体名可能仍有预期外的变化。规范化映射表需要持续维护。

这个细化后的思路解决了您提出的两个关键问题，使得程序的目标更加清晰和健壮。如果您同意这个方向，我们可以开始构思具体的 Python 代码实现了。
```

**user**: 

```
可以开始构思具体的 Python 代码实现了
```

**assistant**: 

```
Okay, let's start building the Python code based on our refined思路.

I'll structure the code with clear functions for each step: loading data, defining prompts, calling the LLM (placeholder), parsing results, normalizing names, and extracting structural relations.

**Assumptions:**

1.  You have a Python environment (>= 3.7 for type hints).
2.  You will replace the placeholder `call_llm` function with your actual LLM API call (e.g., using `openai`, `google-generativeai`, etc.).
3.  You have the `demo3.json` file in the same directory or provide the correct path.
4.  You'll set the `LLM_API_KEY` environment variable or modify the code to handle API key retrieval securely.


import json
import os
import logging
import time # For potential rate limiting
from typing import List, Dict, Set, Tuple, Optional, Any

# --- Configuration ---

# 1. LLM Configuration
# !! IMPORTANT: Replace with your actual LLM provider's method !!
# Example for OpenAI (requires `pip install openai`)
# from openai import OpenAI
# client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
# LLM_MODEL = "gpt-3.5-turbo" # Or your preferred model

# For this example, we use a placeholder
LLM_API_KEY = os.environ.get("YOUR_LLM_API_KEY") # Replace YOUR_LLM_API_KEY with the actual env var name
LLM_MODEL = "your-chosen-llm-model" # Replace with your model identifier

# 2. File Path
INPUT_JSON_PATH = "demo3.json"

# 3. Knowledge Graph Schema (Based on 改进建议.md)
ENTITY_TYPES = [
    "文档", "章节", "主题", "关键词", "人员", "角色",
    "组织", "时间", "事件", "法规"
]

RELATION_TYPES = [
    "隶属关系", "版本关系", "引用", "依据", "责任",
    "审批", "时间", "生效", "关联"
]

# 4. Entity Normalization Map (Example - expand as needed)
CANONICAL_MAP = {
    "客运部": "集团公司客运部",
    "信息技术所": "集团公司信息技术所",
    "科信部": "集团公司科信部",
    "财务部": "集团公司财务部",
    "计统部": "集团公司计统部",
    "电务部": "集团公司电务部",
    "供电部": "集团公司供电部",
    "宣传部": "集团公司宣传部",
    "调度所": "集团公司调度所",
    "集团公司应急领导小组办公室": "集团公司应急领导小组办公室", # Already specific
    "集团公司应急领导小组": "集团公司应急领导小组", # Already specific
    "国铁集团应急领导小组办公室": "国铁集团应急领导小组办公室",
    "国铁集团应急领导小组": "国铁集团应急领导小组",
    "国铁集团客运部": "国铁集团客运部",
    "12306科创中心": "12306科创中心",
    "广铁集团": "中国铁路广州局集团有限公司",
    "集团公司": "中国铁路广州局集团有限公司", # Be careful with context, might need more specific rules
    "本预案": "《广州局集团公司客票发售和预订系统（含互联网售票部分）应急预案》", # Main document title
    "《铁路客票发售和预订系统(含互联网售票部分)应急预案》": "《铁路客票发售和预订系统(含互联网售票部分)应急预案》（铁办客〔2021〕92号）", # Add identifier if possible
    "《广州局集团公司网络安全事件应急预案》": "《广州局集团公司网络安全事件应急预案》（广铁科信〔2019〕105号）",
    "《广州局集团公司信息系统故障应急处置和调查处理办法》": "《广州局集团公司信息系统故障应急处置和调查处理办法》（广铁科信发〔2022〕76号）",
    "客票系统": "客票发售和预订系统", # Normalize system name
    # Add station names if needed, e.g., "广州站": "广州站"
}

# 5. Logging Configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Helper Functions ---

def load_json_data(file_path: str) -> Optional[List[Dict[str, Any]]]:
    """Loads data from a JSON file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logging.info(f"Successfully loaded data from {file_path}")
        return data
    except FileNotFoundError:
        logging.error(f"Error: Input file not found at {file_path}")
        return None
    except json.JSONDecodeError:
        logging.error(f"Error: Could not decode JSON from {file_path}")
        return None

def normalize_entity_name(raw_name: str) -> str:
    """Normalizes entity names using the CANONICAL_MAP."""
    if not isinstance(raw_name, str):
        logging.warning(f"Attempted to normalize non-string value: {raw_name}. Returning as is.")
        return str(raw_name) # Attempt to convert to string if not already
    cleaned_name = raw_name.strip().replace('\n', ' ') # Clean whitespace and newlines
    # Simple lookup, case-sensitive for now
    return CANONICAL_MAP.get(cleaned_name, cleaned_name)

def create_entity_prompt(chunk_content: str) -> str:
    """Creates the prompt for entity extraction."""
    # Provide definitions to help the LLM
    definitions = """
    实体类型定义:
    - 文档：管理规定的文件名称，如《应急预案》。
    - 章节：文档中的具体章节标题，如“1 总则”。
    - 主题：文档或章节的核心议题，如“应急组织机构”。
    - 关键词：文本中重要的名词或术语，如“客票系统”、“应急响应”、“电子客票”。
    - 人员：具体的人名（此文档中可能较少）。
    - 角色：指代具有特定职责的职位或岗位，如“客运部主任”、“售票员”。
    - 组织：涉及的单位、部门或公司，如“中国铁路广州局集团有限公司”、“集团公司客运部”、“信息技术所”、“各车务站段”。
    - 时间：具体的日期、时间点或时间段，如“2021年”、“4小时及以上”、“每年3月”。
    - 事件：文档中描述的具体活动或状况，如“系统突发事件”、“启动应急预案”、“应急演练”、“售票故障”。
    - 法规：引用的其他法规或文件名称及其编号，如“《铁路客票发售和预订系统(含互联网售票部分)应急预案》（铁办客〔2021〕92号）”。
    """
    prompt = f"""
请从以下文本中提取定义的实体类型。

{definitions}

预定义的实体类型列表: {', '.join(ENTITY_TYPES)}

文本：
\"\"\"
{chunk_content}
\"\"\"

请以严格的 JSON 格式输出，包含一个名为 "entities" 的列表，其中每个对象包含 "name" (实体名称) 和 "type" (实体类型)。确保实体名称是文本中实际出现的词语。
例如:
{{
  "entities": [
    {{"name": "集团公司客运部", "type": "组织"}},
    {{"name": "售票故障", "type": "事件"}},
    {{"name": "《铁路客票发售和预订系统(含互联网售票部分)应急预案》（铁办客〔2021〕92号）", "type": "法规"}}
  ]
}}
"""
    return prompt

def create_relation_prompt(chunk_content: str) -> str:
    """Creates the prompt for relation extraction (within the chunk)."""
    # Provide definitions
    definitions = """
    关系类型定义 (请仅提取文本段落内明确描述的关系):
    - 隶属关系 (BelongsTo): 通常是结构化的，此提示词主要关注文本内描述，如“办公室设在客运部”。(结构化部分将后处理)
    - 版本关系 (HasVersion): 指明文档的版本信息或与其他版本的关系 (如“修订版”、“废止旧版”)。
    - 引用 (References): 一个实体提到了另一个实体或文件，如“详见附件5”。
    - 依据 (BasedOn): 指出制定某文件或采取某行动所依据的法规或原则，如“根据...制定本预案”。
    - 责任 (ResponsibleFor): 指明某个角色或组织负责某项任务或职责，如“客运部负责协调”。
    - 审批 (ApprovedBy): 指出某事项需要经过哪个组织或角色批准，如“经...同意后”。
    - 时间 (OccursAt): 事件发生的时间，或规定适用的时间点/段，如“事件影响4小时”、“每年3月开展演练”。
    - 生效 (EffectiveFrom): 规定或文件的生效日期，如“自发布之日起实施”。
    - 关联 (RelatedTo): 实体间的其他关联，如“与...不一致时，以此为准”。
    """
    prompt = f"""
请从以下文本中提取实体之间的关系。请专注于在文本段落中**直接陈述**的关系。

{definitions}

预定义的关系类型列表: {', '.join(RELATION_TYPES)}

文本：
\"\"\"
{chunk_content}
\"\"\"

请以严格的 JSON 格式输出，包含一个名为 "relations" 的列表，其中每个对象包含 "source" (源实体名称), "target" (目标实体名称), 和 "type" (关系类型)。确保实体名称是文本中实际出现的词语。
例如:
{{
  "relations": [
    {{"source": "集团公司应急领导小组办公室", "target": "集团公司客运部", "type": "隶属关系"}},
    {{"source": "本预案", "target": "《铁路客票发售和预订系统(含互联网售票部分)应急预案》", "type": "依据"}},
    {{"source": "客运部", "target": "协调各相关部门", "type": "责任"}}
  ]
}}
"""
    return prompt

# --- LLM Interaction (Placeholder) ---

def call_llm(prompt: str) -> Optional[str]:
    """
    Placeholder function to call the LLM API.
    Replace this with your actual LLM API call logic.
    """
    logging.info(f"--- Sending Prompt to LLM (Length: {len(prompt)}) ---")
    # logging.debug(prompt) # Uncomment to see the full prompt
    logging.info("--- Waiting for LLM Response ---")

    # !! --- REPLACE THIS SECTION --- !!
    # Example using OpenAI client (ensure client is initialized above)
    # try:
    #     response = client.chat.completions.create(
    #         model=LLM_MODEL,
    #         messages=[{"role": "user", "content": prompt}],
    #         temperature=0.1, # Lower temperature for more deterministic output
    #         response_format={ "type": "json_object" } # If supported by model/API
    #     )
    #     llm_output = response.choices[0].message.content
    #     logging.info("--- Received LLM Response ---")
    #     # time.sleep(1) # Add delay if needed for rate limiting
    #     return llm_output
    # except Exception as e:
    #     logging.error(f"LLM API call failed: {e}")
    #     return None

    # --- Placeholder Response (REMOVE WHEN USING REAL LLM) ---
    # Simulate a response for testing parsing logic
    if "提取定义的实体类型" in prompt:
         # Simulate entity extraction response
        logging.warning("Using PLACEHOLDER response for entity extraction.")
        # Simulate based on a known chunk, e.g., chunk_id: e50d6d8c_chunk_3_10
        if "集团公司成立客票系统应急领导小组" in prompt:
             return json.dumps({
                 "entities": [
                     {"name": "集团公司", "type": "组织"},
                     {"name": "客票系统应急领导小组", "type": "组织"},
                     {"name": "集团公司应急领导小组", "type": "组织"},
                     {"name": "科信部", "type": "组织"},
                     {"name": "客运部", "type": "组织"},
                     {"name": "信息技术所", "type": "组织"},
                     {"name": "财务部", "type": "组织"},
                     {"name": "计统部", "type": "组织"},
                     {"name": "电务部", "type": "组织"},
                     {"name": "供电部", "type": "组织"},
                     {"name": "宣传部", "type": "组织"},
                     {"name": "调度所", "type": "组织"},
                     {"name": "广州铁路公安局", "type": "组织"},
                     {"name": "集团公司应急领导小组办公室", "type": "组织"},
                     {"name": "客运部主任", "type": "角色"},
                     {"name": "057-22632", "type": "关键词"}, # Example, maybe not ideal type
                     {"name": "各车务站段", "type": "组织"}
                 ]
             })
        else:
            return json.dumps({"entities": []}) # Default empty for placeholder
    elif "提取实体之间的关系" in prompt:
        # Simulate relation extraction response
        logging.warning("Using PLACEHOLDER response for relation extraction.")
        if "集团公司成立客票系统应急领导小组" in prompt:
            return json.dumps({
                "relations": [
                    {"source": "集团公司应急领导小组办公室", "target": "集团公司客运部", "type": "隶属关系"},
                    {"source": "集团公司应急领导小组办公室", "target": "客运部主任", "type": "责任"}, # Simplified relationship
                    {"source": "各车务站段", "target": "集团公司", "type": "隶属关系"} # Example, might not be explicit
                ]
            })
        else:
             return json.dumps({"relations": []}) # Default empty
    else:
        return None
    # !! --- END REPLACE/REMOVE --- !!


def parse_llm_response(response_text: Optional[str]) -> Optional[Dict[str, List[Dict[str, str]]]]:
    """Safely parses the LLM's JSON response."""
    if not response_text:
        return None
    try:
        # Sometimes LLMs wrap the JSON in  ... 
        if response_text.strip().startswith(""):
            response_text = response_text.strip()[7:-3].strip()
        elif response_text.strip().startswith("`"):
             response_text = response_text.strip()[1:-1].strip()

        parsed_data = json.loads(response_text)
        # Basic validation
        if isinstance(parsed_data, dict) and \
           (('entities' in parsed_data and isinstance(parsed_data['entities'], list)) or \
            ('relations' in parsed_data and isinstance(parsed_data['relations'], list))):
            return parsed_data
        else:
            logging.warning(f"LLM response is valid JSON but not the expected structure: {response_text}")
            return None
    except json.JSONDecodeError as e:
        logging.error(f"Failed to decode LLM JSON response: {e}\nResponse text:\n{response_text}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred during LLM response parsing: {e}\nResponse text:\n{response_text}")
        return None


# --- Main Processing Logic ---

def main():
    """Main function to orchestrate the KG extraction process."""
    data = load_json_data(INPUT_JSON_PATH)
    if not data:
        return

    # Use sets for automatic deduplication
    entities_set: Set[Tuple[str, str]] = set()
    relations_set: Set[Tuple[str, str, str]] = set()
    chunk_map: Dict[str, Dict[str, Any]] = {chunk['chunk_id']: chunk for chunk in data} # For easy lookup
    # Store normalized entities per chunk if needed for complex structural relation logic later
    # chunk_entities: Dict[str, List[Tuple[str, str]]] = {}

    # --- Phase 1: LLM Extraction and Normalization ---
    for i, chunk in enumerate(data):
        chunk_id = chunk.get("chunk_id", f"unknown_{i}")
        content = chunk.get("content")
        heading = chunk.get("heading", "")

        if not content:
            logging.warning(f"Chunk {chunk_id} has no content. Skipping.")
            continue

        logging.info(f"Processing chunk {i+1}/{len(data)}: ID {chunk_id} (Heading: {heading[:50]}...)")

        # 1. Extract Entities
        entity_prompt = create_entity_prompt(content)
        entity_response_text = call_llm(entity_prompt)
        parsed_entities = parse_llm_response(entity_response_text)

        current_chunk_norm_entities = []
        if parsed_entities and 'entities' in parsed_entities:
            for entity in parsed_entities['entities']:
                raw_name = entity.get('name')
                raw_type = entity.get('type')
                if raw_name and raw_type and raw_type in ENTITY_TYPES:
                    normalized_name = normalize_entity_name(raw_name)
                    entity_tuple = (normalized_name, raw_type)
                    entities_set.add(entity_tuple)
                    current_chunk_norm_entities.append(entity_tuple)
                else:
                    logging.warning(f"Invalid entity format or type in chunk {chunk_id}: {entity}")
            # Store if needed later: chunk_entities[chunk_id] = current_chunk_norm_entities

        # 2. Extract Relations (within chunk)
        relation_prompt = create_relation_prompt(content)
        relation_response_text = call_llm(relation_prompt)
        parsed_relations = parse_llm_response(relation_response_text)

        if parsed_relations and 'relations' in parsed_relations:
            for relation in parsed_relations['relations']:
                raw_source = relation.get('source')
                raw_target = relation.get('target')
                raw_type = relation.get('type')
                if raw_source and raw_target and raw_type and raw_type in RELATION_TYPES:
                    normalized_source = normalize_entity_name(raw_source)
                    normalized_target = normalize_entity_name(raw_target)
                    relation_tuple = (normalized_source, normalized_target, raw_type)
                    relations_set.add(relation_tuple)
                else:
                     logging.warning(f"Invalid relation format or type in chunk {chunk_id}: {relation}")

    logging.info(f"Finished LLM extraction. Found {len(entities_set)} unique entities and {len(relations_set)} unique relations so far.")

    # --- Phase 2: Add Structural Relations (BelongsTo) ---
    logging.info("Adding structural 'BelongsTo' relations...")
    structural_relations_added = 0
    for chunk in data:
        chunk_id = chunk.get("chunk_id")
        parent_id = chunk.get("parent_id")
        heading = chunk.get("heading")

        if parent_id and chunk_id and heading:
            parent_chunk = chunk_map.get(parent_id)
            if parent_chunk and parent_chunk.get("heading"):
                # Use normalized heading as the entity name for the chunk/parent
                # Assume chunks with headings map to '章节' or '文档' entities
                # A more robust method might check if these entities were already extracted by LLM
                child_entity_name = normalize_entity_name(heading)
                parent_entity_name = normalize_entity_name(parent_chunk["heading"])

                # Add the BelongsTo relation
                # Add the entities themselves if not already present (assuming type based on hierarchy)
                # Infer type crudely - root is Document, others are Section
                child_type = "章节" # Default assumption for non-root heading
                parent_type = "章节" if parent_chunk.get("parent_id") else "文档"

                entities_set.add((child_entity_name, child_type))
                entities_set.add((parent_entity_name, parent_type))

                relation_tuple = (child_entity_name, parent_entity_name, "隶属关系")
                if relation_tuple not in relations_set:
                    relations_set.add(relation_tuple)
                    structural_relations_added += 1

    logging.info(f"Added {structural_relations_added} structural 'BelongsTo' relations.")

    # --- Output Results ---
    print("\n--- Final Extracted Entities ---")
    # Sort for consistency
    sorted_entities = sorted(list(entities_set))
    for name, type in sorted_entities:
        print(f"- ({type}) {name}")
    print(f"\nTotal Unique Entities: {len(sorted_entities)}")

    print("\n--- Final Extracted Relations ---")
    sorted_relations = sorted(list(relations_set))
    for source, target, type in sorted_relations:
        print(f"- {source} --[{type}]--> {target}")
    print(f"\nTotal Unique Relations: {len(sorted_relations)}")


if __name__ == "__main__":
    main()


**How to Use:**

1.  **Save:** Save the code as a Python file (e.g., `kg_extractor.py`).
2.  **Install Libraries:** If you choose a specific LLM library (like `openai`), install it: `pip install openai`.
3.  **Configure:**
    *   Set the environment variable `YOUR_LLM_API_KEY` (or rename it in the code) to your actual LLM API key.
    *   Update `LLM_MODEL` to the specific model you want to use.
    *   Ensure `INPUT_JSON_PATH` points to your `demo3.json` file.
    *   Review and expand the `CANONICAL_MAP` based on your document's specifics.
4.  **Replace Placeholder:** Modify the `call_llm` function to use your chosen LLM library's API call mechanism. Remove the placeholder `if/else` block within `call_llm`.
5.  **Run:** Execute the script from your terminal: `python kg_extractor.py`.
6.  **Review Output:** Check the printed lists of entities and relations.

**Next Steps & Potential Improvements:**

1.  **Implement Real `call_llm`:** This is the most critical step.
2.  **Refine `CANONICAL_MAP`:** Add more mappings based on observed LLM outputs.
3.  **Improve Structural Relation Logic:** Make the mapping between chunk headings and entity types/names more robust (e.g., check `chunk_entities` first).
4.  **Error Handling:** Add more specific error handling around LLM calls (rate limits, content filtering).
5.  **Confidence Scores:** If your LLM provides confidence scores, you could incorporate them to filter low-confidence extractions.
6.  **Advanced Normalization/Entity Linking:** Use libraries like `spaCy` or dedicated entity linking tools for more sophisticated normalization.
7.  **Output Format:** Save results to CSV, JSONL, or directly into a graph database format (e.g., Neo4j Cypher statements).
8.  **Batching/Async:** For large documents, consider batching chunks or using asynchronous API calls to speed up processing.

This code provides a solid starting point for extracting the knowledge graph using the discussed strategy. Remember to replace the placeholder LLM logic with your actual implementation.
```

**user**: 

```
增加输出格式：
memgraph Cypher statements
```

**assistant**: 

```
Okay, let's add the functionality to output the extracted knowledge graph as Memgraph/Neo4j compatible Cypher statements.

We'll add a new function `generate_cypher_statements` and call it at the end of the `main` function. We also need a helper function to escape strings properly for Cypher.


import json
import os
import logging
import time # For potential rate limiting
from typing import List, Dict, Set, Tuple, Optional, Any

# --- Configuration ---

# [ Existing configurations remain the same: LLM_API_KEY, LLM_MODEL, INPUT_JSON_PATH, ENTITY_TYPES, RELATION_TYPES, CANONICAL_MAP, logging ]
# 1. LLM Configuration
LLM_API_KEY = os.environ.get("YOUR_LLM_API_KEY")
LLM_MODEL = "your-chosen-llm-model"

# 2. File Path
INPUT_JSON_PATH = "demo3.json"
OUTPUT_CYPHER_PATH = "output_graph.cypher" # Optional: File to save Cypher statements

# 3. Knowledge Graph Schema (Based on 改进建议.md)
ENTITY_TYPES = [
    "文档", "章节", "主题", "关键词", "人员", "角色",
    "组织", "时间", "事件", "法规"
]

# Map Chinese types to English for potentially better compatibility or preference
ENTITY_TYPE_MAP_CYPHER = {
    "文档": "Document",
    "章节": "Section",
    "主题": "Topic",
    "关键词": "Keyword",
    "人员": "Person",
    "角色": "Role",
    "组织": "Organization",
    "时间": "Time",
    "事件": "Event",
    "法规": "Regulation"
}


RELATION_TYPES = [
    "隶属关系", "版本关系", "引用", "依据", "责任",
    "审批", "时间", "生效", "关联"
]

# Map Chinese types to English (often uppercase) for Cypher relationships
RELATION_TYPE_MAP_CYPHER = {
    "隶属关系": "BELONGS_TO",
    "版本关系": "HAS_VERSION",
    "引用": "REFERENCES",
    "依据": "BASED_ON",
    "责任": "RESPONSIBLE_FOR",
    "审批": "APPROVED_BY",
    "时间": "OCCURS_AT", # Consider if Time should be a node instead
    "生效": "EFFECTIVE_FROM", # Consider if Time should be a node instead
    "关联": "RELATED_TO"
}


# 4. Entity Normalization Map (Example - expand as needed)
CANONICAL_MAP = {
    "客运部": "集团公司客运部",
    "信息技术所": "集团公司信息技术所",
    "科信部": "集团公司科信部",
    "财务部": "集团公司财务部",
    "计统部": "集团公司计统部",
    "电务部": "集团公司电务部",
    "供电部": "集团公司供电部",
    "宣传部": "集团公司宣传部",
    "调度所": "集团公司调度所",
    "集团公司应急领导小组办公室": "集团公司应急领导小组办公室", # Already specific
    "集团公司应急领导小组": "集团公司应急领导小组", # Already specific
    "国铁集团应急领导小组办公室": "国铁集团应急领导小组办公室",
    "国铁集团应急领导小组": "国铁集团应急领导小组",
    "国铁集团客运部": "国铁集团客运部",
    "12306科创中心": "12306科创中心",
    "广铁集团": "中国铁路广州局集团有限公司",
    "集团公司": "中国铁路广州局集团有限公司", # Be careful with context, might need more specific rules
    "本预案": "《广州局集团公司客票发售和预订系统（含互联网售票部分）应急预案》", # Main document title
    "《铁路客票发售和预订系统(含互联网售票部分)应急预案》": "《铁路客票发售和预订系统(含互联网售票部分)应急预案》（铁办客〔2021〕92号）", # Add identifier if possible
    "《广州局集团公司网络安全事件应急预案》": "《广州局集团公司网络安全事件应急预案》（广铁科信〔2019〕105号）",
    "《广州局集团公司信息系统故障应急处置和调查处理办法》": "《广州局集团公司信息系统故障应急处置和调查处理办法》（广铁科信发〔2022〕76号）",
    "客票系统": "客票发售和预订系统", # Normalize system name
}

# 5. Logging Configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# --- Helper Functions ---

def load_json_data(file_path: str) -> Optional[List[Dict[str, Any]]]:
    """Loads data from a JSON file."""
    # [ Function code remains the same ]
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logging.info(f"Successfully loaded data from {file_path}")
        return data
    except FileNotFoundError:
        logging.error(f"Error: Input file not found at {file_path}")
        return None
    except json.JSONDecodeError:
        logging.error(f"Error: Could not decode JSON from {file_path}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred during JSON loading: {e}")
        return None


def normalize_entity_name(raw_name: str) -> str:
    """Normalizes entity names using the CANONICAL_MAP."""
    # [ Function code remains the same ]
    if not isinstance(raw_name, str):
        logging.warning(f"Attempted to normalize non-string value: {raw_name}. Returning as is.")
        return str(raw_name)
    cleaned_name = raw_name.strip().replace('\n', ' ')
    return CANONICAL_MAP.get(cleaned_name, cleaned_name)

def escape_cypher_string(value: str) -> str:
    """Escapes single quotes and backslashes for Cypher strings."""
    if not isinstance(value, str):
        return str(value) # Return as string if not already
    return value.replace('\\', '\\\\').replace("'", "\\'")

def create_entity_prompt(chunk_content: str) -> str:
    """Creates the prompt for entity extraction."""
    # [ Function code remains the same ]
    definitions = """
    实体类型定义:
    - 文档：管理规定的文件名称，如《应急预案》。
    - 章节：文档中的具体章节标题，如“1 总则”。
    - 主题：文档或章节的核心议题，如“应急组织机构”。
    - 关键词：文本中重要的名词或术语，如“客票系统”、“应急响应”、“电子客票”。
    - 人员：具体的人名（此文档中可能较少）。
    - 角色：指代具有特定职责的职位或岗位，如“客运部主任”、“售票员”。
    - 组织：涉及的单位、部门或公司，如“中国铁路广州局集团有限公司”、“集团公司客运部”、“信息技术所”、“各车务站段”。
    - 时间：具体的日期、时间点或时间段，如“2021年”、“4小时及以上”、“每年3月”。
    - 事件：文档中描述的具体活动或状况，如“系统突发事件”、“启动应急预案”、“应急演练”、“售票故障”。
    - 法规：引用的其他法规或文件名称及其编号，如“《铁路客票发售和预订系统(含互联网售票部分)应急预案》（铁办客〔2021〕92号）”。
    """
    prompt = f"""
请从以下文本中提取定义的实体类型。

{definitions}

预定义的实体类型列表: {', '.join(ENTITY_TYPES)}

文本：
\"\"\"
{chunk_content}
\"\"\"

请以严格的 JSON 格式输出，包含一个名为 "entities" 的列表，其中每个对象包含 "name" (实体名称) 和 "type" (实体类型)。确保实体名称是文本中实际出现的词语。
例如:
{{
  "entities": [
    {{"name": "集团公司客运部", "type": "组织"}},
    {{"name": "售票故障", "type": "事件"}},
    {{"name": "《铁路客票发售和预订系统(含互联网售票部分)应急预案》（铁办客〔2021〕92号）", "type": "法规"}}
  ]
}}
"""
    return prompt

def create_relation_prompt(chunk_content: str) -> str:
    """Creates the prompt for relation extraction (within the chunk)."""
    # [ Function code remains the same ]
    definitions = """
    关系类型定义 (请仅提取文本段落内明确描述的关系):
    - 隶属关系 (BelongsTo): 通常是结构化的，此提示词主要关注文本内描述，如“办公室设在客运部”。(结构化部分将后处理)
    - 版本关系 (HasVersion): 指明文档的版本信息或与其他版本的关系 (如“修订版”、“废止旧版”)。
    - 引用 (References): 一个实体提到了另一个实体或文件，如“详见附件5”。
    - 依据 (BasedOn): 指出制定某文件或采取某行动所依据的法规或原则，如“根据...制定本预案”。
    - 责任 (ResponsibleFor): 指明某个角色或组织负责某项任务或职责，如“客运部负责协调”。
    - 审批 (ApprovedBy): 指出某事项需要经过哪个组织或角色批准，如“经...同意后”。
    - 时间 (OccursAt): 事件发生的时间，或规定适用的时间点/段，如“事件影响4小时”、“每年3月开展演练”。
    - 生效 (EffectiveFrom): 规定或文件的生效日期，如“自发布之日起实施”。
    - 关联 (RelatedTo): 实体间的其他关联，如“与...不一致时，以此为准”。
    """
    prompt = f"""
请从以下文本中提取实体之间的关系。请专注于在文本段落中**直接陈述**的关系。

{definitions}

预定义的关系类型列表: {', '.join(RELATION_TYPES)}

文本：
\"\"\"
{chunk_content}
\"\"\"

请以严格的 JSON 格式输出，包含一个名为 "relations" 的列表，其中每个对象包含 "source" (源实体名称), "target" (目标实体名称), 和 "type" (关系类型)。确保实体名称是文本中实际出现的词语。
例如:
{{
  "relations": [
    {{"source": "集团公司应急领导小组办公室", "target": "集团公司客运部", "type": "隶属关系"}},
    {{"source": "本预案", "target": "《铁路客票发售和预订系统(含互联网售票部分)应急预案》", "type": "依据"}},
    {{"source": "客运部", "target": "协调各相关部门", "type": "责任"}}
  ]
}}
"""
    return prompt


# --- LLM Interaction (Placeholder) ---

def call_llm(prompt: str) -> Optional[str]:
    """
    Placeholder function to call the LLM API.
    Replace this with your actual LLM API call logic.
    """
    # [ Function code remains largely the same, use your actual LLM call here ]
    logging.info(f"--- Sending Prompt to LLM (Length: {len(prompt)}) ---")
    logging.info("--- Waiting for LLM Response ---")

    # !! --- REPLACE THIS SECTION with your actual LLM call --- !!
    # Simulate a response for testing
    if "提取定义的实体类型" in prompt:
         logging.warning("Using PLACEHOLDER response for entity extraction.")
         if "集团公司成立客票系统应急领导小组" in prompt:
             return json.dumps({ "entities": [ {"name": "集团公司", "type": "组织"}, {"name": "客票系统应急领导小组", "type": "组织"}, {"name": "集团公司应急领导小组", "type": "组织"}, {"name": "科信部", "type": "组织"}, {"name": "客运部", "type": "组织"}, {"name": "信息技术所", "type": "组织"}, {"name": "财务部", "type": "组织"}, {"name": "计统部", "type": "组织"}, {"name": "电务部", "type": "组织"}, {"name": "供电部", "type": "组织"}, {"name": "宣传部", "type": "组织"}, {"name": "调度所", "type": "组织"}, {"name": "广州铁路公安局", "type": "组织"}, {"name": "集团公司应急领导小组办公室", "type": "组织"}, {"name": "客运部主任", "type": "角色"}, {"name": "057-22632", "type": "关键词"}, {"name": "各车务站段", "type": "组织"} ] })
         else: return json.dumps({"entities": []})
    elif "提取实体之间的关系" in prompt:
        logging.warning("Using PLACEHOLDER response for relation extraction.")
        if "集团公司成立客票系统应急领导小组" in prompt:
            return json.dumps({ "relations": [ {"source": "集团公司应急领导小组办公室", "target": "集团公司客运部", "type": "隶属关系"}, {"source": "集团公司应急领导小组办公室", "target": "客运部主任", "type": "责任"}, {"source": "各车务站段", "target": "集团公司", "type": "隶属关系"} ] })
        else: return json.dumps({"relations": []})
    else: return None
    # !! --- END REPLACE/REMOVE --- !!


def parse_llm_response(response_text: Optional[str]) -> Optional[Dict[str, List[Dict[str, str]]]]:
    """Safely parses the LLM's JSON response."""
    # [ Function code remains the same ]
    if not response_text: return None
    try:
        if response_text.strip().startswith(""):
            response_text = response_text.strip()[7:-3].strip()
        elif response_text.strip().startswith("`"):
             response_text = response_text.strip()[1:-1].strip()
        parsed_data = json.loads(response_text)
        if isinstance(parsed_data, dict) and \
           (('entities' in parsed_data and isinstance(parsed_data['entities'], list)) or \
            ('relations' in parsed_data and isinstance(parsed_data['relations'], list))):
            return parsed_data
        else:
            logging.warning(f"LLM response is valid JSON but not the expected structure: {response_text}")
            return None
    except json.JSONDecodeError as e:
        logging.error(f"Failed to decode LLM JSON response: {e}\nResponse text:\n{response_text}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred during LLM response parsing: {e}\nResponse text:\n{response_text}")
        return None

# --- Cypher Generation ---

def generate_cypher_statements(entities: Set[Tuple[str, str]], relations: Set[Tuple[str, str, str]]) -> List[str]:
    """Generates Memgraph/Neo4j Cypher MERGE statements."""
    cypher_statements = []

    # Add constraint for uniqueness if desired (recommended)
    # You might want to run these manually once or ensure they exist
    # for entity_type_cn in ENTITY_TYPES:
    #     entity_type_cypher = ENTITY_TYPE_MAP_CYPHER.get(entity_type_cn, entity_type_cn) # Use mapped or original
    #     cypher_statements.append(f"CREATE CONSTRAINT ON (n:{entity_type_cypher}) ASSERT n.name IS UNIQUE;")

    cypher_statements.append("\n// --- Entity Creation ---")
    sorted_entities = sorted(list(entities))
    for name, type_cn in sorted_entities:
        if not name: # Skip empty names
            continue
        entity_type_cypher = ENTITY_TYPE_MAP_CYPHER.get(type_cn, type_cn.replace(" ", "_")) # Map or sanitize
        escaped_name = escape_cypher_string(name)
        # Use MERGE to avoid duplicates
        cypher_statements.append(f"MERGE (:`{entity_type_cypher}` {{name: '{escaped_name}'}});")

    cypher_statements.append("\n// --- Relationship Creation ---")
    sorted_relations = sorted(list(relations))
    for source, target, type_cn in sorted_relations:
        if not source or not target: # Skip if source or target is missing
            continue
        relation_type_cypher = RELATION_TYPE_MAP_CYPHER.get(type_cn, type_cn.upper().replace(" ", "_")) # Map or sanitize
        escaped_source = escape_cypher_string(source)
        escaped_target = escape_cypher_string(target)

        # Find the types of source and target from the entities set for better matching
        source_type_cn = next((t for n, t in sorted_entities if n == source), None)
        target_type_cn = next((t for n, t in sorted_entities if n == target), None)

        if source_type_cn and target_type_cn:
            source_type_cypher = ENTITY_TYPE_MAP_CYPHER.get(source_type_cn, source_type_cn.replace(" ", "_"))
            target_type_cypher = ENTITY_TYPE_MAP_CYPHER.get(target_type_cn, target_type_cn.replace(" ", "_"))
            # Use MERGE for relationships too
            cypher_statements.append(
                f"MATCH (a:`{source_type_cypher}` {{name: '{escaped_source}'}}), (b:`{target_type_cypher}` {{name: '{escaped_target}'}}) "
                f"MERGE (a)-[:`{relation_type_cypher}`]->(b);"
            )
        else:
             # Fallback if type not found (less safe, might match wrong nodes if names overlap across types)
             logging.warning(f"Could not determine types for relationship: ({source})-[{type_cn}]->({target}). Using generic MATCH.")
             cypher_statements.append(
                 f"MATCH (a {{name: '{escaped_source}'}}), (b {{name: '{escaped_target}'}}) "
                 f"MERGE (a)-[:`{relation_type_cypher}`]->(b);"
             )

    return cypher_statements


# --- Main Processing Logic ---

def main():
    """Main function to orchestrate the KG extraction process."""
    data = load_json_data(INPUT_JSON_PATH)
    if not data:
        return

    entities_set: Set[Tuple[str, str]] = set()
    relations_set: Set[Tuple[str, str, str]] = set()
    chunk_map: Dict[str, Dict[str, Any]] = {chunk['chunk_id']: chunk for chunk in data}

    # --- Phase 1: LLM Extraction and Normalization ---
    # [ This loop remains the same as before ]
    for i, chunk in enumerate(data):
        chunk_id = chunk.get("chunk_id", f"unknown_{i}")
        content = chunk.get("content")
        heading = chunk.get("heading", "")
        if not content: continue
        logging.info(f"Processing chunk {i+1}/{len(data)}: ID {chunk_id} (Heading: {heading[:50]}...)")

        # Extract Entities
        entity_prompt = create_entity_prompt(content)
        entity_response_text = call_llm(entity_prompt)
        parsed_entities = parse_llm_response(entity_response_text)
        if parsed_entities and 'entities' in parsed_entities:
            for entity in parsed_entities['entities']:
                raw_name = entity.get('name')
                raw_type = entity.get('type')
                if raw_name and raw_type and raw_type in ENTITY_TYPES:
                    normalized_name = normalize_entity_name(raw_name)
                    entities_set.add((normalized_name, raw_type))
                else: logging.warning(f"Invalid entity format or type in chunk {chunk_id}: {entity}")

        # Extract Relations
        relation_prompt = create_relation_prompt(content)
        relation_response_text = call_llm(relation_prompt)
        parsed_relations = parse_llm_response(relation_response_text)
        if parsed_relations and 'relations' in parsed_relations:
            for relation in parsed_relations['relations']:
                raw_source = relation.get('source')
                raw_target = relation.get('target')
                raw_type = relation.get('type')
                if raw_source and raw_target and raw_type and raw_type in RELATION_TYPES:
                    normalized_source = normalize_entity_name(raw_source)
                    normalized_target = normalize_entity_name(raw_target)
                    relations_set.add((normalized_source, normalized_target, raw_type))
                else: logging.warning(f"Invalid relation format or type in chunk {chunk_id}: {relation}")

    logging.info(f"Finished LLM extraction. Found {len(entities_set)} unique entities and {len(relations_set)} unique relations so far.")

    # --- Phase 2: Add Structural Relations (BelongsTo) ---
    # [ This loop remains the same as before ]
    logging.info("Adding structural 'BelongsTo' relations...")
    structural_relations_added = 0
    for chunk in data:
        chunk_id = chunk.get("chunk_id")
        parent_id = chunk.get("parent_id")
        heading = chunk.get("heading")
        if parent_id and chunk_id and heading:
            parent_chunk = chunk_map.get(parent_id)
            if parent_chunk and parent_chunk.get("heading"):
                child_entity_name = normalize_entity_name(heading)
                parent_entity_name = normalize_entity_name(parent_chunk["heading"])
                if child_entity_name and parent_entity_name: # Ensure names are not empty
                    child_type = "章节"
                    parent_type = "章节" if parent_chunk.get("parent_id") else "文档"
                    entities_set.add((child_entity_name, child_type))
                    entities_set.add((parent_entity_name, parent_type))
                    relation_tuple = (child_entity_name, parent_entity_name, "隶属关系")
                    if relation_tuple not in relations_set:
                        relations_set.add(relation_tuple)
                        structural_relations_added += 1
    logging.info(f"Added {structural_relations_added} structural 'BelongsTo' relations.")


    # --- Output Results ---

    # 1. Console Output (Original)
    print("\n--- Final Extracted Entities (Console) ---")
    sorted_entities = sorted(list(entities_set))
    for name, type in sorted_entities:
        print(f"- ({type}) {name}")
    print(f"\nTotal Unique Entities: {len(sorted_entities)}")

    print("\n--- Final Extracted Relations (Console) ---")
    sorted_relations = sorted(list(relations_set))
    for source, target, type in sorted_relations:
        print(f"- {source} --[{type}]--> {target}")
    print(f"\nTotal Unique Relations: {len(sorted_relations)}")

    # 2. Cypher Statement Generation and Output
    print(f"\n--- Generating Cypher Statements (Memgraph/Neo4j) ---")
    cypher_statements = generate_cypher_statements(entities_set, relations_set)

    # Option A: Print to console
    # print("\n".join(cypher_statements))

    # Option B: Save to file
    try:
        with open(OUTPUT_CYPHER_PATH, 'w', encoding='utf-8') as f:
            f.write(";\n".join(cypher_statements) + ";\n") # Add semicolon after each statement
        print(f"\nCypher statements saved to: {OUTPUT_CYPHER_PATH}")
    except IOError as e:
        print(f"\nError writing Cypher statements to file {OUTPUT_CYPHER_PATH}: {e}")
        print("\nCypher Statements:\n")
        print(";\n".join(cypher_statements) + ";\n") # Print to console as fallback


if __name__ == "__main__":
    main()


**Changes Made:**

1.  **Cypher Configuration:** Added `OUTPUT_CYPHER_PATH`, `ENTITY_TYPE_MAP_CYPHER`, and `RELATION_TYPE_MAP_CYPHER` to map Chinese types to preferred English/uppercase Cypher labels/types.
2.  **`escape_cypher_string` Function:** Added a helper to escape single quotes and backslashes in names used within Cypher strings.
3.  **`generate_cypher_statements` Function:**
    *   Takes the final `entities_set` and `relations_set` as input.
    *   Iterates through entities, generating `MERGE (:<EntityType> {name: '<EscapedName>'});` statements. It uses the `ENTITY_TYPE_MAP_CYPHER`.
    *   Iterates through relations, generating `MATCH (a:<SourceType> {name:'...'}), (b:<TargetType> {name:'...'}) MERGE (a)-[:<RelationType>]->(b);` statements. It tries to find the node types first for a more specific `MATCH`. It uses `RELATION_TYPE_MAP_CYPHER`.
    *   Uses the `escape_cypher_string` function for all names.
    *   Uses backticks (`) around labels and relationship types just in case they contain special characters or conflict with keywords (though the mapped ones should be safe).
4.  **`main` Function Integration:**
    *   Calls `generate_cypher_statements` after all entities and relations are collected.
    *   Includes logic to either print the Cypher statements to the console or save them to the file specified in `OUTPUT_CYPHER_PATH`. Statements are separated by semicolons.

Now, when you run the script (after replacing the placeholder LLM call), it will print the entities and relations as before, and then generate and save (or print) the corresponding Cypher `MERGE` statements. You can then execute these statements in Memgraph Lab, Memgraph command line, or Neo4j Browser to populate your graph database.
```



--------------------


<a href="https://chatboxai.app" style="display: flex; align-items: center;">
<img src='https://chatboxai.app/icon.png' style='width: 40px; height: 40px; padding-right: 6px'>
<b style='font-size:30px'>Chatbox AI</b>
</a>
