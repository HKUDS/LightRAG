###############################################################################
# LightRAG 性能优化配置
# 此配置文件专门用于提升索引速度
#
# 性能分析：
# - 默认配置 MAX_ASYNC=4 导致每批次100个chunks需要1000-1500秒
# - 优化后预计可以将速度提升 4-8 倍
#
# 使用方法：
#   1. 根据您的 LLM API 速率限制调整以下参数
#   2. 复制此文件为 .env: cp .env.performance .env
#   3. 重启 LightRAG 服务
###############################################################################

###############################################################################
# 并发配置优化 (Concurrency Configuration)
###############################################################################

### MAX_ASYNC - LLM并发请求数（最重要的性能参数！）
#
# 说明：控制同时进行的 LLM API 调用数量
#
# 性能影响分析：
#   - 默认值 4:  100 chunks → 25轮处理 → ~1500秒/批次 (0.07 chunks/s)
#   - 设置为 16: 100 chunks → 7轮处理  → ~400秒/批次  (0.25 chunks/s) [4x提升]
#   - 设置为 32: 100 chunks → 4轮处理  → ~200秒/批次  (0.5 chunks/s)  [8x提升]
#
# 推荐设置：
#   - OpenAI API (有速率限制):         16-24
#   - Azure OpenAI (企业版):          32-64
#   - 自托管模型 (Ollama/vLLM):       64-128
#   - Claude API:                     8-16 (速率限制较严格)
#
# ⚠️ 注意：设置过高可能触发 API 速率限制 (Rate Limit)
MAX_ASYNC=16

### MAX_PARALLEL_INSERT - 并行处理文档数
#
# 说明：同时处理的文档数量
#
# 推荐设置：MAX_ASYNC / 3 ~ MAX_ASYNC / 4
#   - MAX_ASYNC=16 时: 建议 4-5
#   - MAX_ASYNC=32 时: 建议 8-10
#
# ⚠️ 注意：设置过高会增加实体/关系命名冲突，降低合并阶段效率
MAX_PARALLEL_INSERT=4

### EMBEDDING_FUNC_MAX_ASYNC - Embedding 并发数
#
# 说明：同时进行的 Embedding API 调用数量
#
# 推荐设置：
#   - OpenAI Embeddings:              16-32
#   - 本地 Embedding 模型:           32-64
EMBEDDING_FUNC_MAX_ASYNC=16

### EMBEDDING_BATCH_NUM - Embedding 批处理大小
#
# 说明：单次 Embedding 请求处理的文本数量
#
# 推荐设置：
#   - 默认值 10 太小，建议增加到 32-64
#   - 如果使用本地模型，可以设置为 100-200
EMBEDDING_BATCH_NUM=32

###############################################################################
# 超时配置 (Timeout Configuration)
###############################################################################

### LLM_TIMEOUT - LLM 请求超时时间（秒）
#
# 说明：单次 LLM API 调用的最大等待时间
#
# 推荐设置：
#   - 云端 API (OpenAI/Claude):      180 (3分钟)
#   - 自托管模型 (快速):             60-120
#   - 自托管模型 (大模型):           300-600
LLM_TIMEOUT=180

### EMBEDDING_TIMEOUT - Embedding 请求超时时间（秒）
#
# 推荐设置：
#   - 云端 API:                       30
#   - 本地模型:                       10-20
EMBEDDING_TIMEOUT=30

###############################################################################
# 预期性能提升
###############################################################################
#
# 使用此优化配置后，预期性能：
#
# | 配置场景              | 批次耗时  | 吞吐量        | 提升倍数 |
# |----------------------|-----------|---------------|---------|
# | 默认配置 (MAX_ASYNC=4)   | ~1500秒   | 0.07 chunks/s | 1x      |
# | 优化配置 (MAX_ASYNC=16)  | ~400秒    | 0.25 chunks/s | 4x      |
# | 激进配置 (MAX_ASYNC=32)  | ~200秒    | 0.5 chunks/s  | 8x      |
#
# 您的 1417 chunks 总耗时预计：
#   - 当前: ~20478秒 (5.7小时)   ✗
#   - 优化后: ~5000秒 (1.4小时)   ✓ [4x 提升]
#   - 激进优化: ~2500秒 (0.7小时) ✓ [8x 提升]
#
###############################################################################

###############################################################################
# 其他常用配置（根据需要取消注释）
###############################################################################

# ### Logging Configuration
# LOG_LEVEL=INFO
# LOG_MAX_BYTES=10485760
# LOG_BACKUP_COUNT=5

# ### LLM Configuration
# LLM_BINDING=openai
# LLM_BINDING_HOST=https://api.openai.com/v1
# LLM_MODEL_NAME=gpt-4o-mini

# ### Embedding Configuration
# EMBEDDING_BINDING=openai
# EMBEDDING_BINDING_HOST=https://api.openai.com/v1
# EMBEDDING_MODEL_NAME=text-embedding-3-small
# EMBEDDING_DIM=1536

###############################################################################
# 高级优化建议
###############################################################################
#
# 1. 使用本地 LLM 模型（避免网络延迟）：
#    - Ollama + DeepSeek-R1 / Qwen2.5
#    - vLLM + Llama-3.1-70B
#
# 2. 使用本地 Embedding 模型：
#    - sentence-transformers
#    - BGE-M3 / GTE-large
#
# 3. 升级到更快的图数据库：
#    - Neo4j → Memgraph (更快的内存图数据库)
#    - NetworkX → Neo4j (生产环境)
#
# 4. 使用 SSD 存储（如果使用 JSON/NetworkX 存储）
#
# 5. 关闭 gleaning（如果不需要高精度）：
#    - entity_extract_max_gleaning=0
#
###############################################################################
