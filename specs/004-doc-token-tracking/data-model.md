# Data Model: Document Processing Token Tracking

**Feature**: 004-doc-token-tracking
**Date**: 2025-12-25

## Entities

### TokenUsage

Represents token consumption for a single document processing operation.

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| embedding_tokens | integer | yes | Total tokens consumed by embedding API calls |
| llm_input_tokens | integer | yes | Total prompt tokens consumed by LLM calls |
| llm_output_tokens | integer | yes | Total completion tokens generated by LLM calls |
| total_chunks | integer | yes | Number of text chunks created from document |
| embedding_model | string | no | Embedding model identifier (null if local model) |
| llm_model | string | no | LLM model identifier (null if local model) |

**Validation Rules**:
- All token counts must be >= 0
- total_chunks must be >= 0
- Model names are optional (null for local models)

**Example**:
```json
{
  "embedding_tokens": 93,
  "llm_input_tokens": 7850,
  "llm_output_tokens": 462,
  "total_chunks": 1,
  "embedding_model": "text-embedding-3-small",
  "llm_model": "openai/gpt-4o-mini"
}
```

### DocumentStatusMetadata

Extended metadata object stored with document processing status.

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| processing_start_time | integer | yes | Unix timestamp when processing started |
| processing_end_time | integer | yes | Unix timestamp when processing completed |
| token_usage | TokenUsage | yes | Token consumption data |

**Example**:
```json
{
  "processing_start_time": 1766671645,
  "processing_end_time": 1766671752,
  "token_usage": {
    "embedding_tokens": 93,
    "llm_input_tokens": 7850,
    "llm_output_tokens": 462,
    "total_chunks": 1,
    "embedding_model": "text-embedding-3-small",
    "llm_model": "openai/gpt-4o-mini"
  }
}
```

### DocumentTrackingResponse

Response from GET /documents/track_status/{track_id} endpoint.

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| track_id | string | yes | Tracking identifier for the document batch |
| documents | array[DocumentStatus] | yes | List of document statuses |
| status_summary | object | yes | Aggregated status counts |

### DocumentStatus

Individual document status within tracking response.

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| id | string | yes | Document identifier |
| status | string | yes | Processing status (pending, processing, processed, failed) |
| chunks_count | integer | no | Number of chunks (when processed) |
| metadata | DocumentStatusMetadata | no | Token usage and timing (when processed) |
| error_msg | string | no | Error message (when failed) |

## State Transitions

```
pending → processing → processed (with token_usage in metadata)
    ↓         ↓
    └─────────┴──→ failed (may have partial token_usage)
```

## Storage

All data stored in existing `lightrag_doc_status` table:

```sql
-- Existing table, no schema changes
lightrag_doc_status.metadata JSONB
```

**Indexing**: Existing indexes on `workspace`, `track_id`, `status` are sufficient.

## Relationships

```
Document (1) ─────────── (1) DocumentStatus
                              │
                              └── metadata
                                    │
                                    └── token_usage
```

Each document has exactly one status record with embedded token_usage in metadata.
